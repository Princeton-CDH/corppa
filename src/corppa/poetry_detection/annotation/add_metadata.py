# -*- coding: utf-8 -*-
"""preprocess_annotation_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOwTYffKXGvLiXmwmRZHpxTuMUDxJGCh

## Preprocessing data for poetry annotation task
"""

import os
from PIL import Image
from tqdm import tqdm

import pandas as pd
import gzip
import json


#!python -m pip install jupyterlab-prodigy
#!pip install pillow
#!pip install tqdm

"""### converting .TIF files"""

# we need to convert the .TIF images to .jpg (Google Chrome does not render .TIF, Safari does)
# all Hathitrust images are already in .jpg format


def tif_to_jpg(directory, output_directory):
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    tif_files = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith(".TIF") or file.endswith(".tif"):
                tif_files.append(os.path.join(root, file))

    for img_path in tqdm(tif_files, desc="Converting...", unit="image"):
        img = Image.open(img_path)
        relative_path = os.path.relpath(img_path, directory)
        jpg_path = os.path.join(
            output_directory, os.path.splitext(relative_path)[0] + ".jpg"
        )

        if not os.path.exists(os.path.dirname(jpg_path)):
            os.makedirs(os.path.dirname(jpg_path))

        img.save(jpg_path, "JPEG")


# apply to input and output dirs
tif_to_jpg("Gale_tif", "Gale")

"""### loading the JSONL file and merging it with the metadata (+ correcting path names)"""


def combine_data(jsonl_path, csv_path):
    # load metadata (we get title, author, and publication year from this file)
    metadata_df = pd.read_csv(csv_path)

    # open and read Laure's JSONL file (we get the poem text and the work_id from this file)
    def read_jsonl(file_path):
        data = []
        with gzip.open(file_path, "rt", encoding="utf-8") as file:
            for line in file:
                json_data = json.loads(line)
                data.append(json_data)
        return pd.DataFrame(data)

    # read JSONL
    jsonl_df = read_jsonl(jsonl_path)
    # replace .tif with .jpg in image_path (quick fix for the image path issue)
    jsonl_df["image_path"] = jsonl_df["image_path"].str.replace(".TIF", ".jpg")

    # merge data on 'work_id' from JSONL file and 'ID' from metadata CSV
    combined_df = pd.merge(
        jsonl_df,
        metadata_df[["ID", "title", "author", "pub_date"]],
        left_on="work_id",
        right_on="ID",
        how="left",
    )

    # drop duplicate ID column
    combined_df.drop(columns=["ID"], inplace=True)

    # add meta information to each record
    # we will use this to display the title, author, and publication date in the annotation interface
    combined_df["meta"] = combined_df.apply(
        lambda row: {
            "title": row["title"],
            "author": row["author"],
            "pub_date": row["pub_date"],
        },
        axis=1,
    )

    # put it all together in the format prodigy expects
    combined_df = combined_df.drop(columns=["title", "author", "pub_date"])
    combined_df.rename(
        columns={"text": "text", "image_path": "image_path"}, inplace=True
    )

    return combined_df


# apply the function to the test set
combined_data = combine_data(
    "data/poem_testset_pages.jsonl.gz", "data/poem-testset-meta.csv"
)
print(len(combined_data))
combined_data.head()

"""what here still needs to happen is the selection of the actual poem-focused test set, now we're using ALL the images from the works!"""

# prep data for prodigy
combined_data.to_json("data/testset-db.jsonl", orient="records", lines=True)

"""we will run Prodigy for this data, using a custom recipe to annotate the poems
this is the recipe (also stored seperately as a .py file):



```python
import prodigy
from prodigy.components.loaders import JSONL
import spacy

@prodigy.recipe("ppa_poetry_annotation")
def ppa_poetry_annotation(dataset: str, source: str, labels: str):
    nlp = spacy.blank("en")  # use blank spaCy model for tokenization
    stream = JSONL(source)  # load jsonlines into stream

    def tokenize_stream(stream):
        for task in stream:
            if task.get("text"):
                doc = nlp(task["text"])
                task["tokens"] = [
                    {"text": token.text, "start": token.idx, "end": token.idx + len(token.text), "id": i}
                    for i, token in enumerate(doc)
                ]
            # point to image server
            if 'image_path' in task:
                task['image_path'] = f"http://localhost:8000/{task['image_path']}"
            yield task

    tokenized_stream = tokenize_stream(stream)

    # split labels by commas and strip any whitespace
    label_list = [label.strip() for label in labels.split(",")]

    blocks = [
        {"view_id": "html", "html_template": "<img src='{{image_path}}' width='500'>"},
        {"view_id": "spans_manual", "labels": label_list},
    ]

    return {
        "dataset": dataset,
        "stream": tokenized_stream,
        "view_id": "blocks",
        "config": {
            "blocks": blocks,
            "labels": label_list,
            "show_flag": True,  # show flag button to mark weird/difficult examples
            "hide_newlines": False,  # ensure newlines are shown
            "allow_newline_highlight": True,  # allow highlighting newlines
            "honor_token_whitespace": True,  # reflect whitespace accurately
            "custom_theme": {
                "labels": {
                    "POETRY": "#FFA500",  # label color for POETRY
                    "PROSODY": "#00BFFF"  # label color for PROSODY
                },
                "hide_true_newline_tokens": False
            }
        }
    }
```

now move to the terminal and run the following command to start the annotation task

run this in terminal: `prodigy ppa_poetry_annotation poetry_dataset data/testset-db.jsonl "POETRY,PROSODIC CONCEPT" -F annotate_poetry.py`

when done: `prodigy db-out poetry_dataset > annotations.jsonl`
"""

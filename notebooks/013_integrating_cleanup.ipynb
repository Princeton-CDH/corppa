{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating cleanup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ryanheuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys;sys.path.append('..')\n",
    "from ppanlp.cleanup import *\n",
    "from ppanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_agnostic(txt):\n",
    "    return re.findall(r\"[\\w']+|[.,!?; -—–'\\n]\", txt)\n",
    "\n",
    "def untokenize_agnostic(l):\n",
    "    return ''.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"Oh no I can't poss-ibly———---- ! ££3eee-e\"\n",
    "assert untokenize_agnostic(tokenize_agnostic(s)) == s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_str(txt, use_nltk_tokenizer=False, **page_attrs):\n",
    "    page_text = txt\n",
    "    # dicts to store specific corrections and their counts\n",
    "    specific_ocr_corrections = []\n",
    "    specific_linebreak_corrections = []\n",
    "    specific_long_s_corrections = []\n",
    "    correction_rules = load_correction_rules()\n",
    "    clever_f_s_hack_rules = load_f_s_hack_corrections()\n",
    "\n",
    "    # add a dictionary for specific f ſ hack corrections\n",
    "    specific_f_s_hack_corrections = []\n",
    "\n",
    "    # counters for corrections\n",
    "    linebreak_corrections = 0\n",
    "    ocr_corrections = 0\n",
    "    long_s_corrections = 0\n",
    "    f_s_word_replacements = 0\n",
    "\n",
    "    # rejoin line breaks before tokenization and log corrections\n",
    "    page_text, corrections = rejoin_linebreaks(page_text, specific_linebreak_corrections)\n",
    "    linebreak_corrections += corrections\n",
    "\n",
    "    # apply correction for long 's'\n",
    "    corrected_text, corrections = replace_historic_long_s(page_text, specific_long_s_corrections)\n",
    "    long_s_corrections += corrections\n",
    "    page_text = corrected_text\n",
    "\n",
    "    # tokenization\n",
    "    tokens = word_tokenize(page_text) if use_nltk_tokenizer else tokenize_agnostic(page_text)\n",
    "\n",
    "    # apply OCR corrections on tokens and log corrections\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in correction_rules:\n",
    "            corrected_token = correction_rules[token]\n",
    "            ocr_corrections += 1\n",
    "            specific_ocr_corrections.append((token,corrected_token))\n",
    "        else:\n",
    "            corrected_token = token\n",
    "        corrected_tokens.append(corrected_token)\n",
    "\n",
    "    # apply f-ſ-s hack corrections on tokens and log corrections\n",
    "    for i, token in enumerate(corrected_tokens):\n",
    "        if token in clever_f_s_hack_rules:\n",
    "            corrected_token = clever_f_s_hack_rules[token]\n",
    "            f_s_word_replacements += 1\n",
    "            specific_f_s_hack_corrections.append((token,corrected_token))\n",
    "            corrected_tokens[i] = corrected_token\n",
    "\n",
    "    token_count = len(corrected_tokens)\n",
    "\n",
    "    # convert corrected tokens back to text for further processing\n",
    "    corrected_text = untokenize(corrected_tokens) if use_nltk_tokenizer else untokenize_agnostic(corrected_tokens)\n",
    "\n",
    "    corrected_tokens_real = [x for x in corrected_tokens if any(y.isalpha() for y in x)]\n",
    "\n",
    "    # create output dictionary\n",
    "    def as_counts(l):\n",
    "        return l\n",
    "        # return dict(Counter(l))\n",
    "\n",
    "    return {\n",
    "        'page_text':page_text, \n",
    "        **page_attrs, \n",
    "        'page_text_clean':corrected_text, \n",
    "        # 'page_num_tokens':token_count,\n",
    "        'page_tokens':corrected_tokens_real,\n",
    "        'corrections': {\n",
    "            'headers':as_counts(page_attrs.get('corrections',{}).get('headers',[])),\n",
    "            'ocr':as_counts(specific_ocr_corrections),\n",
    "            'linebreaks':as_counts(specific_linebreak_corrections),\n",
    "            'long_s':as_counts(specific_long_s_corrections),\n",
    "            'f_s':as_counts(specific_f_s_hack_corrections),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_text': 'Paradife lost defigned a——___---_-- paradife',\n",
       " 'page_text_clean': 'Paradise lost designed a——___---_-- paradise',\n",
       " 'page_tokens': ['Paradise', 'lost', 'designed', 'a', 'paradise'],\n",
       " 'corrections': {'headers': [],\n",
       "  'ocr': [('defigned', 'designed'), ('paradife', 'paradise')],\n",
       "  'linebreaks': [('de-\\nfigned', 'defigned')],\n",
       "  'long_s': [('loſt', 'lost')],\n",
       "  'f_s': [('Paradife', 'Paradise')]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_str('Paradife loſt de-\\nfigned a——___---_-- paradife')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PPACorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = corpus.text('mdp.39015019158776').pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_page(page_d):\n",
    "    txt=page_d.get('page_text_clean', page_d.get('page_text',''))\n",
    "    odx=cleanup_str(txt, **page_d)\n",
    "    return odx\n",
    "\n",
    "def cleanup_pages(pages_ld):\n",
    "    if type(pages_ld) == pd.DataFrame: pages_ld=pages_ld.to_dict('records')\n",
    "    pages_ld = process_headers(pages_ld, remove_headers=True) # ideally, we want to set this later when calling the function\n",
    "    pages_ld = [cleanup_page(page_d) for page_d in pages_ld]\n",
    "    return pages_ld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup_pages(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11493/1893144 [03:03<6:39:05, 78.58it/s]"
     ]
    }
   ],
   "source": [
    "save_plaintext_mini_corpus_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

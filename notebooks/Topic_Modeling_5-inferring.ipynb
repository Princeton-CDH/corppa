{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5gXNfcVjvDc"
      },
      "source": [
        "# Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C36S18zo8w3b"
      },
      "outputs": [],
      "source": [
        "# !pip install orjson sqlitedict tomotopy nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import orjson\n",
        "import zlib\n",
        "import tomotopy as tp\n",
        "from sqlitedict import SqliteDict\n",
        "import topicwizard\n",
        "import random\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAu1Sgxsydd",
        "outputId": "bc9aa14d-c79e-482a-fbfb-a97d7c65b968"
      },
      "outputs": [],
      "source": [
        "# corpus\n",
        "path_corpus=os.path.expanduser('~/ppa_data/solrcorpus2')\n",
        "path_metadata = os.path.join(path_corpus, 'metadata.csv')\n",
        "path_pages = os.path.join(path_corpus, 'corpus.sqlitedict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TeT1qRuDtPtn",
        "outputId": "ad89b624-cfc7-4c98-9fcf-c743aa8542d8"
      },
      "outputs": [],
      "source": [
        "# Read metadata\n",
        "# df_metadata = pd.read_csv(path_metadata).fillna('').set_index('work_id')\n",
        "# df_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pcRHm7iet_jz"
      },
      "outputs": [],
      "source": [
        "def encode_cache(x): return sqlite3.Binary(zlib.compress(orjson.dumps(x)))\n",
        "def decode_cache(x): return orjson.loads(zlib.decompress(bytes(x)))\n",
        "def get_pages_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='texts', encode=encode_cache, decode=decode_cache)\n",
        "def get_meta_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='metadata', encode=encode_cache, decode=decode_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "CLUSTER_KEY='cluster_id_s'\n",
        "\n",
        "def iter_pages(lim=None,min_num_words=None,max_pages_per_doc=None,max_pages_per_cluster=None, collections={}):\n",
        "    num=0\n",
        "    clustercounts=Counter()\n",
        "    breaknow=False\n",
        "    with get_pages_db() as db, get_meta_db() as mdb:\n",
        "        for work_id in tqdm(list(db.keys()),desc='Iterating works',position=0):\n",
        "            if breaknow: break\n",
        "\n",
        "            meta = mdb[work_id]\n",
        "            if collections and not set(meta['collections']) & set(collections):\n",
        "                continue\n",
        "            pages = db[work_id]\n",
        "            cluster = meta.get(CLUSTER_KEY,work_id)\n",
        "\n",
        "            if min_num_words:\n",
        "                pages = [d for d in pages if len(d['page_tokens'])>=min_num_words]\n",
        "\n",
        "            if max_pages_per_doc:\n",
        "                random.shuffle(pages)\n",
        "                pages=pages[:max_pages_per_doc]\n",
        "\n",
        "            pbar2=tqdm(pages,desc='Iterating pages',position=1,disable=True)\n",
        "            for page in pbar2:\n",
        "                if not max_pages_per_cluster or clustercounts[cluster]<max_pages_per_cluster:\n",
        "                    yield dict(\n",
        "                        work_cluster = cluster,\n",
        "                        **page\n",
        "                    )\n",
        "                    clustercounts[cluster]+=1\n",
        "                    num+=1\n",
        "                    if lim and num>=lim:\n",
        "                        breaknow=True\n",
        "                        break\n",
        "            pbar2.close()\n",
        "\n",
        "def iter_corpus(lim=None,max_pages_per_doc=25,**kwargs):\n",
        "    yield from iter_pages(lim=lim,min_num_words=25,collections={'Literary','Linguistic'},max_pages_per_doc=max_pages_per_doc,**kwargs)\n",
        "\n",
        "def iter_sample(lim=None):\n",
        "    yield from iter_corpus(lim=lim, max_pages_per_cluster=25, max_pages_per_doc=25)\n",
        "\n",
        "# next(iter_pages(collections=['Linguistic']))\n",
        "# for x in iter_pages(max_pages_per_cluster=1): pass\n",
        "# for i,x in enumerate(iter_corpus()): pass\n",
        "# i\n",
        "# next(iter_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords as stops\n",
        "stopwords = set(stops.words('english'))\n",
        "def clean_toks(toks):\n",
        "    return [tok for tok in toks if len(tok)>3 and tok not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topic_model(ntopic=50, force=False, niter=100):\n",
        "    fn=f'data.tomotopy.model.ntopic={ntopic}.bin'\n",
        "    fnindex=fn+'.index.json'\n",
        "    if force or not os.path.exists(fn) or not os.path.exists(fnindex):\n",
        "        mdl = tp.LDAModel(k=50)\n",
        "        docd={}\n",
        "        for page in iter_sample():\n",
        "            toks = clean_toks(page['page_tokens'])\n",
        "            docd[page['page_id']] = mdl.add_doc(toks)\n",
        "\n",
        "        def getdesc():\n",
        "            return f'Training model (ndocs={len(docd)}, log-likelihood = {mdl.ll_per_word:.4})')\n",
        "        pbar=tqdm(list(range(0, niter, 10)),desc=getdesc(),position=0)\n",
        "        for i in pbar:\n",
        "            pbar.set_description(getdesc())\n",
        "            mdl.train(10)\n",
        "        mdl.save(fn)\n",
        "        with open(fnindex,'wb') as of:\n",
        "            of.write(orjson.dumps(docd))\n",
        "    else:\n",
        "        mdl = tp.LDAModel.load(fn)\n",
        "        with open(fnindex,'rb') as f:\n",
        "            docd=orjson.loads(f.read())\n",
        "\n",
        "    mdl.summary()\n",
        "    return mdl,docd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating works: 100%|██████████| 6319/6319 [03:14<00:00, 32.46it/s]\n",
            "Training model (ndocs=102248, log-likelihood = -10.4): 100%|██████████| 10/10 [02:29<00:00, 14.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Basic Info>\n",
            "| LDAModel (current version: 0.12.5)\n",
            "| 102993 docs, 14788350 words\n",
            "| Total Vocabs: 1218037, Used Vocabs: 1218037\n",
            "| Entropy of words: 10.02966\n",
            "| Entropy of term-weighted words: 10.02966\n",
            "| Removed Vocabs: <NA>\n",
            "|\n",
            "<Training Info>\n",
            "| Iterations: 100, Burn-in steps: 0\n",
            "| Optimization Interval: 10\n",
            "| Log-likelihood per word: -10.38120\n",
            "|\n",
            "<Initial Parameters>\n",
            "| tw: TermWeight.ONE\n",
            "| min_cf: 0 (minimum collection frequency of words)\n",
            "| min_df: 0 (minimum document frequency of words)\n",
            "| rm_top: 0 (the number of top words to be removed)\n",
            "| k: 50 (the number of topics between 1 ~ 32767)\n",
            "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
            "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
            "| seed: 3097823306 (random seed)\n",
            "| trained in version 0.12.5\n",
            "|\n",
            "<Parameters>\n",
            "| alpha (Dirichlet prior on the per-document topic distributions)\n",
            "|  [0.26227176 0.11506399 0.03285861 0.07679065 0.00923859 0.02513527\n",
            "|   0.08124888 0.02287436 0.05314855 0.00749319 0.08974517 0.05729252\n",
            "|   0.03945235 0.01331402 0.00816904 0.02727689 0.02713322 0.1241152\n",
            "|   0.01238072 0.05581073 0.1470048  0.12732139 0.05423285 0.02434245\n",
            "|   0.0613206  0.09788665 0.13075775 0.09247503 0.08456635 0.03035676\n",
            "|   0.05751467 0.00943628 0.014477   0.01619946 0.00542959 0.03527955\n",
            "|   0.00735967 0.006345   0.00703526 0.01561359 0.06057166 0.03019896\n",
            "|   0.01486184 0.21018605 0.04944267 0.07348954 0.02910629 0.00843257\n",
            "|   0.0265325  0.12212832]\n",
            "| eta (Dirichlet prior on the per-topic word distribution)\n",
            "|  0.01\n",
            "|\n",
            "<Topics>\n",
            "| #0 (1351831) : would must words upon sense\n",
            "| #1 (473429) : like little white tree black\n",
            "| #2 (167407) : part wind rhymes pret tear\n",
            "| #3 (336767) : thou thee shall lord come\n",
            "| #4 (56336) : page read comma colon line\n",
            "| #5 (202466) : quod quam sunt quid esse\n",
            "| #6 (403130) : king years year first great\n",
            "| #7 (170959) : used part water body called\n",
            "| #8 (211943) : church religion christ holy christian\n",
            "| #9 (55001) : cast beat weighed bring pref\n",
            "| #10 (539195) : fame upon much would every\n",
            "| #11 (308767) : english latin french language greek\n",
            "| #12 (277599) : verse line syllables lines syllable\n",
            "| #13 (146659) : dans vous pour plus nous\n",
            "| #14 (60644) : τους bible περί προς note\n",
            "| #15 (179636) : play plays shakespeare stage spenser\n",
            "| #16 (171636) : thing make place state person\n",
            "| #17 (570719) : great poetry style genius much\n",
            "| #18 (86851) : ablative hine genitive dative note\n",
            "| #19 (332358) : edition first book text century\n",
            "| #20 (777040) : love like light heart earth\n",
            "| #21 (518860) : said time came would could\n",
            "| #22 (262907) : john london thomas william printed\n",
            "| #23 (188881) : verb tense present verbs past\n",
            "| #24 (233188) : river water town land ship\n",
            "| #25 (536925) : work english study school language\n",
            "| #26 (805697) : poetry life poet work even\n",
            "| #27 (379937) : blood arms death thus battle\n",
            "| #28 (383709) : words write sentences sentence following\n",
            "| #29 (222938) : sound vowel sounds vowels long\n",
            "| #30 (291395) : words word form pronunciation spelling\n",
            "| #31 (141825) : nicht sich eine auch dass\n",
            "| #32 (79107) : rife life write fill foul\n",
            "| #33 (97092) : tion ness ment sion less\n",
            "| #34 (85898) : speling hwen hwig reform dhat\n",
            "| #35 (138885) : plural woman female wife male\n",
            "| #36 (64119) : spanish para como madrid lope\n",
            "| #37 (58379) : goth prep agus ασur fear\n",
            "| #38 (59802) : tile fully ally kara fron\n",
            "| #39 (155667) : ther alle also whan chaucer\n",
            "| #40 (508077) : noun verb used sentence nouns\n",
            "| #41 (147975) : number three four first times\n",
            "| #42 (67924) : note paris france french dame\n",
            "| #43 (904026) : would good much well know\n",
            "| #44 (304379) : university york english book college\n",
            "| #45 (320671) : voice music tone vocal sound\n",
            "| #46 (161964) : thle fame tile cafe thie\n",
            "| #47 (60844) : part name fide water spring\n",
            "| #48 (93553) : gives able webster senate international\n",
            "| #49 (633353) : would people great country state\n",
            "|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mdl,docd = topic_model(force=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "docd_test={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating works: 100%|██████████| 6319/6319 [03:09<00:00, 33.32it/s]\n"
          ]
        }
      ],
      "source": [
        "num_pages = sum(1 for _ in iter_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating pages:   0%|          | 128/152677 [01:12<24:02:43,  1.76it/s]\n",
            "Iterating works:   0%|          | 6/6319 [01:12<21:11:44, 12.09s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39melif\u001b[39;00m pid \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m docd:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     docd_test[pid] \u001b[39m=\u001b[39m doc \u001b[39m=\u001b[39m mdl\u001b[39m.\u001b[39mmake_doc(clean_toks(page[\u001b[39m'\u001b[39m\u001b[39mpage_tokens\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     mdl\u001b[39m.\u001b[39;49minfer(doc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ryanheuser/github/ppa-nlp/notebooks/Topic_Modeling_5.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     doc \u001b[39m=\u001b[39m mdl\u001b[39m.\u001b[39mdocs[docd[pid]]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# other docs\n",
        "o=[]\n",
        "for page in tqdm(iter_sample(),position=2,desc='Iterating all pages', total=num_pages):\n",
        "    pid = page['page_id']\n",
        "    if pid in docd_test:\n",
        "        doc = docd_test[pid]\n",
        "    elif pid not in docd:\n",
        "        docd_test[pid] = doc = mdl.make_doc(clean_toks(page['page_tokens']))\n",
        "        mdl.infer(doc)\n",
        "    else:\n",
        "        doc = mdl.docs[docd[pid]]\n",
        "    o.append(pd.Series(doc.get_topic_dist(), name=pid))\n",
        "odf=pd.DataFrame(o).rename_axis('page_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>page_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>uc1.$b305400_iii</th>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.282014</td>\n",
              "      <td>0.000486</td>\n",
              "      <td>0.001371</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.004875</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uc1.$b305400_v</th>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.114077</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.009701</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.018845</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uc1.$b305400_vi</th>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.054625</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.067287</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uc1.$b305400_vii</th>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.059264</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.006668</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.001329</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uc1.$b305400_viii</th>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.110961</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011584</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         0         1         2         3         4         5   \\\n",
              "page_id                                                                         \n",
              "uc1.$b305400_iii   0.000228  0.000153  0.006725  0.000124  0.282014  0.000486   \n",
              "uc1.$b305400_v     0.000091  0.000061  0.114077  0.000049  0.009701  0.000193   \n",
              "uc1.$b305400_vi    0.000064  0.000043  0.054625  0.000035  0.000296  0.000137   \n",
              "uc1.$b305400_vii   0.000062  0.000042  0.059264  0.000034  0.006668  0.000132   \n",
              "uc1.$b305400_viii  0.000105  0.000071  0.110961  0.000057  0.000484  0.000224   \n",
              "\n",
              "                         6         7         8         9   ...        40  \\\n",
              "page_id                                                    ...             \n",
              "uc1.$b305400_iii   0.001371  0.000698  0.000856  0.000194  ...  0.001732   \n",
              "uc1.$b305400_v     0.000543  0.018845  0.000339  0.000077  ...  0.000687   \n",
              "uc1.$b305400_vi    0.000386  0.000197  0.000241  0.000055  ...  0.000488   \n",
              "uc1.$b305400_vii   0.000374  0.000190  0.000233  0.000053  ...  0.000472   \n",
              "uc1.$b305400_viii  0.000631  0.000322  0.000394  0.000089  ...  0.011584   \n",
              "\n",
              "                         41        42        43        44        45        46  \\\n",
              "page_id                                                                         \n",
              "uc1.$b305400_iii   0.000150  0.004875  0.000163  0.000159  0.003068  0.000161   \n",
              "uc1.$b305400_v     0.000059  0.001933  0.000065  0.000063  0.001217  0.000064   \n",
              "uc1.$b305400_vi    0.000042  0.067287  0.000046  0.000045  0.000864  0.000045   \n",
              "uc1.$b305400_vii   0.000041  0.001329  0.000044  0.000043  0.000836  0.000044   \n",
              "uc1.$b305400_viii  0.000069  0.002246  0.000075  0.000073  0.001413  0.000074   \n",
              "\n",
              "                         47        48        49  \n",
              "page_id                                          \n",
              "uc1.$b305400_iii   0.000534  0.000806  0.000478  \n",
              "uc1.$b305400_v     0.000212  0.000320  0.000190  \n",
              "uc1.$b305400_vi    0.000150  0.000227  0.000135  \n",
              "uc1.$b305400_vii   0.000146  0.000220  0.000130  \n",
              "uc1.$b305400_viii  0.000246  0.000371  0.000220  \n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "odf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.00114124, 0.00199177, 0.00821502, 0.03095148, 0.01164062,\n",
              "       0.00874124, 0.00112182, 0.01039761, 0.00265881, 0.00264824,\n",
              "       0.00590874, 0.01194252, 0.00747224, 0.00148831, 0.0172281 ,\n",
              "       0.00588976, 0.01296221, 0.00131755, 0.01865524, 0.00094265,\n",
              "       0.00167942, 0.00830613, 0.00193604, 0.00270271, 0.0010566 ,\n",
              "       0.00086996, 0.00100074, 0.00289788, 0.00132043, 0.00145429,\n",
              "       0.00621457, 0.00326153, 0.00111377, 0.00592177, 0.00106186,\n",
              "       0.73067397, 0.00122145, 0.01505502, 0.00167707, 0.00206258,\n",
              "       0.00118021, 0.00091567, 0.00350079, 0.00093837, 0.00772377,\n",
              "       0.01788062, 0.00345174, 0.00479487, 0.00398178, 0.00082921],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.get_topic_dist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc=mdl.docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(33, 0.8941167593002319),\n",
              " (46, 0.03659585118293762),\n",
              " (47, 0.007205671165138483),\n",
              " (15, 0.005247470922768116),\n",
              " (13, 0.0050822049379348755),\n",
              " (28, 0.004184012766927481),\n",
              " (4, 0.0032977887894958258),\n",
              " (25, 0.0030990480445325375),\n",
              " (9, 0.002993296831846237),\n",
              " (2, 0.0025174636393785477)]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.get_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5gXNfcVjvDc"
      },
      "source": [
        "# Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "C36S18zo8w3b"
      },
      "outputs": [],
      "source": [
        "# !pip install -U orjson sqlitedict tomotopy nltk pyLDAvis altair ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import orjson\n",
        "import zlib\n",
        "import tomotopy as tp\n",
        "from sqlitedict import SqliteDict\n",
        "import topicwizard\n",
        "import random\n",
        "import pyLDAvis\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords as stops\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAu1Sgxsydd",
        "outputId": "bc9aa14d-c79e-482a-fbfb-a97d7c65b968"
      },
      "outputs": [],
      "source": [
        "# corpus\n",
        "path_corpus=os.path.expanduser('~/ppa_data/solrcorpus2')\n",
        "path_metadata = os.path.join(path_corpus, 'metadata.csv')\n",
        "path_pages = os.path.join(path_corpus, 'corpus.sqlitedict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TeT1qRuDtPtn",
        "outputId": "ad89b624-cfc7-4c98-9fcf-c743aa8542d8"
      },
      "outputs": [],
      "source": [
        "# Read metadata\n",
        "# df_metadata = pd.read_csv(path_metadata).fillna('').set_index('work_id')\n",
        "# df_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pcRHm7iet_jz"
      },
      "outputs": [],
      "source": [
        "def encode_cache(x): return sqlite3.Binary(zlib.compress(orjson.dumps(x)))\n",
        "def decode_cache(x): return orjson.loads(zlib.decompress(bytes(x)))\n",
        "def get_pages_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='texts', encode=encode_cache, decode=decode_cache)\n",
        "def get_meta_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='metadata', encode=encode_cache, decode=decode_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "CLUSTER_KEY='cluster_id_s'\n",
        "\n",
        "def iter_pages(lim=None,min_num_words=None,max_pages_per_doc=None,max_pages_per_cluster=None, collections={}):\n",
        "    num=0\n",
        "    clustercounts=Counter()\n",
        "    breaknow=False\n",
        "    with get_pages_db() as db, get_meta_db() as mdb:\n",
        "        for work_id in tqdm(list(db.keys()),desc='Iterating works',position=0):\n",
        "            if breaknow: break\n",
        "\n",
        "            meta = mdb[work_id]\n",
        "            if collections and not set(meta['collections']) & set(collections):\n",
        "                continue\n",
        "            pages = db[work_id]\n",
        "            cluster = meta.get(CLUSTER_KEY,work_id)\n",
        "\n",
        "            if min_num_words:\n",
        "                pages = [d for d in pages if len(d['page_tokens'])>=min_num_words]\n",
        "\n",
        "            if max_pages_per_doc:\n",
        "                random.shuffle(pages)\n",
        "                pages=pages[:max_pages_per_doc]\n",
        "\n",
        "            pbar2=tqdm(pages,desc='Iterating pages',position=1,disable=True)\n",
        "            for page in pbar2:\n",
        "                if not max_pages_per_cluster or clustercounts[cluster]<max_pages_per_cluster:\n",
        "                    yield dict(\n",
        "                        work_cluster = cluster,\n",
        "                        **page\n",
        "                    )\n",
        "                    clustercounts[cluster]+=1\n",
        "                    num+=1\n",
        "                    if lim and num>=lim:\n",
        "                        breaknow=True\n",
        "                        break\n",
        "            pbar2.close()\n",
        "\n",
        "def iter_corpus(lim=None,max_pages_per_doc=25,**kwargs):\n",
        "    yield from iter_pages(lim=lim,min_num_words=25,collections={'Literary','Linguistic'},max_pages_per_doc=max_pages_per_doc,**kwargs)\n",
        "\n",
        "def iter_sample(lim=None):\n",
        "    yield from iter_corpus(lim=lim, max_pages_per_cluster=25, max_pages_per_doc=25)\n",
        "\n",
        "# next(iter_pages(collections=['Linguistic']))\n",
        "# for x in iter_pages(max_pages_per_cluster=1): pass\n",
        "# for i,x in enumerate(iter_corpus()): pass\n",
        "# i\n",
        "# next(iter_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stopwords = set(stops.words('english'))\n",
        "def clean_toks(toks):\n",
        "    return [tok for tok in toks if len(tok)>3 and tok not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topic_model(ntopic=50, force=False, niter=100):\n",
        "    fn=f'data.tomotopy.model.ntopic={ntopic}.bin'\n",
        "    fnindex=fn+'.index.json'\n",
        "    if force or not os.path.exists(fn) or not os.path.exists(fnindex):\n",
        "        mdl = tp.LDAModel(k=50)\n",
        "        docd={}\n",
        "        for page in iter_sample():\n",
        "            toks = clean_toks(page['page_tokens'])\n",
        "            docd[page['page_id']] = mdl.add_doc(toks)\n",
        "\n",
        "        def getdesc():\n",
        "            return f'Training model (ndocs={len(docd)}, log-likelihood = {mdl.ll_per_word:.4})'\n",
        "        pbar=tqdm(list(range(0, niter, 10)),desc=getdesc(),position=0)\n",
        "        for i in pbar:\n",
        "            pbar.set_description(getdesc())\n",
        "            mdl.train(10)\n",
        "        mdl.save(fn)\n",
        "        with open(fnindex,'wb') as of:\n",
        "            of.write(orjson.dumps(docd))\n",
        "    else:\n",
        "        print(f'Loading model: {fn}')\n",
        "        mdl = tp.LDAModel.load(fn)\n",
        "        print(f'Loading model index: {fnindex}')\n",
        "        with open(fnindex,'rb') as f:\n",
        "            docd=orjson.loads(f.read())\n",
        "\n",
        "    mdl.summary()\n",
        "    return mdl,docd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: data.tomotopy.model.ntopic=50.bin\n",
            "Loading model index: data.tomotopy.model.ntopic=50.bin.index.json\n",
            "<Basic Info>\n",
            "| LDAModel (current version: 0.12.5)\n",
            "| 102993 docs, 14788350 words\n",
            "| Total Vocabs: 1218037, Used Vocabs: 1218037\n",
            "| Entropy of words: 10.02966\n",
            "| Entropy of term-weighted words: 10.02966\n",
            "| Removed Vocabs: <NA>\n",
            "|\n",
            "<Training Info>\n",
            "| Iterations: 100, Burn-in steps: 0\n",
            "| Optimization Interval: 10\n",
            "| Log-likelihood per word: -10.38120\n",
            "|\n",
            "<Initial Parameters>\n",
            "| tw: TermWeight.ONE\n",
            "| min_cf: 0 (minimum collection frequency of words)\n",
            "| min_df: 0 (minimum document frequency of words)\n",
            "| rm_top: 0 (the number of top words to be removed)\n",
            "| k: 50 (the number of topics between 1 ~ 32767)\n",
            "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
            "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
            "| seed: 3097823306 (random seed)\n",
            "| trained in version 0.12.5\n",
            "|\n",
            "<Parameters>\n",
            "| alpha (Dirichlet prior on the per-document topic distributions)\n",
            "|  [0.26227176 0.11506399 0.03285861 0.07679065 0.00923859 0.02513527\n",
            "|   0.08124888 0.02287436 0.05314855 0.00749319 0.08974517 0.05729252\n",
            "|   0.03945235 0.01331402 0.00816904 0.02727689 0.02713322 0.1241152\n",
            "|   0.01238072 0.05581073 0.1470048  0.12732139 0.05423285 0.02434245\n",
            "|   0.0613206  0.09788665 0.13075775 0.09247503 0.08456635 0.03035676\n",
            "|   0.05751467 0.00943628 0.014477   0.01619946 0.00542959 0.03527955\n",
            "|   0.00735967 0.006345   0.00703526 0.01561359 0.06057166 0.03019896\n",
            "|   0.01486184 0.21018605 0.04944267 0.07348954 0.02910629 0.00843257\n",
            "|   0.0265325  0.12212832]\n",
            "| eta (Dirichlet prior on the per-topic word distribution)\n",
            "|  0.01\n",
            "|\n",
            "<Topics>\n",
            "| #0 (1351831) : would must words upon sense\n",
            "| #1 (473429) : like little white tree black\n",
            "| #2 (167407) : part wind rhymes pret tear\n",
            "| #3 (336767) : thou thee shall lord come\n",
            "| #4 (56336) : page read comma colon line\n",
            "| #5 (202466) : quod quam sunt quid esse\n",
            "| #6 (403130) : king years year first great\n",
            "| #7 (170959) : used part water body called\n",
            "| #8 (211943) : church religion christ holy christian\n",
            "| #9 (55001) : cast beat weighed bring pref\n",
            "| #10 (539195) : fame upon much would every\n",
            "| #11 (308767) : english latin french language greek\n",
            "| #12 (277599) : verse line syllables lines syllable\n",
            "| #13 (146659) : dans vous pour plus nous\n",
            "| #14 (60644) : τους bible περί προς note\n",
            "| #15 (179636) : play plays shakespeare stage spenser\n",
            "| #16 (171636) : thing make place state person\n",
            "| #17 (570719) : great poetry style genius much\n",
            "| #18 (86851) : ablative hine genitive dative note\n",
            "| #19 (332358) : edition first book text century\n",
            "| #20 (777040) : love like light heart earth\n",
            "| #21 (518860) : said time came would could\n",
            "| #22 (262907) : john london thomas william printed\n",
            "| #23 (188881) : verb tense present verbs past\n",
            "| #24 (233188) : river water town land ship\n",
            "| #25 (536925) : work english study school language\n",
            "| #26 (805697) : poetry life poet work even\n",
            "| #27 (379937) : blood arms death thus battle\n",
            "| #28 (383709) : words write sentences sentence following\n",
            "| #29 (222938) : sound vowel sounds vowels long\n",
            "| #30 (291395) : words word form pronunciation spelling\n",
            "| #31 (141825) : nicht sich eine auch dass\n",
            "| #32 (79107) : rife life write fill foul\n",
            "| #33 (97092) : tion ness ment sion less\n",
            "| #34 (85898) : speling hwen hwig reform dhat\n",
            "| #35 (138885) : plural woman female wife male\n",
            "| #36 (64119) : spanish para como madrid lope\n",
            "| #37 (58379) : goth prep agus ασur fear\n",
            "| #38 (59802) : tile fully ally kara fron\n",
            "| #39 (155667) : ther alle also whan chaucer\n",
            "| #40 (508077) : noun verb used sentence nouns\n",
            "| #41 (147975) : number three four first times\n",
            "| #42 (67924) : note paris france french dame\n",
            "| #43 (904026) : would good much well know\n",
            "| #44 (304379) : university york english book college\n",
            "| #45 (320671) : voice music tone vocal sound\n",
            "| #46 (161964) : thle fame tile cafe thie\n",
            "| #47 (60844) : part name fide water spring\n",
            "| #48 (93553) : gives able webster senate international\n",
            "| #49 (633353) : would people great country state\n",
            "|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mdl,docd = topic_model(force=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_pyldavis():\n",
        "    print('Calculating topic_term_dists')\n",
        "    topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
        "\n",
        "\n",
        "    print('Calculating doc_topic_dists')\n",
        "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
        "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
        "\n",
        "    print('Calculating doc_lengths')\n",
        "    doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
        "\n",
        "\n",
        "    print('Calculating vocab')\n",
        "    vocab = list(mdl.used_vocabs)\n",
        "    term_frequency = mdl.used_vocab_freq\n",
        "\n",
        "    print('preparing data')\n",
        "    prepared_data = pyLDAvis.prepare(\n",
        "        topic_term_dists, \n",
        "        doc_topic_dists, \n",
        "        doc_lengths, \n",
        "        vocab, \n",
        "        term_frequency,\n",
        "        start_index=0, # tomotopy starts topic ids with 0, pyLDAvis with 1\n",
        "        sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
        "    )\n",
        "\n",
        "    print('saving html')\n",
        "    pyLDAvis.save_html(prepared_data, 'ldavis.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "!open ldavis.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102993"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_topic_dists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dfclust():\n",
        "    id2doc={v:k for k,v in docd.items()}\n",
        "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
        "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
        "    index,values = zip(*[(id2doc[i],x) for i,x in enumerate(doc_topic_dists) if i in id2doc])\n",
        "    dftopicdist = pd.DataFrame(values, index=index)\n",
        "    with get_meta_db() as mdb:\n",
        "        dfmeta = pd.DataFrame({'work_id':wid, **mdb[wid]} for wid in tqdm(mdb, total=len(mdb), position=0, desc='Gathering metadata')).set_index('work_id')\n",
        "    w2c = dict(zip(dfmeta.index, dfmeta[CLUSTER_KEY]))\n",
        "    dftopicdist['work_id']=[i.split('_')[0] for i in dftopicdist.index]\n",
        "    dftopicdist['cluster']=[w2c.get(work_id,work_id) for work_id in dftopicdist.work_id]\n",
        "    dfclust_avgs=dftopicdist.groupby('cluster').mean(numeric_only=True)\n",
        "    dfclust_meta = dfmeta.drop_duplicates(CLUSTER_KEY).set_index(CLUSTER_KEY)\n",
        "    return dfclust_meta.join(dfclust_avgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gathering metadata: 100%|██████████| 6319/6319 [00:02<00:00, 2843.98it/s]\n"
          ]
        }
      ],
      "source": [
        "dfclust = get_dfclust()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "from ipywidgets import interactive, interact, interact_manual, HBox\n",
        "from functools import cache\n",
        "\n",
        "tnums=list(range(mdl.k))\n",
        "topicwords_d = {tnum:', '.join([i for i,j in mdl.get_topic_words(tnum)]) for tnum in tnums}\n",
        "topicnames = [f'{tnum}: {topicwords_d[tnum]}' for tnum in tnums]\n",
        "\n",
        "def get_topic_name(tnum):\n",
        "    return topicnames[tnum]\n",
        "\n",
        "def get_wordcloud(tnum):\n",
        "    wc = WordCloud(background_color='white', width=800, height=400)\n",
        "    wordcloud = wc.generate_from_frequencies(dict(mdl.get_topic_words(tnum, top_n=100)))\n",
        "    return wordcloud\n",
        "\n",
        "@cache\n",
        "def get_figdf(tnum):\n",
        "    collections={'Linguistic','Literary'}\n",
        "    figdf=dfclust.reset_index()[[CLUSTER_KEY,'title','author','pub_date','publisher','pub_place','source_url', 'collections', tnum]]\n",
        "    figdf['collections']=[[x for x in c if x in collections] for c in figdf.collections]\n",
        "    figdf['collections'] = figdf['collections'].apply(lambda x: 'Linguistic' if 'Linguistic' in set(x) else (x[0] if x else x))\n",
        "    figdf=figdf[figdf.collections.apply(bool)]\n",
        "    figdf.columns = ['cluster', 'title', 'author', 'date', 'publisher', 'pubplace', 'source', 'genre', 'topic']\n",
        "    figdf = figdf[1700<=figdf.date]\n",
        "    return figdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0: would, must, words, upon, sense, first, without, general, mind, thus'"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_topic_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%timeit\n",
        "# get_figdf(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @interact(tname=topicnames)\n",
        "def plot_topic(tname):\n",
        "    tnum=int(tname.split(':')[0])\n",
        "    figdf=get_figdf(tnum)\n",
        "    topicwords = topicwords_d.get(tnum)\n",
        "    wordcloud=get_wordcloud(tnum)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    \n",
        "    fig = alt.Chart(figdf).mark_circle(size=60).encode(\n",
        "        x = alt.X('date', scale=alt.Scale(domain=[1700, 1920])),\n",
        "        y='topic',\n",
        "        color='genre',\n",
        "        tooltip=figdf.columns.tolist()\n",
        "    ).interactive(\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=400,\n",
        "    # ).facet(\n",
        "    #     facet='genre:N',\n",
        "    #     columns=2,\n",
        "    # ).resolve_scale(\n",
        "    #     y='independent'\n",
        "    ).properties(\n",
        "        title = f'Topic {tnum}: {topicwords}'\n",
        "    )\n",
        "    return fig "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install plotnine\n",
        "import plotnine as p9\n",
        "p9.options.figure_size=(8,4)\n",
        "\n",
        "def plot_topic_min(tnum):\n",
        "    figdf=get_figdf(tnum)\n",
        "    figdf['period']=figdf['date'].apply(str)\n",
        "    figdf=figdf.groupby(['genre','period']).median(numeric_only=True).reset_index()\n",
        "    figdf=pd.concat(\n",
        "        gdf.assign(topic=gdf.topic.rolling(10).mean())\n",
        "        for g,gdf in figdf.groupby('genre')\n",
        "    )\n",
        "    fig=p9.ggplot(figdf, p9.aes(x='date',y='topic',color='genre'))\n",
        "    fig+=p9.geom_point()\n",
        "    fig+=p9.geom_smooth(method='loess')\n",
        "    fig+=p9.labs(\n",
        "        title=get_topic_name(tnum),\n",
        "        x='Date of publication',\n",
        "        y='Prevalence of Topic'\n",
        "    )\n",
        "    fig+=p9.theme_classic()\n",
        "    odir='timeplots'\n",
        "    os.makedirs(odir,exist_ok=True)\n",
        "    fig.save(f'{odir}/fig.timeplot.tnum={tnum}.png')\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_topic_min(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for tnum in range(mdl.k): plot_topic_min(tnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def get_cluster_name(clustid):\n",
        "    meta = dict(dfclust_meta.loc[clustid])\n",
        "    return f'{meta[\"title\"].strip(punctuation)[:50]} ({str(meta[\"pub_date\"])[:4]}) [{meta[\"source_url\"]}]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The imperial encyclopaedic dictionary (1901) [https://hdl.handle.net/2027/mdp.39015050663247]'"
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_cluster_name('mdp.39015050663247')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_wordclouds(odir='wordclouds'):\n",
        "    my_dpi=75\n",
        "    os.makedirs(odir, exist_ok=True)\n",
        "    for tnum in tqdm(list(range(mdl.k)), desc='Saving wordclouds'):\n",
        "        wordcloud=get_wordcloud(tnum)\n",
        "        plt.box(False)\n",
        "        plt.figure(figsize=(600/my_dpi, 200/my_dpi), dpi=my_dpi)\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.savefig(f'{odir}/fig.wordcloud.tnum={tnum}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving wordclouds:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "save_wordclouds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [],
      "source": [
        "def geturl(tnum):\n",
        "    return f'https://raw.githubusercontent.com/Princeton-CDH/ppa-nlp/develop/notebooks/wordclouds/fig.wordcloud.tnum%3D{tnum}.png'\n",
        "\n",
        "def geturl2(tnum):\n",
        "    return f'https://raw.githubusercontent.com/Princeton-CDH/ppa-nlp/develop/notebooks/timeplots/fig.timeplot.tnum%3D{tnum}.png'\n",
        "\n",
        "\n",
        "def get_topic_info_df():\n",
        "    tld=[]\n",
        "    for tnum in tqdm(list(range(mdl.k)), desc='Gathering info on topics'):\n",
        "        td={\n",
        "            'Topic':tnum,\n",
        "            'Topic Name':'',\n",
        "            'Top Words':', '.join([i for i,j in mdl.get_topic_words(tnum, top_n=50)]),\n",
        "            'Top Documents':'* '+('\\n* '.join(get_cluster_name(c) for c in dfclust.sort_values(tnum,ascending=False).index[:5])),\n",
        "            'Word Cloud':f'=IMAGE(\"{geturl(tnum)}\")',\n",
        "            'Historical Plot':f'=IMAGE(\"{geturl2(tnum)}\")',\n",
        "        }\n",
        "        tld.append(td)\n",
        "    tdf=pd.DataFrame(tld).set_index('Topic')\n",
        "    return tdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gathering info on topics: 100%|██████████| 50/50 [00:01<00:00, 30.21it/s]\n"
          ]
        }
      ],
      "source": [
        "tdf=get_topic_info_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /Users/ryanheuser/.pyenv/versions/3.10.7/lib/python3.10/site-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /Users/ryanheuser/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install openpyxl\n",
        "tdf.to_excel('data.topic_info.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

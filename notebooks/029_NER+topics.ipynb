{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys;sys.path.append('..')\n",
    "from ppanlp import *\n",
    "ppa = PPA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "class NERModel:\n",
    "    def __init__(self, corpus=None):\n",
    "        self.corpus = corpus if corpus is not None else PPA()\n",
    "        self.ent2pages=defaultdict(set)\n",
    "        self.ent2count=Counter()\n",
    "        self._annodf=None\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_ent(ent):\n",
    "        o=ent.strip(punctuation).title()\n",
    "        if o.endswith(\"'S\"): o = o[:-2]\n",
    "        return o\n",
    "    \n",
    "    @staticmethod\n",
    "    def iter_by_page(iterr):\n",
    "        last_pageid=None\n",
    "        last_l=[]\n",
    "        for res in iterr:\n",
    "            pageid=res[0]\n",
    "            if last_l and pageid!=last_pageid:\n",
    "                yield (pageid,last_l)\n",
    "                last_l=[]\n",
    "            last_l.append(res[1:])\n",
    "            last_pageid = pageid\n",
    "        if last_l: yield last_l\n",
    "        \n",
    "    def iter_ents(self,ent_types:set=None,lim=None,by_page=False,ents=None):\n",
    "        def iterr():\n",
    "            with self.corpus.ents_db(flag='r') as db:\n",
    "                total=len(db)\n",
    "                iterr=tqdm(db.items(),desc='Iterating over saved ents',position=0,total=total)\n",
    "                for page_id,page_ents in iterr:\n",
    "                    for ent,ent_type in page_ents:\n",
    "                        ent = self.clean_ent(ent)\n",
    "                        if (not ent_types or ent_type in ent_types) and (not ents or ent in ents):\n",
    "                            yield page_id,ent,ent_type\n",
    "        oiterr = (self.iter_by_page(iterr()) if by_page else iterr())\n",
    "        yield from iterlim(oiterr,lim)\n",
    "    \n",
    "                            \n",
    "    def iter_persons(self, **kwargs):\n",
    "        kwargs['ent_types']={'PERSON'}\n",
    "        yield from self.iter_ents(**kwargs)\n",
    "    \n",
    "    def count_ents(self, **kwargs):\n",
    "        self.ent2count=Counter()\n",
    "        for res in self.iter_ents(**kwargs):\n",
    "            page_id,ent = res[:2]\n",
    "            self.ent2pages[ent].add(page_id)\n",
    "            self.ent2count[ent]+=1\n",
    "        self.ent2count_s = pd.Series(self.ent2count).sort_values(ascending=False)\n",
    "        return self.ent2count_s\n",
    "    \n",
    "    def count_persons(self, **kwargs):\n",
    "        kwargs['ent_types']={'PERSON'}\n",
    "        return self.count_ents(**kwargs)\n",
    "    \n",
    "    def prep_anno_df(self, min_count=100):\n",
    "        s = ner.ent2count_s\n",
    "        s = s[s>=min_count]\n",
    "        df = pd.DataFrame({'count':s}).rename_axis('name')\n",
    "        df['is_valid'] = ''\n",
    "        return df\n",
    "    \n",
    "    @cached_property\n",
    "    def path_to_anno(self): return os.path.join(self.corpus.path_data, 'data.ner.to_anno.csv')\n",
    "    @cached_property\n",
    "    def path_anno(self): return os.path.join(self.corpus.path_data, 'data.ner.anno.csv')\n",
    "    \n",
    "    def load_anno_df(self, fn=None, force=False):\n",
    "        if force or self._annodf is None:\n",
    "            fn=fn if fn else self.path_anno\n",
    "            self._annodf = pd.read_csv(fn).set_index('name').fillna('')\n",
    "        return self._annodf\n",
    "    \n",
    "    @cached_property\n",
    "    def anno_df(self): return self.load_anno_df()\n",
    "\n",
    "    @cached_property\n",
    "    def anno_ents(self): \n",
    "        df=self.anno_df\n",
    "        df=df[df.is_valid.str.startswith('y')]\n",
    "        return set(df.index)\n",
    "\n",
    "    def iter_ents_anno(self, **kwargs):\n",
    "        kwargs['ents']=self.anno_ents\n",
    "        yield from self.iter_ents(**kwargs)\n",
    "\n",
    "    def iter_persons_anno(self, **kwargs):\n",
    "        kwargs['ent_types']={'PERSON'}\n",
    "        yield from self.iter_ents_anno(**kwargs)\n",
    "\n",
    "    def link_persons(self, min_page_count=2,**kwargs):\n",
    "        import networkx as nx\n",
    "        last_pageid=None\n",
    "        last_ents = []\n",
    "        G = nx.Graph()\n",
    "        l = [x for x in self.iter_persons_anno(by_page=True) if len(x)==2]\n",
    "        for pageid,page_ents in l:\n",
    "            for a1,b1 in page_ents:\n",
    "                for a2,b2 in page_ents:\n",
    "                    if a1<a2:\n",
    "                        if G.has_edge(a1,a2): \n",
    "                            G.edges[a1,a2]['weight']+=1\n",
    "                        else: \n",
    "                            G.add_edge(a1,a2,weight=0)\n",
    "        \n",
    "        bad=[(a,b) for a,b,d in G.edges(data=True) if d['weight']<min_page_count]\n",
    "        G.remove_edges_from(bad)\n",
    "        return G\n",
    "    \n",
    "#     def person_cooccurence_matrix(self, min_page_count=2,**kwargs):\n",
    "#         persons = defaultdict(Counter)\n",
    "#         numpages = 0\n",
    "#         for pagedata in self.iter_persons_anno(by_page=True):\n",
    "#             if len(pagedata)!=2: continue \n",
    "#             numpages+=1\n",
    "#             pageid,pageents = pagedata\n",
    "#             pageents = {x[0] for x in pageents}\n",
    "#             for x in pageents:\n",
    "#                 for y in pageents:\n",
    "#                     if x!=y:\n",
    "#                         persons[x][y]+=1\n",
    "#         df = pd.DataFrame(persons).fillna(0) / numpages * 1000\n",
    "#         return df\n",
    "        \n",
    "    def person_cooccurence(self, min_page_count=5,**kwargs):\n",
    "        person1 = Counter()\n",
    "        person2 = Counter()\n",
    "        numpages = 0\n",
    "        pair_pages = defaultdict(set)\n",
    "        for pagedata in self.iter_persons_anno(by_page=True):\n",
    "            if len(pagedata)!=2: continue \n",
    "            numpages+=1\n",
    "            pageid,pageents = pagedata\n",
    "            pageents = {x[0] for x in pageents}\n",
    "            for x in pageents:\n",
    "                person1[x]+=1\n",
    "                for y in pageents:\n",
    "                    if x<y:\n",
    "                        person2[x,y]+=1\n",
    "                        pair_pages[x,y].add(pageid)\n",
    "        \n",
    "        # calc probs\n",
    "        person1_sum = sum(person1.values())\n",
    "        person2_sum = sum(person2.values())\n",
    "        person1_probs = {k:v/person1_sum for k,v in person1.items()}\n",
    "        person2_probs = {k:v/person2_sum for k,v in person2.items()}\n",
    "\n",
    "        o=[]\n",
    "        for pair in tqdm(person2_probs):\n",
    "            if min_page_count and person2[pair]<min_page_count: continue\n",
    "            p1,p2 = pair\n",
    "            p1_prob,p2_prob = person1_probs[p1],person1_probs[p2]\n",
    "            prob_exp = p1_prob * p2_prob\n",
    "            prob_obs = person2_probs[pair]\n",
    "            od={\n",
    "                'person1':p1, 'person2':p2, \n",
    "                'count1':person1[p1], 'count2':person1[p2], 'count_both':person2[pair],\n",
    "                'prob1':p1_prob, 'prob2':p2_prob, \n",
    "                'prob_exp':prob_exp, 'prob_obs':prob_obs, 'obsexp':prob_obs/prob_exp,\n",
    "                'pair_pages':list(pair_pages[pair])\n",
    "            }\n",
    "            o.append(od)\n",
    "        return pd.DataFrame(o).sort_values('obsexp',ascending=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ner = NERModel(ppa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = ner.person_cooccurence()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = df.query('obsexp>=10 & count_both>=10')\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding topic models\n",
    "qd=dict(min_doc_len=25,max_per_cluster=50,frac=1)\n",
    "tm = ppa.topic_model(model_type='bertopic', **qd)\n",
    "tm.mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdf = tm.mdl.get_topic_info()\n",
    "tdf.columns = [x.lower() for x in tdf]\n",
    "tdf['representative_docs_ids']=[[tm.doc2id[doc] for doc in docs] for docs in tdf.representative_docs]\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "pages_tdf = {pageid for pages in tdf.representative_docs_ids for pageid in pages}\n",
    "pages_pdf = {pageid for pages in pdf.pair_pages for pageid in pages}\n",
    "pages_both = pages_tdf & pages_pdf\n",
    "\n",
    "# topic -> doc\n",
    "for tname,tids in zip(tdf.name, tdf.representative_docs_ids):\n",
    "    for tid in tids[:1]:\n",
    "        if tid in pages_both:\n",
    "            G.add_edge(tname,tid)\n",
    "\n",
    "# doc -> person\n",
    "nodes = set(G.nodes())\n",
    "for pageid,person,_ in ner.iter_persons_anno():\n",
    "    if pageid in pages_both:\n",
    "        G.add_edge(pageid,person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.order(),G.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "nt = Network(notebook=True, cdn_resources='in_line')\n",
    "nt.from_nx(G)\n",
    "nt.show('tmp.nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

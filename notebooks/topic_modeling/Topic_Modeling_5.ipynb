{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5gXNfcVjvDc"
      },
      "source": [
        "# Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C36S18zo8w3b"
      },
      "outputs": [],
      "source": [
        "# !pip install -U orjson sqlitedict tomotopy nltk pyLDAvis altair ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import orjson\n",
        "import zlib\n",
        "import tomotopy as tp\n",
        "from sqlitedict import SqliteDict\n",
        "import topicwizard\n",
        "import random\n",
        "import pyLDAvis\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords as stops\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAu1Sgxsydd",
        "outputId": "bc9aa14d-c79e-482a-fbfb-a97d7c65b968"
      },
      "outputs": [],
      "source": [
        "# corpus\n",
        "path_corpus=os.path.expanduser('~/ppa_data/solrcorpus2')\n",
        "path_metadata = os.path.join(path_corpus, 'metadata.csv')\n",
        "path_pages = os.path.join(path_corpus, 'corpus.sqlitedict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TeT1qRuDtPtn",
        "outputId": "ad89b624-cfc7-4c98-9fcf-c743aa8542d8"
      },
      "outputs": [],
      "source": [
        "# Read metadata\n",
        "# df_metadata = pd.read_csv(path_metadata).fillna('').set_index('work_id')\n",
        "# df_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcRHm7iet_jz"
      },
      "outputs": [],
      "source": [
        "def encode_cache(x): return sqlite3.Binary(zlib.compress(orjson.dumps(x)))\n",
        "def decode_cache(x): return orjson.loads(zlib.decompress(bytes(x)))\n",
        "def get_pages_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='texts', encode=encode_cache, decode=decode_cache)\n",
        "def get_meta_db():\n",
        "    return SqliteDict(path_pages, flag='r', tablename='metadata', encode=encode_cache, decode=decode_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CLUSTER_KEY='cluster_id_s'\n",
        "\n",
        "def iter_pages(lim=None,min_num_words=None,max_pages_per_doc=None,max_pages_per_cluster=None, collections={}):\n",
        "    num=0\n",
        "    clustercounts=Counter()\n",
        "    breaknow=False\n",
        "    with get_pages_db() as db, get_meta_db() as mdb:\n",
        "        for work_id in tqdm(list(db.keys()),desc='Iterating works',position=0):\n",
        "            if breaknow: break\n",
        "\n",
        "            meta = mdb[work_id]\n",
        "            if collections and not set(meta['collections']) & set(collections):\n",
        "                continue\n",
        "            pages = db[work_id]\n",
        "            cluster = meta.get(CLUSTER_KEY,work_id)\n",
        "\n",
        "            if min_num_words:\n",
        "                pages = [d for d in pages if len(d['page_tokens'])>=min_num_words]\n",
        "\n",
        "            if max_pages_per_doc:\n",
        "                random.shuffle(pages)\n",
        "                pages=pages[:max_pages_per_doc]\n",
        "\n",
        "            pbar2=tqdm(pages,desc='Iterating pages',position=1,disable=True)\n",
        "            for page in pbar2:\n",
        "                if not max_pages_per_cluster or clustercounts[cluster]<max_pages_per_cluster:\n",
        "                    yield dict(\n",
        "                        work_cluster = cluster,\n",
        "                        **page\n",
        "                    )\n",
        "                    clustercounts[cluster]+=1\n",
        "                    num+=1\n",
        "                    if lim and num>=lim:\n",
        "                        breaknow=True\n",
        "                        break\n",
        "            pbar2.close()\n",
        "\n",
        "def iter_corpus(lim=None,max_pages_per_doc=25,**kwargs):\n",
        "    yield from iter_pages(lim=lim,min_num_words=25,collections={'Literary','Linguistic'},max_pages_per_doc=max_pages_per_doc,**kwargs)\n",
        "\n",
        "def iter_sample(lim=None):\n",
        "    yield from iter_corpus(lim=lim, max_pages_per_cluster=25, max_pages_per_doc=25)\n",
        "\n",
        "# next(iter_pages(collections=['Linguistic']))\n",
        "# for x in iter_pages(max_pages_per_cluster=1): pass\n",
        "# for i,x in enumerate(iter_corpus()): pass\n",
        "# i\n",
        "# next(iter_corpus())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "stopwords = set(stops.words('english'))\n",
        "def clean_toks(toks):\n",
        "    return [tok for tok in toks if len(tok)>3 and tok not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topic_model(ntopic=50, force=False, niter=100):\n",
        "    fn=f'data.tomotopy.model.ntopic={ntopic}.bin'\n",
        "    fnindex=fn+'.index.json'\n",
        "    if force or not os.path.exists(fn) or not os.path.exists(fnindex):\n",
        "        mdl = tp.LDAModel(k=50)\n",
        "        docd={}\n",
        "        for page in iter_sample():\n",
        "            toks = clean_toks(page['page_tokens'])\n",
        "            docd[page['page_id']] = mdl.add_doc(toks)\n",
        "\n",
        "        def getdesc():\n",
        "            return f'Training model (ndocs={len(docd)}, log-likelihood = {mdl.ll_per_word:.4})'\n",
        "        pbar=tqdm(list(range(0, niter, 10)),desc=getdesc(),position=0)\n",
        "        for i in pbar:\n",
        "            pbar.set_description(getdesc())\n",
        "            mdl.train(10)\n",
        "        mdl.save(fn)\n",
        "        with open(fnindex,'wb') as of:\n",
        "            of.write(orjson.dumps(docd))\n",
        "    else:\n",
        "        print(f'Loading model: {fn}')\n",
        "        mdl = tp.LDAModel.load(fn)\n",
        "        print(f'Loading model index: {fnindex}')\n",
        "        with open(fnindex,'rb') as f:\n",
        "            docd=orjson.loads(f.read())\n",
        "\n",
        "    mdl.summary()\n",
        "    return mdl,docd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mdl,docd = topic_model(force=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_pyldavis():\n",
        "    print('Calculating topic_term_dists')\n",
        "    topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
        "\n",
        "\n",
        "    print('Calculating doc_topic_dists')\n",
        "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
        "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
        "\n",
        "    print('Calculating doc_lengths')\n",
        "    doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
        "\n",
        "\n",
        "    print('Calculating vocab')\n",
        "    vocab = list(mdl.used_vocabs)\n",
        "    term_frequency = mdl.used_vocab_freq\n",
        "\n",
        "    print('preparing data')\n",
        "    prepared_data = pyLDAvis.prepare(\n",
        "        topic_term_dists, \n",
        "        doc_topic_dists, \n",
        "        doc_lengths, \n",
        "        vocab, \n",
        "        term_frequency,\n",
        "        start_index=0, # tomotopy starts topic ids with 0, pyLDAvis with 1\n",
        "        sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
        "    )\n",
        "\n",
        "    print('saving html')\n",
        "    pyLDAvis.save_html(prepared_data, 'ldavis.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!open ldavis.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_topic_dists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dfclust():\n",
        "    id2doc={v:k for k,v in docd.items()}\n",
        "    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
        "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
        "    index,values = zip(*[(id2doc[i],x) for i,x in enumerate(doc_topic_dists) if i in id2doc])\n",
        "    dftopicdist = pd.DataFrame(values, index=index)\n",
        "    with get_meta_db() as mdb:\n",
        "        dfmeta = pd.DataFrame({'work_id':wid, **mdb[wid]} for wid in tqdm(mdb, total=len(mdb), position=0, desc='Gathering metadata')).set_index('work_id')\n",
        "    w2c = dict(zip(dfmeta.index, dfmeta[CLUSTER_KEY]))\n",
        "    dftopicdist['work_id']=[i.split('_')[0] for i in dftopicdist.index]\n",
        "    dftopicdist['cluster']=[w2c.get(work_id,work_id) for work_id in dftopicdist.work_id]\n",
        "    dfclust_avgs=dftopicdist.groupby('cluster').mean(numeric_only=True)\n",
        "    dfclust_meta = dfmeta.drop_duplicates(CLUSTER_KEY).set_index(CLUSTER_KEY)\n",
        "    return dfclust_meta.join(dfclust_avgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfclust = get_dfclust()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "from ipywidgets import interactive, interact, interact_manual, HBox\n",
        "from functools import cache\n",
        "\n",
        "tnums=list(range(mdl.k))\n",
        "topicwords_d = {tnum:', '.join([i for i,j in mdl.get_topic_words(tnum)]) for tnum in tnums}\n",
        "topicnames = [f'{tnum}: {topicwords_d[tnum]}' for tnum in tnums]\n",
        "\n",
        "def get_topic_name(tnum):\n",
        "    return topicnames[tnum]\n",
        "\n",
        "def get_wordcloud(tnum):\n",
        "    wc = WordCloud(background_color='white', width=800, height=400)\n",
        "    wordcloud = wc.generate_from_frequencies(dict(mdl.get_topic_words(tnum, top_n=100)))\n",
        "    return wordcloud\n",
        "\n",
        "@cache\n",
        "def get_figdf(tnum):\n",
        "    collections={'Linguistic','Literary'}\n",
        "    figdf=dfclust.reset_index()[[CLUSTER_KEY,'title','author','pub_date','publisher','pub_place','source_url', 'collections', tnum]]\n",
        "    figdf['collections']=[[x for x in c if x in collections] for c in figdf.collections]\n",
        "    figdf['collections'] = figdf['collections'].apply(lambda x: 'Linguistic' if 'Linguistic' in set(x) else (x[0] if x else x))\n",
        "    figdf=figdf[figdf.collections.apply(bool)]\n",
        "    figdf.columns = ['cluster', 'title', 'author', 'date', 'publisher', 'pubplace', 'source', 'genre', 'topic']\n",
        "    figdf = figdf[1700<=figdf.date]\n",
        "    return figdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_topic_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%timeit\n",
        "# get_figdf(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @interact(tname=topicnames)\n",
        "def plot_topic(tname):\n",
        "    tnum=int(tname.split(':')[0])\n",
        "    figdf=get_figdf(tnum)\n",
        "    topicwords = topicwords_d.get(tnum)\n",
        "    wordcloud=get_wordcloud(tnum)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    \n",
        "    fig = alt.Chart(figdf).mark_circle(size=60).encode(\n",
        "        x = alt.X('date', scale=alt.Scale(domain=[1700, 1920])),\n",
        "        y='topic',\n",
        "        color='genre',\n",
        "        tooltip=figdf.columns.tolist()\n",
        "    ).interactive(\n",
        "    ).properties(\n",
        "        width=800,\n",
        "        height=400,\n",
        "    # ).facet(\n",
        "    #     facet='genre:N',\n",
        "    #     columns=2,\n",
        "    # ).resolve_scale(\n",
        "    #     y='independent'\n",
        "    ).properties(\n",
        "        title = f'Topic {tnum}: {topicwords}'\n",
        "    )\n",
        "    return fig "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install plotnine\n",
        "import plotnine as p9\n",
        "p9.options.figure_size=(8,4)\n",
        "\n",
        "def plot_topic_min(tnum):\n",
        "    figdf=get_figdf(tnum)\n",
        "    figdf['period']=figdf['date'].apply(str)\n",
        "    figdf=figdf.groupby(['genre','period']).median(numeric_only=True).reset_index()\n",
        "    figdf=pd.concat(\n",
        "        gdf.assign(topic=gdf.topic.rolling(10).mean())\n",
        "        for g,gdf in figdf.groupby('genre')\n",
        "    )\n",
        "    fig=p9.ggplot(figdf, p9.aes(x='date',y='topic',color='genre'))\n",
        "    fig+=p9.geom_point()\n",
        "    fig+=p9.geom_smooth(method='loess')\n",
        "    fig+=p9.labs(\n",
        "        title=get_topic_name(tnum),\n",
        "        x='Date of publication',\n",
        "        y='Prevalence of Topic'\n",
        "    )\n",
        "    fig+=p9.theme_classic()\n",
        "    odir='timeplots'\n",
        "    os.makedirs(odir,exist_ok=True)\n",
        "    fig.save(f'{odir}/fig.timeplot.tnum={tnum}.png')\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_topic_min(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for tnum in range(mdl.k): plot_topic_min(tnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "def get_cluster_name(clustid):\n",
        "    meta = dict(dfclust_meta.loc[clustid])\n",
        "    return f'{meta[\"title\"].strip(punctuation)[:50]} ({str(meta[\"pub_date\"])[:4]}) [{meta[\"source_url\"]}]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_cluster_name('mdp.39015050663247')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_wordclouds(odir='wordclouds'):\n",
        "    my_dpi=75\n",
        "    os.makedirs(odir, exist_ok=True)\n",
        "    for tnum in tqdm(list(range(mdl.k)), desc='Saving wordclouds'):\n",
        "        wordcloud=get_wordcloud(tnum)\n",
        "        plt.box(False)\n",
        "        plt.figure(figsize=(800/my_dpi, 400/my_dpi), dpi=my_dpi)\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.savefig(f'{odir}/fig.wordcloud.tnum={tnum}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_wordclouds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def geturl(tnum):\n",
        "    return f'https://raw.githubusercontent.com/Princeton-CDH/ppa-nlp/develop/notebooks/wordclouds/fig.wordcloud.tnum%3D{tnum}.png'\n",
        "\n",
        "def geturl2(tnum):\n",
        "    return f'https://raw.githubusercontent.com/Princeton-CDH/ppa-nlp/develop/notebooks/timeplots/fig.timeplot.tnum%3D{tnum}.png'\n",
        "\n",
        "\n",
        "def get_topic_info_df():\n",
        "    tld=[]\n",
        "    for tnum in tqdm(list(range(mdl.k)), desc='Gathering info on topics'):\n",
        "        td={\n",
        "            'Topic':tnum,\n",
        "            'Topic Name':'',\n",
        "            'Top Words':', '.join([i for i,j in mdl.get_topic_words(tnum, top_n=50)]),\n",
        "            'Top Documents':'* '+('\\n* '.join(get_cluster_name(c) for c in dfclust.sort_values(tnum,ascending=False).index[:5])),\n",
        "            'Word Cloud':f'=IMAGE(\"{geturl(tnum)}\")',\n",
        "            'Historical Plot':f'=IMAGE(\"{geturl2(tnum)}\")',\n",
        "        }\n",
        "        tld.append(td)\n",
        "    tdf=pd.DataFrame(tld).set_index('Topic')\n",
        "    return tdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tdf=get_topic_info_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install openpyxl\n",
        "tdf.to_excel('data.topic_info.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5gXNfcVjvDc"
      },
      "source": [
        "# Topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lbHvJ-_I0Wx7"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU pip wheel\n",
        "# !pip install -qU gensim tqdm pandas nltk numpy\n",
        "# import nltk\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C36S18zo8w3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import random\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOTAL_TOPICS = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAu1Sgxsydd",
        "outputId": "bc9aa14d-c79e-482a-fbfb-a97d7c65b968"
      },
      "outputs": [],
      "source": [
        "# corpus\n",
        "path_corpus=os.path.expanduser('~/ppa_data/solrcorpus')\n",
        "path_metadata = os.path.join(path_corpus, 'metadata.csv')\n",
        "path_minimal = os.path.join(path_corpus, 'minimal.jsonl')\n",
        "path_texts = os.path.join(path_corpus, 'texts')\n",
        "path_minimal_numlines=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TeT1qRuDtPtn",
        "outputId": "ad89b624-cfc7-4c98-9fcf-c743aa8542d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>work_id</th>\n",
              "      <th>source_id</th>\n",
              "      <th>group_id_s</th>\n",
              "      <th>source_t</th>\n",
              "      <th>source_url</th>\n",
              "      <th>title</th>\n",
              "      <th>sort_title</th>\n",
              "      <th>pub_date</th>\n",
              "      <th>pub_place</th>\n",
              "      <th>publisher</th>\n",
              "      <th>...</th>\n",
              "      <th>cluster_id_s</th>\n",
              "      <th>item_type</th>\n",
              "      <th>order</th>\n",
              "      <th>work_type_s</th>\n",
              "      <th>last_modified</th>\n",
              "      <th>_version_</th>\n",
              "      <th>first_page_i</th>\n",
              "      <th>book_journal_s</th>\n",
              "      <th>subtitle</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dul1.ark:|13960|t6d23816n</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/dul1.ark:/13960/t6...</td>\n",
              "      <td>Lyrical ballads, with other poems: in two volu...</td>\n",
              "      <td>Lyrical ballads, with other poems: in two volu...</td>\n",
              "      <td>1802.0</td>\n",
              "      <td>Philadelphia</td>\n",
              "      <td>James Humphreys</td>\n",
              "      <td>...</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>full-work</td>\n",
              "      <td>2023-02-02T15:00:45.957Z</td>\n",
              "      <td>1756731849797795847</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dul1.ark:|13960|t6d23816n-p11</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n-p11</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/dul1.ark:/13960/t6...</td>\n",
              "      <td>Preface [to Lyrical Ballads]</td>\n",
              "      <td>Preface [to Lyrical Ballads]</td>\n",
              "      <td>1802.0</td>\n",
              "      <td>Philadelphia</td>\n",
              "      <td>James Humphreys</td>\n",
              "      <td>...</td>\n",
              "      <td>dul1.ark:/13960/t6d23816n-p11</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>excerpt</td>\n",
              "      <td>2023-02-14T19:59:25.780Z</td>\n",
              "      <td>1757837803736006658</td>\n",
              "      <td>11.0</td>\n",
              "      <td>Lyrical ballads, with other poems: in two volu...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>inu.32000006890968</td>\n",
              "      <td>inu.32000006890968</td>\n",
              "      <td>inu.32000006890968</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/inu.32000006890968</td>\n",
              "      <td>The mystical poets of the English church,</td>\n",
              "      <td>mystical poets of the English church,</td>\n",
              "      <td>1919.0</td>\n",
              "      <td>London</td>\n",
              "      <td>Society for Promoting Christian Knowledge</td>\n",
              "      <td>...</td>\n",
              "      <td>osmondmystical</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>full-work</td>\n",
              "      <td>2023-02-14T19:34:42.875Z</td>\n",
              "      <td>1757836248763858945</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hvd.hn5bic</td>\n",
              "      <td>hvd.hn5bic</td>\n",
              "      <td>hvd.hn5bic</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/hvd.hn5bic</td>\n",
              "      <td>An analysis of the derivative words in the Eng...</td>\n",
              "      <td>analysis of the derivative words in the Englis...</td>\n",
              "      <td>1841.0</td>\n",
              "      <td>New York</td>\n",
              "      <td>Clement &amp; Packard</td>\n",
              "      <td>...</td>\n",
              "      <td>townanalysis</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>full-work</td>\n",
              "      <td>2023-02-14T19:59:31.530Z</td>\n",
              "      <td>1757837809744347137</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>or, A key to their precise analytic definition...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CW0100721039</td>\n",
              "      <td>CW0100721039</td>\n",
              "      <td>CW0100721039</td>\n",
              "      <td>['Gale']</td>\n",
              "      <td>https://link.gale.com/apps/doc/CW0100721039/EC...</td>\n",
              "      <td>A new and general biographical dictionary</td>\n",
              "      <td>new and general biographical dictionary contai...</td>\n",
              "      <td>1798.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson, J. Johnson, J. Nichols, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>CW0100721039</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>full-work</td>\n",
              "      <td>2023-02-14T19:34:42.875Z</td>\n",
              "      <td>1757836248776441856</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>containing an historical and critical account ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6934</th>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/njp.32101076530979</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td>1799.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson</td>\n",
              "      <td>...</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>full-work</td>\n",
              "      <td>2023-02-02T15:00:44.887Z</td>\n",
              "      <td>1756731848685256707</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6935</th>\n",
              "      <td>njp.32101076530979-p464</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979-p476</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/njp.32101076530979</td>\n",
              "      <td>On Shakespeare</td>\n",
              "      <td>On Shakespeare</td>\n",
              "      <td>1799.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson</td>\n",
              "      <td>...</td>\n",
              "      <td>njp.32101076530979-p476</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>excerpt</td>\n",
              "      <td>2023-02-14T19:59:23.288Z</td>\n",
              "      <td>1757837801082060801</td>\n",
              "      <td>476.0</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6936</th>\n",
              "      <td>njp.32101076530979-p482</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979-p494</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/njp.32101076530979</td>\n",
              "      <td>On Stile and Versification</td>\n",
              "      <td>On Stile and Versification</td>\n",
              "      <td>1799.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson</td>\n",
              "      <td>...</td>\n",
              "      <td>njp.32101076530979-p494</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>excerpt</td>\n",
              "      <td>2023-02-14T19:59:23.288Z</td>\n",
              "      <td>1757837801083109376</td>\n",
              "      <td>494.0</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6937</th>\n",
              "      <td>njp.32101076530979-p514</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979-p526</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/njp.32101076530979</td>\n",
              "      <td>On Epic Poetry</td>\n",
              "      <td>On Epic Poetry</td>\n",
              "      <td>1799.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson</td>\n",
              "      <td>...</td>\n",
              "      <td>njp.32101076530979-p526</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>excerpt</td>\n",
              "      <td>2023-02-14T19:59:23.288Z</td>\n",
              "      <td>1757837801079963654</td>\n",
              "      <td>526.0</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6938</th>\n",
              "      <td>njp.32101076530979-p545</td>\n",
              "      <td>njp.32101076530979</td>\n",
              "      <td>njp.32101076530979-p557</td>\n",
              "      <td>['HathiTrust']</td>\n",
              "      <td>https://hdl.handle.net/2027/njp.32101076530979</td>\n",
              "      <td>On Dramatic Poetry</td>\n",
              "      <td>On Dramatic Poetry</td>\n",
              "      <td>1799.0</td>\n",
              "      <td>London</td>\n",
              "      <td>G.G. and J. Robinson</td>\n",
              "      <td>...</td>\n",
              "      <td>njp.32101076530979-p557</td>\n",
              "      <td>work</td>\n",
              "      <td>0</td>\n",
              "      <td>excerpt</td>\n",
              "      <td>2023-02-14T19:59:23.288Z</td>\n",
              "      <td>1757837801079963649</td>\n",
              "      <td>557.0</td>\n",
              "      <td>Essays philosophical and moral, historical and...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6939 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            work_id                  source_id  \\\n",
              "0         dul1.ark:|13960|t6d23816n  dul1.ark:/13960/t6d23816n   \n",
              "1     dul1.ark:|13960|t6d23816n-p11  dul1.ark:/13960/t6d23816n   \n",
              "2                inu.32000006890968         inu.32000006890968   \n",
              "3                        hvd.hn5bic                 hvd.hn5bic   \n",
              "4                      CW0100721039               CW0100721039   \n",
              "...                             ...                        ...   \n",
              "6934             njp.32101076530979         njp.32101076530979   \n",
              "6935        njp.32101076530979-p464         njp.32101076530979   \n",
              "6936        njp.32101076530979-p482         njp.32101076530979   \n",
              "6937        njp.32101076530979-p514         njp.32101076530979   \n",
              "6938        njp.32101076530979-p545         njp.32101076530979   \n",
              "\n",
              "                         group_id_s        source_t  \\\n",
              "0         dul1.ark:/13960/t6d23816n  ['HathiTrust']   \n",
              "1     dul1.ark:/13960/t6d23816n-p11  ['HathiTrust']   \n",
              "2                inu.32000006890968  ['HathiTrust']   \n",
              "3                        hvd.hn5bic  ['HathiTrust']   \n",
              "4                      CW0100721039        ['Gale']   \n",
              "...                             ...             ...   \n",
              "6934             njp.32101076530979  ['HathiTrust']   \n",
              "6935        njp.32101076530979-p476  ['HathiTrust']   \n",
              "6936        njp.32101076530979-p494  ['HathiTrust']   \n",
              "6937        njp.32101076530979-p526  ['HathiTrust']   \n",
              "6938        njp.32101076530979-p557  ['HathiTrust']   \n",
              "\n",
              "                                             source_url  \\\n",
              "0     https://hdl.handle.net/2027/dul1.ark:/13960/t6...   \n",
              "1     https://hdl.handle.net/2027/dul1.ark:/13960/t6...   \n",
              "2        https://hdl.handle.net/2027/inu.32000006890968   \n",
              "3                https://hdl.handle.net/2027/hvd.hn5bic   \n",
              "4     https://link.gale.com/apps/doc/CW0100721039/EC...   \n",
              "...                                                 ...   \n",
              "6934     https://hdl.handle.net/2027/njp.32101076530979   \n",
              "6935     https://hdl.handle.net/2027/njp.32101076530979   \n",
              "6936     https://hdl.handle.net/2027/njp.32101076530979   \n",
              "6937     https://hdl.handle.net/2027/njp.32101076530979   \n",
              "6938     https://hdl.handle.net/2027/njp.32101076530979   \n",
              "\n",
              "                                                  title  \\\n",
              "0     Lyrical ballads, with other poems: in two volu...   \n",
              "1                          Preface [to Lyrical Ballads]   \n",
              "2             The mystical poets of the English church,   \n",
              "3     An analysis of the derivative words in the Eng...   \n",
              "4             A new and general biographical dictionary   \n",
              "...                                                 ...   \n",
              "6934  Essays philosophical and moral, historical and...   \n",
              "6935                                     On Shakespeare   \n",
              "6936                         On Stile and Versification   \n",
              "6937                                     On Epic Poetry   \n",
              "6938                                 On Dramatic Poetry   \n",
              "\n",
              "                                             sort_title pub_date  \\\n",
              "0     Lyrical ballads, with other poems: in two volu...   1802.0   \n",
              "1                          Preface [to Lyrical Ballads]   1802.0   \n",
              "2                 mystical poets of the English church,   1919.0   \n",
              "3     analysis of the derivative words in the Englis...   1841.0   \n",
              "4     new and general biographical dictionary contai...   1798.0   \n",
              "...                                                 ...      ...   \n",
              "6934  Essays philosophical and moral, historical and...   1799.0   \n",
              "6935                                     On Shakespeare   1799.0   \n",
              "6936                         On Stile and Versification   1799.0   \n",
              "6937                                     On Epic Poetry   1799.0   \n",
              "6938                                 On Dramatic Poetry   1799.0   \n",
              "\n",
              "          pub_place                                          publisher  ...  \\\n",
              "0      Philadelphia                                    James Humphreys  ...   \n",
              "1     Philadelphia                                     James Humphreys  ...   \n",
              "2            London          Society for Promoting Christian Knowledge  ...   \n",
              "3          New York                                  Clement & Packard  ...   \n",
              "4           London   G.G. and J. Robinson, J. Johnson, J. Nichols, ...  ...   \n",
              "...             ...                                                ...  ...   \n",
              "6934         London                               G.G. and J. Robinson  ...   \n",
              "6935         London                               G.G. and J. Robinson  ...   \n",
              "6936         London                               G.G. and J. Robinson  ...   \n",
              "6937         London                               G.G. and J. Robinson  ...   \n",
              "6938         London                               G.G. and J. Robinson  ...   \n",
              "\n",
              "                       cluster_id_s item_type order work_type_s  \\\n",
              "0         dul1.ark:/13960/t6d23816n      work     0   full-work   \n",
              "1     dul1.ark:/13960/t6d23816n-p11      work     0     excerpt   \n",
              "2                    osmondmystical      work     0   full-work   \n",
              "3                      townanalysis      work     0   full-work   \n",
              "4                      CW0100721039      work     0   full-work   \n",
              "...                             ...       ...   ...         ...   \n",
              "6934             njp.32101076530979      work     0   full-work   \n",
              "6935        njp.32101076530979-p476      work     0     excerpt   \n",
              "6936        njp.32101076530979-p494      work     0     excerpt   \n",
              "6937        njp.32101076530979-p526      work     0     excerpt   \n",
              "6938        njp.32101076530979-p557      work     0     excerpt   \n",
              "\n",
              "                 last_modified            _version_ first_page_i  \\\n",
              "0     2023-02-02T15:00:45.957Z  1756731849797795847                \n",
              "1     2023-02-14T19:59:25.780Z  1757837803736006658         11.0   \n",
              "2     2023-02-14T19:34:42.875Z  1757836248763858945                \n",
              "3     2023-02-14T19:59:31.530Z  1757837809744347137                \n",
              "4     2023-02-14T19:34:42.875Z  1757836248776441856                \n",
              "...                        ...                  ...          ...   \n",
              "6934  2023-02-02T15:00:44.887Z  1756731848685256707                \n",
              "6935  2023-02-14T19:59:23.288Z  1757837801082060801        476.0   \n",
              "6936  2023-02-14T19:59:23.288Z  1757837801083109376        494.0   \n",
              "6937  2023-02-14T19:59:23.288Z  1757837801079963654        526.0   \n",
              "6938  2023-02-14T19:59:23.288Z  1757837801079963649        557.0   \n",
              "\n",
              "                                         book_journal_s  \\\n",
              "0                                                         \n",
              "1     Lyrical ballads, with other poems: in two volu...   \n",
              "2                                                         \n",
              "3                                                         \n",
              "4                                                         \n",
              "...                                                 ...   \n",
              "6934                                                      \n",
              "6935  Essays philosophical and moral, historical and...   \n",
              "6936  Essays philosophical and moral, historical and...   \n",
              "6937  Essays philosophical and moral, historical and...   \n",
              "6938  Essays philosophical and moral, historical and...   \n",
              "\n",
              "                                               subtitle notes  \n",
              "0                                                              \n",
              "1                                                              \n",
              "2                                                              \n",
              "3     or, A key to their precise analytic definition...        \n",
              "4     containing an historical and critical account ...        \n",
              "...                                                 ...   ...  \n",
              "6934                                                           \n",
              "6935                                                           \n",
              "6936                                                           \n",
              "6937                                                           \n",
              "6938                                                           \n",
              "\n",
              "[6939 rows x 25 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read metadata\n",
        "df_metadata = pd.read_csv(path_metadata).fillna('')\n",
        "df_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords as stops\n",
        "stopwords = set(stops.words('english')) | {'one','may'}\n",
        "# stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "import orjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "pcRHm7iet_jz"
      },
      "outputs": [],
      "source": [
        "import jsonlines,orjson\n",
        "\n",
        "def iter_id_tokens(min_toks=50):\n",
        "    global path_minimal_numlines\n",
        "    if path_minimal_numlines == None:\n",
        "        with open(path_minimal) as f:\n",
        "            path_minimal_numlines = sum(1 for line in tqdm(f,desc='Getting number of lines',position=0))\n",
        "\n",
        "    with open(path_minimal) as f:\n",
        "        for ln in tqdm(f,total=path_minimal_numlines,desc='Iterating over jsonl',position=0):\n",
        "            d=orjson.loads(ln)\n",
        "            yield (d['id'], d['toks'])\n",
        "    \n",
        "def iter_tokens():\n",
        "    for id,toks in iter_id_tokens():\n",
        "        yield toks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_KesWR_v36_",
        "outputId": "8c14f532-3a6c-40e5-ba9f-60abd24aba2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating over jsonl: 100%|██████████| 2097745/2097745 [01:34<00:00, 22295.15it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1982342"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# iter = iter_id_tokens()\n",
        "# next(iter)\n",
        "all_page_ids = {id for id,toks in iter_id_tokens()}\n",
        "len(all_page_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next(iter) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj2lB4_b_eFO"
      },
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "\n",
        "We can now perform feature engineering by leveraging a simple Bag of Words\n",
        "model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbCkD1h3wfXY",
        "outputId": "8a24f561-cd13-4626-b1e4-b475bd3234a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 09:51:53,498 : INFO : loading Dictionary object from /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.dictionary.pkl\n",
            "2023-11-29 09:51:55,060 : INFO : Dictionary lifecycle event {'fname': '/Users/ryanheuser/ppa_data/solrcorpus/data.gensim.dictionary.pkl', 'datetime': '2023-11-29T09:51:55.060602', 'gensim': '4.3.2', 'python': '3.12.0 (main, Nov  7 2023, 18:55:06) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.0-x86_64-i386-64bit', 'event': 'loaded'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2027040"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fn=os.path.join(path_corpus,'data.gensim.dictionary.pkl')\n",
        "if not os.path.exists(fn):\n",
        "    dictionary = gensim.corpora.Dictionary(iter_tokens())\n",
        "    dictionary.save(fn)\n",
        "else:\n",
        "    dictionary = gensim.corpora.Dictionary.load(fn)\n",
        "len(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 09:51:59,997 : INFO : discarding 1927040 tokens: [('the', 1992801), ('womn', 89), ('and', 1940820), ('atreet', 24), ('elpjjia', 1), ('genium', 130), ('in', 1951311), ('of', 1978646), ('papiniane', 3), ('14j', 41)]...\n",
            "2023-11-29 09:51:59,998 : INFO : keeping 100000 tokens which were in no less than 10 and no more than 1887970 (=90.0%) documents\n",
            "2023-11-29 09:52:00,723 : INFO : resulting dictionary: Dictionary<100000 unique tokens: [\"'r\", 'bennett', 'coleridge', 'collection', 'duke']...>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dictionary.filter_extremes(no_above=.9, no_below=10)\n",
        "# dictionary.filter_extremes(keep_n=50000)\n",
        "len(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbDVZkO5wNHv",
        "outputId": "00f27db3-3658-4142-ba1d-7409bb601e83"
      },
      "outputs": [],
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "SLICE_LENGTH = 1000\n",
        "from collections import Counter\n",
        "\n",
        "def iter_corpus(slice_length=SLICE_LENGTH,max_pages_per_work=100,min_words_per_page=25):\n",
        "    lastid=None\n",
        "    toks=[]\n",
        "    ids=[]\n",
        "    idnumpages=0\n",
        "    for i,(page_id,page_toks) in enumerate(iter_id_tokens()):\n",
        "        id=page_id.split('_')[0]\n",
        "        \n",
        "        if not min_words_per_page or len(page_toks) >= min_words_per_page:\n",
        "            toks.extend([tok for tok in page_toks if tok not in stopwords and len(tok)>2])\n",
        "            ids.append(page_id)\n",
        "            idnumpages+=1\n",
        "\n",
        "        if lastid!=None and id!=lastid:\n",
        "            # if len(toks)>=slice_length and (not max_pages_per_work or idnumpages<=max_pages_per_work):\n",
        "            if len(toks)>=slice_length and (not max_pages_per_work or idnumpages<=max_pages_per_work):\n",
        "                yield (lastid,ids,toks)\n",
        "            toks = []\n",
        "            ids = []\n",
        "            idnumpages = 0\n",
        "        \n",
        "        lastid = id\n",
        "\n",
        "def iter_corpus(max_pages_per_work=25,min_words_per_page=25):\n",
        "    idnumpages=Counter()\n",
        "    for i,(page_id,page_toks) in enumerate(iter_id_tokens()):\n",
        "        page_toks = [tok for tok in page_toks if tok not in stopwords and len(tok)>2]\n",
        "        id=page_id.split('_')[0]\n",
        "        if not min_words_per_page or len(page_toks) >= min_words_per_page:\n",
        "            idnumpages[id]+=1\n",
        "            if not max_pages_per_work or idnumpages[id]<=max_pages_per_work:\n",
        "                yield (id,page_id,page_toks)\n",
        "        \n",
        "\n",
        "def iter_corpus_tokens(slice_length=SLICE_LENGTH):\n",
        "    for work_id,page_ids,toks in iter_corpus(slice_length=slice_length):\n",
        "        yield dictionary.doc2bow(toks)\n",
        "\n",
        "# next(iter_corpus())\n",
        "# next(iter_corpus_tokens())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "cp_fhm91-3R-"
      },
      "outputs": [],
      "source": [
        "# Transforming corpus into bag of words vectors\n",
        "def get_gensim_corpus(force=False):\n",
        "    fn=os.path.join(path_corpus,'data.gensim.corpus.pkl')\n",
        "    if force or not os.path.exists(fn):\n",
        "        gensim.corpora.MmCorpus.serialize(\n",
        "            fname=fn,\n",
        "            corpus=iter_corpus_tokens(),\n",
        "            id2word=dictionary,\n",
        "        )\n",
        "\n",
        "    corpus = gensim.corpora.MmCorpus(fn)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KUrfWNcWzGx2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 08:55:14,550 : INFO : storing corpus in Matrix Market format to /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.corpus.pkl\n",
            "2023-11-29 08:55:14,572 : INFO : saving sparse matrix to /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.corpus.pkl\n",
            "Iterating over jsonl:   0%|          | 0/2097745 [00:00<?, ?it/s]2023-11-29 08:55:14,575 : INFO : PROGRESS: saving document #0\n",
            "Iterating over jsonl:   1%|          | 12146/2097745 [00:00<02:48, 12371.07it/s]2023-11-29 08:55:15,663 : INFO : PROGRESS: saving document #1000\n",
            "Iterating over jsonl:   1%|▏         | 26892/2097745 [00:02<02:37, 13125.88it/s]2023-11-29 08:55:16,800 : INFO : PROGRESS: saving document #2000\n",
            "Iterating over jsonl:   2%|▏         | 39738/2097745 [00:03<02:29, 13734.47it/s]2023-11-29 08:55:17,771 : INFO : PROGRESS: saving document #3000\n",
            "Iterating over jsonl:   3%|▎         | 55076/2097745 [00:04<02:29, 13656.56it/s]2023-11-29 08:55:19,104 : INFO : PROGRESS: saving document #4000\n",
            "Iterating over jsonl:   3%|▎         | 66238/2097745 [00:05<03:20, 10136.69it/s]2023-11-29 08:55:20,051 : INFO : PROGRESS: saving document #5000\n",
            "Iterating over jsonl:   4%|▍         | 79402/2097745 [00:06<02:12, 15237.74it/s]2023-11-29 08:55:20,946 : INFO : PROGRESS: saving document #6000\n",
            "Iterating over jsonl:   4%|▍         | 90449/2097745 [00:07<02:15, 14790.84it/s]2023-11-29 08:55:21,709 : INFO : PROGRESS: saving document #7000\n",
            "Iterating over jsonl:   5%|▍         | 103977/2097745 [00:08<03:34, 9302.91it/s] 2023-11-29 08:55:22,901 : INFO : PROGRESS: saving document #8000\n",
            "Iterating over jsonl:   6%|▌         | 118121/2097745 [00:09<02:16, 14498.00it/s]2023-11-29 08:55:24,106 : INFO : PROGRESS: saving document #9000\n",
            "Iterating over jsonl:   6%|▌         | 130450/2097745 [00:10<02:19, 14057.48it/s]2023-11-29 08:55:24,958 : INFO : PROGRESS: saving document #10000\n",
            "Iterating over jsonl:   7%|▋         | 142185/2097745 [00:11<02:34, 12653.29it/s]2023-11-29 08:55:26,052 : INFO : PROGRESS: saving document #11000\n",
            "Iterating over jsonl:   7%|▋         | 153433/2097745 [00:12<02:15, 14362.77it/s]2023-11-29 08:55:26,851 : INFO : PROGRESS: saving document #12000\n",
            "Iterating over jsonl:   8%|▊         | 166949/2097745 [00:13<03:05, 10427.52it/s]2023-11-29 08:55:28,056 : INFO : PROGRESS: saving document #13000\n",
            "Iterating over jsonl:   9%|▊         | 178846/2097745 [00:14<02:41, 11893.99it/s]2023-11-29 08:55:29,072 : INFO : PROGRESS: saving document #14000\n",
            "Iterating over jsonl:   9%|▉         | 191496/2097745 [00:15<02:32, 12505.39it/s]2023-11-29 08:55:30,051 : INFO : PROGRESS: saving document #15000\n",
            "Iterating over jsonl:  10%|▉         | 204403/2097745 [00:16<02:25, 13055.44it/s]2023-11-29 08:55:31,035 : INFO : PROGRESS: saving document #16000\n",
            "Iterating over jsonl:  10%|█         | 219358/2097745 [00:17<03:04, 10174.54it/s]2023-11-29 08:55:32,243 : INFO : PROGRESS: saving document #17000\n",
            "Iterating over jsonl:  11%|█         | 230067/2097745 [00:18<03:07, 9936.27it/s] 2023-11-29 08:55:33,464 : INFO : PROGRESS: saving document #18000\n",
            "Iterating over jsonl:  12%|█▏        | 242601/2097745 [00:19<02:05, 14807.72it/s]2023-11-29 08:55:34,319 : INFO : PROGRESS: saving document #19000\n",
            "Iterating over jsonl:  12%|█▏        | 257449/2097745 [00:20<02:16, 13490.61it/s]2023-11-29 08:55:35,266 : INFO : PROGRESS: saving document #20000\n",
            "Iterating over jsonl:  13%|█▎        | 269267/2097745 [00:21<02:02, 14978.12it/s]2023-11-29 08:55:36,094 : INFO : PROGRESS: saving document #21000\n",
            "Iterating over jsonl:  13%|█▎        | 279680/2097745 [00:22<02:18, 13134.88it/s]2023-11-29 08:55:36,967 : INFO : PROGRESS: saving document #22000\n",
            "Iterating over jsonl:  14%|█▍        | 294793/2097745 [00:23<01:59, 15041.47it/s]2023-11-29 08:55:37,872 : INFO : PROGRESS: saving document #23000\n",
            "Iterating over jsonl:  15%|█▍        | 306645/2097745 [00:24<02:34, 11558.79it/s]2023-11-29 08:55:39,067 : INFO : PROGRESS: saving document #24000\n",
            "Iterating over jsonl:  15%|█▌        | 319629/2097745 [00:25<02:13, 13272.33it/s]2023-11-29 08:55:40,038 : INFO : PROGRESS: saving document #25000\n",
            "Iterating over jsonl:  16%|█▌        | 332031/2097745 [00:26<01:58, 14906.39it/s]2023-11-29 08:55:40,933 : INFO : PROGRESS: saving document #26000\n",
            "Iterating over jsonl:  16%|█▋        | 345250/2097745 [00:27<03:09, 9264.26it/s] 2023-11-29 08:55:42,094 : INFO : PROGRESS: saving document #27000\n",
            "Iterating over jsonl:  17%|█▋        | 357688/2097745 [00:28<01:59, 14617.37it/s]2023-11-29 08:55:43,019 : INFO : PROGRESS: saving document #28000\n",
            "Iterating over jsonl:  18%|█▊        | 372567/2097745 [00:29<02:03, 14013.15it/s]2023-11-29 08:55:44,205 : INFO : PROGRESS: saving document #29000\n",
            "Iterating over jsonl:  18%|█▊        | 385751/2097745 [00:30<02:12, 12942.98it/s]2023-11-29 08:55:45,203 : INFO : PROGRESS: saving document #30000\n",
            "Iterating over jsonl:  19%|█▉        | 399046/2097745 [00:31<02:00, 14101.46it/s]2023-11-29 08:55:46,260 : INFO : PROGRESS: saving document #31000\n",
            "Iterating over jsonl:  19%|█▉        | 408960/2097745 [00:32<02:21, 11894.05it/s]2023-11-29 08:55:47,203 : INFO : PROGRESS: saving document #32000\n",
            "Iterating over jsonl:  20%|██        | 422266/2097745 [00:33<02:17, 12141.74it/s]2023-11-29 08:55:48,289 : INFO : PROGRESS: saving document #33000\n",
            "Iterating over jsonl:  21%|██        | 433535/2097745 [00:34<02:00, 13841.03it/s]2023-11-29 08:55:49,185 : INFO : PROGRESS: saving document #34000\n",
            "Iterating over jsonl:  21%|██▏       | 446420/2097745 [00:35<02:00, 13674.73it/s]2023-11-29 08:55:50,372 : INFO : PROGRESS: saving document #35000\n",
            "Iterating over jsonl:  22%|██▏       | 459012/2097745 [00:36<02:18, 11872.90it/s]2023-11-29 08:55:51,411 : INFO : PROGRESS: saving document #36000\n",
            "Iterating over jsonl:  23%|██▎       | 472147/2097745 [00:37<01:57, 13829.27it/s]2023-11-29 08:55:52,338 : INFO : PROGRESS: saving document #37000\n",
            "Iterating over jsonl:  23%|██▎       | 484410/2097745 [00:38<01:49, 14704.04it/s]2023-11-29 08:55:53,174 : INFO : PROGRESS: saving document #38000\n",
            "Iterating over jsonl:  24%|██▎       | 496935/2097745 [00:39<02:22, 11246.96it/s]2023-11-29 08:55:54,316 : INFO : PROGRESS: saving document #39000\n",
            "Iterating over jsonl:  24%|██▍       | 508876/2097745 [00:40<02:04, 12762.63it/s]2023-11-29 08:55:55,198 : INFO : PROGRESS: saving document #40000\n",
            "Iterating over jsonl:  25%|██▍       | 520038/2097745 [00:41<01:42, 15403.45it/s]2023-11-29 08:55:55,934 : INFO : PROGRESS: saving document #41000\n",
            "Iterating over jsonl:  25%|██▌       | 532325/2097745 [00:42<01:59, 13098.61it/s]2023-11-29 08:55:56,908 : INFO : PROGRESS: saving document #42000\n",
            "Iterating over jsonl:  26%|██▌       | 546154/2097745 [00:43<01:55, 13463.07it/s]2023-11-29 08:55:57,955 : INFO : PROGRESS: saving document #43000\n",
            "Iterating over jsonl:  27%|██▋       | 557272/2097745 [00:44<02:30, 10225.79it/s]2023-11-29 08:55:59,080 : INFO : PROGRESS: saving document #44000\n",
            "Iterating over jsonl:  27%|██▋       | 568968/2097745 [00:45<01:50, 13793.37it/s]2023-11-29 08:55:59,963 : INFO : PROGRESS: saving document #45000\n",
            "Iterating over jsonl:  28%|██▊       | 580123/2097745 [00:46<02:02, 12392.86it/s]2023-11-29 08:56:00,896 : INFO : PROGRESS: saving document #46000\n",
            "Iterating over jsonl:  28%|██▊       | 593362/2097745 [00:47<01:49, 13763.14it/s]2023-11-29 08:56:01,769 : INFO : PROGRESS: saving document #47000\n",
            "Iterating over jsonl:  29%|██▉       | 604260/2097745 [00:47<01:51, 13425.58it/s]2023-11-29 08:56:02,547 : INFO : PROGRESS: saving document #48000\n",
            "Iterating over jsonl:  29%|██▉       | 617525/2097745 [00:48<01:45, 14058.11it/s]2023-11-29 08:56:03,469 : INFO : PROGRESS: saving document #49000\n",
            "Iterating over jsonl:  30%|███       | 630358/2097745 [00:49<01:37, 15034.07it/s]2023-11-29 08:56:04,259 : INFO : PROGRESS: saving document #50000\n",
            "Iterating over jsonl:  31%|███       | 641526/2097745 [00:50<01:50, 13168.30it/s]2023-11-29 08:56:05,209 : INFO : PROGRESS: saving document #51000\n",
            "Iterating over jsonl:  31%|███       | 653611/2097745 [00:51<01:37, 14884.07it/s]2023-11-29 08:56:05,988 : INFO : PROGRESS: saving document #52000\n",
            "Iterating over jsonl:  32%|███▏      | 665724/2097745 [00:52<01:46, 13457.71it/s]2023-11-29 08:56:06,961 : INFO : PROGRESS: saving document #53000\n",
            "Iterating over jsonl:  32%|███▏      | 677942/2097745 [00:53<01:56, 12141.96it/s]2023-11-29 08:56:07,934 : INFO : PROGRESS: saving document #54000\n",
            "Iterating over jsonl:  33%|███▎      | 694049/2097745 [00:54<01:35, 14634.30it/s]2023-11-29 08:56:09,243 : INFO : PROGRESS: saving document #55000\n",
            "Iterating over jsonl:  34%|███▎      | 705880/2097745 [00:55<01:32, 15068.00it/s]2023-11-29 08:56:09,969 : INFO : PROGRESS: saving document #56000\n",
            "Iterating over jsonl:  34%|███▍      | 716599/2097745 [00:56<01:35, 14527.85it/s]2023-11-29 08:56:10,734 : INFO : PROGRESS: saving document #57000\n",
            "Iterating over jsonl:  35%|███▍      | 730641/2097745 [00:57<01:47, 12764.85it/s]2023-11-29 08:56:11,980 : INFO : PROGRESS: saving document #58000\n",
            "Iterating over jsonl:  35%|███▌      | 742036/2097745 [00:58<01:41, 13404.62it/s]2023-11-29 08:56:12,992 : INFO : PROGRESS: saving document #59000\n",
            "Iterating over jsonl:  36%|███▌      | 755683/2097745 [00:59<01:47, 12509.73it/s]2023-11-29 08:56:14,058 : INFO : PROGRESS: saving document #60000\n",
            "Iterating over jsonl:  37%|███▋      | 770136/2097745 [01:00<01:29, 14812.90it/s]2023-11-29 08:56:14,961 : INFO : PROGRESS: saving document #61000\n",
            "Iterating over jsonl:  37%|███▋      | 782694/2097745 [01:01<01:28, 14795.38it/s]2023-11-29 08:56:15,846 : INFO : PROGRESS: saving document #62000\n",
            "Iterating over jsonl:  38%|███▊      | 795197/2097745 [01:02<01:24, 15372.10it/s]2023-11-29 08:56:16,695 : INFO : PROGRESS: saving document #63000\n",
            "Iterating over jsonl:  38%|███▊      | 805999/2097745 [01:03<01:48, 11912.29it/s]2023-11-29 08:56:17,604 : INFO : PROGRESS: saving document #64000\n",
            "Iterating over jsonl:  39%|███▉      | 819519/2097745 [01:03<01:27, 14652.43it/s]2023-11-29 08:56:18,553 : INFO : PROGRESS: saving document #65000\n",
            "Iterating over jsonl:  40%|███▉      | 830330/2097745 [01:04<01:27, 14518.53it/s]2023-11-29 08:56:19,288 : INFO : PROGRESS: saving document #66000\n",
            "Iterating over jsonl:  40%|████      | 840756/2097745 [01:05<01:29, 14078.69it/s]2023-11-29 08:56:20,086 : INFO : PROGRESS: saving document #67000\n",
            "Iterating over jsonl:  41%|████      | 854097/2097745 [01:06<02:02, 10151.58it/s]2023-11-29 08:56:21,307 : INFO : PROGRESS: saving document #68000\n",
            "Iterating over jsonl:  41%|████▏     | 867231/2097745 [01:07<01:41, 12099.88it/s]2023-11-29 08:56:22,655 : INFO : PROGRESS: saving document #69000\n",
            "Iterating over jsonl:  42%|████▏     | 882064/2097745 [01:09<02:14, 9062.54it/s] 2023-11-29 08:56:23,894 : INFO : PROGRESS: saving document #70000\n",
            "Iterating over jsonl:  43%|████▎     | 894713/2097745 [01:10<01:12, 16531.01it/s]2023-11-29 08:56:24,770 : INFO : PROGRESS: saving document #71000\n",
            "Iterating over jsonl:  43%|████▎     | 906753/2097745 [01:11<01:37, 12245.31it/s]2023-11-29 08:56:25,884 : INFO : PROGRESS: saving document #72000\n",
            "Iterating over jsonl:  44%|████▍     | 920294/2097745 [01:12<01:15, 15683.23it/s]2023-11-29 08:56:26,820 : INFO : PROGRESS: saving document #73000\n",
            "Iterating over jsonl:  45%|████▍     | 934948/2097745 [01:13<01:16, 15250.19it/s]2023-11-29 08:56:28,086 : INFO : PROGRESS: saving document #74000\n",
            "Iterating over jsonl:  45%|████▌     | 949461/2097745 [01:14<01:31, 12570.84it/s]2023-11-29 08:56:29,311 : INFO : PROGRESS: saving document #75000\n",
            "Iterating over jsonl:  46%|████▌     | 961286/2097745 [01:15<01:22, 13811.26it/s]2023-11-29 08:56:30,171 : INFO : PROGRESS: saving document #76000\n",
            "Iterating over jsonl:  47%|████▋     | 975496/2097745 [01:16<01:37, 11556.69it/s]2023-11-29 08:56:31,324 : INFO : PROGRESS: saving document #77000\n",
            "Iterating over jsonl:  47%|████▋     | 987612/2097745 [01:17<01:34, 11754.91it/s]2023-11-29 08:56:32,341 : INFO : PROGRESS: saving document #78000\n",
            "Iterating over jsonl:  48%|████▊     | 999163/2097745 [01:18<01:37, 11212.74it/s]2023-11-29 08:56:33,336 : INFO : PROGRESS: saving document #79000\n",
            "Iterating over jsonl:  48%|████▊     | 1012015/2097745 [01:19<01:28, 12228.88it/s]2023-11-29 08:56:34,445 : INFO : PROGRESS: saving document #80000\n",
            "Iterating over jsonl:  49%|████▉     | 1026652/2097745 [01:20<01:16, 14017.21it/s]2023-11-29 08:56:35,475 : INFO : PROGRESS: saving document #81000\n",
            "Iterating over jsonl:  50%|████▉     | 1040603/2097745 [01:22<01:43, 10225.40it/s]2023-11-29 08:56:36,756 : INFO : PROGRESS: saving document #82000\n",
            "Iterating over jsonl:  50%|█████     | 1054615/2097745 [01:23<01:18, 13253.47it/s]2023-11-29 08:56:37,916 : INFO : PROGRESS: saving document #83000\n",
            "Iterating over jsonl:  51%|█████     | 1067700/2097745 [01:24<01:09, 14856.78it/s]2023-11-29 08:56:38,787 : INFO : PROGRESS: saving document #84000\n",
            "Iterating over jsonl:  52%|█████▏    | 1080869/2097745 [01:25<01:36, 10503.43it/s]2023-11-29 08:56:39,907 : INFO : PROGRESS: saving document #85000\n",
            "Iterating over jsonl:  52%|█████▏    | 1095072/2097745 [01:26<01:12, 13761.44it/s]2023-11-29 08:56:40,949 : INFO : PROGRESS: saving document #86000\n",
            "Iterating over jsonl:  53%|█████▎    | 1105725/2097745 [01:27<01:05, 15039.11it/s]2023-11-29 08:56:41,757 : INFO : PROGRESS: saving document #87000\n",
            "Iterating over jsonl:  53%|█████▎    | 1119065/2097745 [01:28<01:14, 13162.40it/s]2023-11-29 08:56:42,764 : INFO : PROGRESS: saving document #88000\n",
            "Iterating over jsonl:  54%|█████▍    | 1131004/2097745 [01:29<01:05, 14723.61it/s]2023-11-29 08:56:43,609 : INFO : PROGRESS: saving document #89000\n",
            "Iterating over jsonl:  54%|█████▍    | 1141284/2097745 [01:29<01:29, 10682.19it/s]2023-11-29 08:56:44,554 : INFO : PROGRESS: saving document #90000\n",
            "Iterating over jsonl:  55%|█████▌    | 1154921/2097745 [01:30<01:32, 10224.24it/s]2023-11-29 08:56:45,630 : INFO : PROGRESS: saving document #91000\n",
            "Iterating over jsonl:  56%|█████▌    | 1168779/2097745 [01:31<01:13, 12661.61it/s]2023-11-29 08:56:46,630 : INFO : PROGRESS: saving document #92000\n",
            "Iterating over jsonl:  56%|█████▋    | 1182718/2097745 [01:32<01:02, 14689.98it/s]2023-11-29 08:56:47,554 : INFO : PROGRESS: saving document #93000\n",
            "Iterating over jsonl:  57%|█████▋    | 1196004/2097745 [01:34<01:52, 7994.86it/s] 2023-11-29 08:56:48,879 : INFO : PROGRESS: saving document #94000\n",
            "Iterating over jsonl:  58%|█████▊    | 1210256/2097745 [01:35<01:13, 12072.06it/s]2023-11-29 08:56:49,934 : INFO : PROGRESS: saving document #95000\n",
            "Iterating over jsonl:  58%|█████▊    | 1221831/2097745 [01:36<01:08, 12809.34it/s]2023-11-29 08:56:50,945 : INFO : PROGRESS: saving document #96000\n",
            "Iterating over jsonl:  59%|█████▉    | 1235098/2097745 [01:37<01:23, 10274.15it/s]2023-11-29 08:56:51,970 : INFO : PROGRESS: saving document #97000\n",
            "Iterating over jsonl:  60%|█████▉    | 1249527/2097745 [01:38<01:06, 12814.57it/s]2023-11-29 08:56:53,418 : INFO : PROGRESS: saving document #98000\n",
            "Iterating over jsonl:  60%|██████    | 1262945/2097745 [01:39<01:03, 13135.63it/s]2023-11-29 08:56:54,460 : INFO : PROGRESS: saving document #99000\n",
            "Iterating over jsonl:  61%|██████    | 1277268/2097745 [01:40<01:03, 12987.32it/s]2023-11-29 08:56:55,492 : INFO : PROGRESS: saving document #100000\n",
            "Iterating over jsonl:  61%|██████▏   | 1286785/2097745 [01:41<00:58, 13974.73it/s]2023-11-29 08:56:56,215 : INFO : PROGRESS: saving document #101000\n",
            "Iterating over jsonl:  62%|██████▏   | 1301742/2097745 [01:42<01:01, 12897.64it/s]2023-11-29 08:56:57,296 : INFO : PROGRESS: saving document #102000\n",
            "Iterating over jsonl:  62%|██████▏   | 1310841/2097745 [01:43<00:51, 15364.22it/s]2023-11-29 08:56:58,007 : INFO : PROGRESS: saving document #103000\n",
            "Iterating over jsonl:  63%|██████▎   | 1325902/2097745 [01:44<01:08, 11332.50it/s]2023-11-29 08:56:59,406 : INFO : PROGRESS: saving document #104000\n",
            "Iterating over jsonl:  64%|██████▍   | 1338494/2097745 [01:45<00:49, 15445.72it/s]2023-11-29 08:57:00,250 : INFO : PROGRESS: saving document #105000\n",
            "Iterating over jsonl:  64%|██████▍   | 1350943/2097745 [01:46<00:49, 15146.51it/s]2023-11-29 08:57:01,111 : INFO : PROGRESS: saving document #106000\n",
            "Iterating over jsonl:  65%|██████▌   | 1366803/2097745 [01:48<01:02, 11711.33it/s]2023-11-29 08:57:02,637 : INFO : PROGRESS: saving document #107000\n",
            "Iterating over jsonl:  66%|██████▌   | 1380751/2097745 [01:49<01:02, 11448.09it/s]2023-11-29 08:57:03,825 : INFO : PROGRESS: saving document #108000\n",
            "Iterating over jsonl:  66%|██████▋   | 1393131/2097745 [01:50<00:45, 15393.13it/s]2023-11-29 08:57:04,652 : INFO : PROGRESS: saving document #109000\n",
            "Iterating over jsonl:  67%|██████▋   | 1405746/2097745 [01:51<00:52, 13151.70it/s]2023-11-29 08:57:05,713 : INFO : PROGRESS: saving document #110000\n",
            "Iterating over jsonl:  68%|██████▊   | 1418667/2097745 [01:52<01:06, 10182.53it/s]2023-11-29 08:57:06,949 : INFO : PROGRESS: saving document #111000\n",
            "Iterating over jsonl:  68%|██████▊   | 1431974/2097745 [01:53<00:48, 13705.85it/s]2023-11-29 08:57:07,900 : INFO : PROGRESS: saving document #112000\n",
            "Iterating over jsonl:  69%|██████▉   | 1443644/2097745 [01:54<00:51, 12750.40it/s]2023-11-29 08:57:08,833 : INFO : PROGRESS: saving document #113000\n",
            "Iterating over jsonl:  69%|██████▉   | 1455636/2097745 [01:55<00:57, 11160.19it/s]2023-11-29 08:57:09,867 : INFO : PROGRESS: saving document #114000\n",
            "Iterating over jsonl:  70%|███████   | 1470294/2097745 [01:56<00:44, 14040.66it/s]2023-11-29 08:57:10,910 : INFO : PROGRESS: saving document #115000\n",
            "Iterating over jsonl:  71%|███████   | 1483453/2097745 [01:57<00:45, 13485.15it/s]2023-11-29 08:57:11,747 : INFO : PROGRESS: saving document #116000\n",
            "Iterating over jsonl:  71%|███████▏  | 1496630/2097745 [01:58<00:40, 14742.06it/s]2023-11-29 08:57:12,615 : INFO : PROGRESS: saving document #117000\n",
            "Iterating over jsonl:  72%|███████▏  | 1508510/2097745 [01:58<00:47, 12476.50it/s]2023-11-29 08:57:13,550 : INFO : PROGRESS: saving document #118000\n",
            "Iterating over jsonl:  73%|███████▎  | 1522581/2097745 [01:59<00:43, 13341.76it/s]2023-11-29 08:57:14,532 : INFO : PROGRESS: saving document #119000\n",
            "Iterating over jsonl:  73%|███████▎  | 1532779/2097745 [02:00<00:43, 12962.58it/s]2023-11-29 08:57:15,431 : INFO : PROGRESS: saving document #120000\n",
            "Iterating over jsonl:  74%|███████▎  | 1545455/2097745 [02:01<00:32, 16844.60it/s]2023-11-29 08:57:16,200 : INFO : PROGRESS: saving document #121000\n",
            "Iterating over jsonl:  74%|███████▍  | 1558552/2097745 [02:02<00:35, 14985.15it/s]2023-11-29 08:57:17,083 : INFO : PROGRESS: saving document #122000\n",
            "Iterating over jsonl:  75%|███████▍  | 1569685/2097745 [02:03<00:44, 11892.03it/s]2023-11-29 08:57:18,059 : INFO : PROGRESS: saving document #123000\n",
            "Iterating over jsonl:  75%|███████▌  | 1581707/2097745 [02:04<00:44, 11609.23it/s]2023-11-29 08:57:19,130 : INFO : PROGRESS: saving document #124000\n",
            "Iterating over jsonl:  76%|███████▌  | 1592543/2097745 [02:05<00:39, 12734.10it/s]2023-11-29 08:57:20,039 : INFO : PROGRESS: saving document #125000\n",
            "Iterating over jsonl:  77%|███████▋  | 1606551/2097745 [02:06<00:32, 15019.73it/s]2023-11-29 08:57:20,973 : INFO : PROGRESS: saving document #126000\n",
            "Iterating over jsonl:  77%|███████▋  | 1620528/2097745 [02:07<00:38, 12349.56it/s]2023-11-29 08:57:22,144 : INFO : PROGRESS: saving document #127000\n",
            "Iterating over jsonl:  78%|███████▊  | 1634439/2097745 [02:08<00:34, 13324.80it/s]2023-11-29 08:57:23,247 : INFO : PROGRESS: saving document #128000\n",
            "Iterating over jsonl:  78%|███████▊  | 1645569/2097745 [02:09<00:33, 13649.84it/s]2023-11-29 08:57:24,173 : INFO : PROGRESS: saving document #129000\n",
            "Iterating over jsonl:  79%|███████▉  | 1660124/2097745 [02:10<00:36, 12027.26it/s]2023-11-29 08:57:25,335 : INFO : PROGRESS: saving document #130000\n",
            "Iterating over jsonl:  80%|███████▉  | 1672101/2097745 [02:11<00:32, 13216.25it/s]2023-11-29 08:57:26,256 : INFO : PROGRESS: saving document #131000\n",
            "Iterating over jsonl:  80%|████████  | 1682721/2097745 [02:12<00:29, 14195.22it/s]2023-11-29 08:57:27,026 : INFO : PROGRESS: saving document #132000\n",
            "Iterating over jsonl:  81%|████████  | 1694568/2097745 [02:13<00:34, 11682.87it/s]2023-11-29 08:57:28,082 : INFO : PROGRESS: saving document #133000\n",
            "Iterating over jsonl:  81%|████████▏ | 1709048/2097745 [02:14<00:26, 14652.16it/s]2023-11-29 08:57:29,180 : INFO : PROGRESS: saving document #134000\n",
            "Iterating over jsonl:  82%|████████▏ | 1722782/2097745 [02:15<00:28, 13272.13it/s]2023-11-29 08:57:30,251 : INFO : PROGRESS: saving document #135000\n",
            "Iterating over jsonl:  83%|████████▎ | 1740035/2097745 [02:17<00:33, 10531.00it/s]2023-11-29 08:57:31,854 : INFO : PROGRESS: saving document #136000\n",
            "Iterating over jsonl:  84%|████████▎ | 1756671/2097745 [02:18<00:32, 10592.07it/s]2023-11-29 08:57:33,118 : INFO : PROGRESS: saving document #137000\n",
            "Iterating over jsonl:  84%|████████▍ | 1770646/2097745 [02:19<00:23, 13750.65it/s]2023-11-29 08:57:34,120 : INFO : PROGRESS: saving document #138000\n",
            "Iterating over jsonl:  85%|████████▌ | 1783625/2097745 [02:20<00:20, 15131.67it/s]2023-11-29 08:57:34,971 : INFO : PROGRESS: saving document #139000\n",
            "Iterating over jsonl:  86%|████████▌ | 1795402/2097745 [02:21<00:21, 13811.03it/s]2023-11-29 08:57:35,854 : INFO : PROGRESS: saving document #140000\n",
            "Iterating over jsonl:  86%|████████▌ | 1808826/2097745 [02:22<00:19, 14477.28it/s]2023-11-29 08:57:36,859 : INFO : PROGRESS: saving document #141000\n",
            "Iterating over jsonl:  87%|████████▋ | 1824833/2097745 [02:23<00:19, 14293.63it/s]2023-11-29 08:57:38,054 : INFO : PROGRESS: saving document #142000\n",
            "Iterating over jsonl:  88%|████████▊ | 1836034/2097745 [02:24<00:15, 16477.80it/s]2023-11-29 08:57:38,784 : INFO : PROGRESS: saving document #143000\n",
            "Iterating over jsonl:  88%|████████▊ | 1849677/2097745 [02:25<00:22, 11006.53it/s]2023-11-29 08:57:39,906 : INFO : PROGRESS: saving document #144000\n",
            "Iterating over jsonl:  89%|████████▉ | 1863744/2097745 [02:26<00:21, 11035.44it/s]2023-11-29 08:57:41,112 : INFO : PROGRESS: saving document #145000\n",
            "Iterating over jsonl:  89%|████████▉ | 1875501/2097745 [02:27<00:16, 13242.78it/s]2023-11-29 08:57:41,968 : INFO : PROGRESS: saving document #146000\n",
            "Iterating over jsonl:  90%|█████████ | 1888758/2097745 [02:28<00:14, 14687.30it/s]2023-11-29 08:57:42,807 : INFO : PROGRESS: saving document #147000\n",
            "Iterating over jsonl:  91%|█████████ | 1904528/2097745 [02:29<00:18, 10351.48it/s]2023-11-29 08:57:44,197 : INFO : PROGRESS: saving document #148000\n",
            "Iterating over jsonl:  91%|█████████▏| 1916810/2097745 [02:30<00:19, 9318.31it/s] 2023-11-29 08:57:45,294 : INFO : PROGRESS: saving document #149000\n",
            "Iterating over jsonl:  92%|█████████▏| 1927987/2097745 [02:31<00:14, 12048.99it/s]2023-11-29 08:57:46,259 : INFO : PROGRESS: saving document #150000\n",
            "Iterating over jsonl:  93%|█████████▎| 1942175/2097745 [02:32<00:10, 14243.46it/s]2023-11-29 08:57:47,141 : INFO : PROGRESS: saving document #151000\n",
            "Iterating over jsonl:  93%|█████████▎| 1956111/2097745 [02:33<00:09, 15099.19it/s]2023-11-29 08:57:48,149 : INFO : PROGRESS: saving document #152000\n",
            "Iterating over jsonl:  94%|█████████▍| 1968528/2097745 [02:34<00:10, 12524.42it/s]2023-11-29 08:57:49,010 : INFO : PROGRESS: saving document #153000\n",
            "Iterating over jsonl:  94%|█████████▍| 1981349/2097745 [02:35<00:09, 12702.99it/s]2023-11-29 08:57:50,056 : INFO : PROGRESS: saving document #154000\n",
            "Iterating over jsonl:  95%|█████████▌| 1995773/2097745 [02:36<00:06, 16132.35it/s]2023-11-29 08:57:50,959 : INFO : PROGRESS: saving document #155000\n",
            "Iterating over jsonl:  96%|█████████▌| 2007746/2097745 [02:37<00:06, 12947.02it/s]2023-11-29 08:57:51,947 : INFO : PROGRESS: saving document #156000\n",
            "Iterating over jsonl:  96%|█████████▋| 2019624/2097745 [02:38<00:05, 14862.95it/s]2023-11-29 08:57:52,812 : INFO : PROGRESS: saving document #157000\n",
            "Iterating over jsonl:  97%|█████████▋| 2032201/2097745 [02:39<00:07, 9264.35it/s] 2023-11-29 08:57:54,031 : INFO : PROGRESS: saving document #158000\n",
            "Iterating over jsonl:  97%|█████████▋| 2044736/2097745 [02:40<00:04, 10940.33it/s]2023-11-29 08:57:55,218 : INFO : PROGRESS: saving document #159000\n",
            "Iterating over jsonl:  98%|█████████▊| 2057492/2097745 [02:41<00:03, 12769.20it/s]2023-11-29 08:57:56,220 : INFO : PROGRESS: saving document #160000\n",
            "Iterating over jsonl:  99%|█████████▊| 2070804/2097745 [02:42<00:02, 11613.63it/s]2023-11-29 08:57:57,223 : INFO : PROGRESS: saving document #161000\n",
            "Iterating over jsonl:  99%|█████████▉| 2084932/2097745 [02:43<00:01, 10663.99it/s]2023-11-29 08:57:58,561 : INFO : PROGRESS: saving document #162000\n",
            "Iterating over jsonl: 100%|█████████▉| 2095977/2097745 [02:44<00:00, 14538.70it/s]2023-11-29 08:57:59,405 : INFO : PROGRESS: saving document #163000\n",
            "Iterating over jsonl: 100%|██████████| 2097745/2097745 [02:44<00:00, 12725.07it/s]\n",
            "2023-11-29 08:57:59,428 : INFO : saved 163069x100000 matrix, density=0.110% (17994269/16306900000)\n",
            "2023-11-29 08:57:59,429 : INFO : saving MmCorpus index to /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.corpus.pkl.index\n",
            "2023-11-29 08:57:59,455 : INFO : loaded corpus index from /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.corpus.pkl.index\n",
            "2023-11-29 08:57:59,456 : INFO : initializing cython corpus reader from /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.corpus.pkl\n",
            "2023-11-29 08:57:59,457 : INFO : accepted corpus with 163069 documents, 100000 features, 17994269 non-zero entries\n"
          ]
        }
      ],
      "source": [
        "corpus = get_gensim_corpus(force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "083_UQPlJZy_",
        "outputId": "fd84c9d0-7cc4-4645-9d4f-df2f8ca616f7"
      },
      "outputs": [],
      "source": [
        "def topic_model(num_topics=TOTAL_TOPICS, sample=1000, force=False):\n",
        "    fn=os.path.join(path_corpus,f'data.gensim.lda_model.ntopic={num_topics}.pkl')\n",
        "    corpus_sample = corpus\n",
        "\n",
        "    if force or not os.path.exists(fn):\n",
        "        lda_model = gensim.models.ldamulticore.LdaMulticore(\n",
        "            corpus=corpus_sample, \n",
        "            id2word=dictionary,\n",
        "            num_topics=num_topics,\n",
        "            workers=7,\n",
        "            # chunksize=100,\n",
        "            # alpha='auto', \n",
        "            # eta='auto', \n",
        "            random_state=42,\n",
        "            iterations=50, \n",
        "            passes=5, \n",
        "            # eval_every=None\n",
        "        )\n",
        "        lda_model.save(fn)\n",
        "    else:\n",
        "        lda_model = gensim.models.LdaModel.load(fn)\n",
        "    return lda_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 09:14:37,436 : INFO : using symmetric alpha at 0.01\n",
            "2023-11-29 09:14:37,437 : INFO : using symmetric eta at 0.01\n",
            "2023-11-29 09:14:37,457 : INFO : using serial LDA version on this node\n",
            "2023-11-29 09:14:38,720 : INFO : running online LDA training, 100 topics, 5 passes over the supplied corpus of 163069 documents, updating every 14000 documents, evaluating every ~140000 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "2023-11-29 09:14:38,721 : INFO : training LDA model using 7 processes\n",
            "2023-11-29 09:14:49,508 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #2000/163069, outstanding queue size 1\n",
            "2023-11-29 09:14:49,827 : INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #4000/163069, outstanding queue size 2\n",
            "2023-11-29 09:14:50,090 : INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #6000/163069, outstanding queue size 3\n",
            "2023-11-29 09:14:50,355 : INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #8000/163069, outstanding queue size 4\n",
            "2023-11-29 09:14:50,698 : INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #10000/163069, outstanding queue size 5\n",
            "2023-11-29 09:14:51,080 : INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #12000/163069, outstanding queue size 6\n",
            "2023-11-29 09:14:51,495 : INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #14000/163069, outstanding queue size 7\n",
            "2023-11-29 09:14:51,931 : INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #16000/163069, outstanding queue size 8\n",
            "2023-11-29 09:14:52,341 : INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #18000/163069, outstanding queue size 9\n",
            "2023-11-29 09:14:52,574 : INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #20000/163069, outstanding queue size 10\n",
            "2023-11-29 09:14:52,831 : INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #22000/163069, outstanding queue size 11\n",
            "2023-11-29 09:14:53,102 : INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #24000/163069, outstanding queue size 12\n",
            "2023-11-29 09:14:53,344 : INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #26000/163069, outstanding queue size 13\n",
            "2023-11-29 09:14:53,610 : INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #28000/163069, outstanding queue size 14\n",
            "2023-11-29 09:14:53,877 : INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #30000/163069, outstanding queue size 15\n",
            "2023-11-29 09:14:54,124 : INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #32000/163069, outstanding queue size 16\n",
            "2023-11-29 09:14:54,382 : INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #34000/163069, outstanding queue size 17\n",
            "2023-11-29 09:14:54,667 : INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #36000/163069, outstanding queue size 18\n",
            "2023-11-29 09:14:54,908 : INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #38000/163069, outstanding queue size 19\n",
            "2023-11-29 09:14:55,189 : INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #40000/163069, outstanding queue size 20\n",
            "2023-11-29 09:14:55,447 : INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #42000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:10,526 : INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #44000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:10,946 : INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #46000/163069, outstanding queue size 22\n",
            "2023-11-29 09:15:11,190 : INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #48000/163069, outstanding queue size 23\n",
            "2023-11-29 09:15:16,674 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:15:17,516 : INFO : topic #51 (0.010): 0.005*\"words\" + 0.004*\"upon\" + 0.003*\"english\" + 0.003*\"would\" + 0.003*\"john\" + 0.003*\"part\" + 0.003*\"first\" + 0.003*\"sound\" + 0.003*\"see\" + 0.002*\"man\"\n",
            "2023-11-29 09:15:17,520 : INFO : topic #22 (0.010): 0.004*\"verse\" + 0.004*\"would\" + 0.004*\"words\" + 0.003*\"english\" + 0.003*\"word\" + 0.003*\"time\" + 0.003*\"made\" + 0.003*\"upon\" + 0.003*\"every\" + 0.003*\"great\"\n",
            "2023-11-29 09:15:17,524 : INFO : topic #90 (0.010): 0.006*\"words\" + 0.005*\"language\" + 0.004*\"english\" + 0.004*\"art\" + 0.003*\"must\" + 0.003*\"would\" + 0.003*\"first\" + 0.003*\"use\" + 0.003*\"much\" + 0.003*\"word\"\n",
            "2023-11-29 09:15:17,529 : INFO : topic #69 (0.010): 0.005*\"lesson\" + 0.005*\"two\" + 0.004*\"word\" + 0.003*\"upon\" + 0.003*\"words\" + 0.003*\"used\" + 0.002*\"book\" + 0.002*\"also\" + 0.002*\"form\" + 0.002*\"der\"\n",
            "2023-11-29 09:15:17,533 : INFO : topic #24 (0.010): 0.006*\"words\" + 0.004*\"would\" + 0.004*\"english\" + 0.004*\"book\" + 0.003*\"time\" + 0.003*\"every\" + 0.003*\"art\" + 0.003*\"work\" + 0.003*\"first\" + 0.003*\"could\"\n",
            "2023-11-29 09:15:17,561 : INFO : topic diff=73.107765, rho=1.000000\n",
            "2023-11-29 09:15:17,788 : INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #50000/163069, outstanding queue size 18\n",
            "2023-11-29 09:15:18,034 : INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #52000/163069, outstanding queue size 19\n",
            "2023-11-29 09:15:18,292 : INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #54000/163069, outstanding queue size 20\n",
            "2023-11-29 09:15:18,519 : INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #56000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:31,234 : INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #58000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:31,686 : INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #60000/163069, outstanding queue size 22\n",
            "2023-11-29 09:15:32,966 : INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #62000/163069, outstanding queue size 22\n",
            "2023-11-29 09:15:33,192 : INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #64000/163069, outstanding queue size 23\n",
            "2023-11-29 09:15:38,207 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:15:39,032 : INFO : topic #80 (0.010): 0.005*\"language\" + 0.005*\"words\" + 0.004*\"man\" + 0.004*\"time\" + 0.004*\"word\" + 0.003*\"first\" + 0.003*\"without\" + 0.003*\"thought\" + 0.003*\"use\" + 0.003*\"would\"\n",
            "2023-11-29 09:15:39,037 : INFO : topic #97 (0.010): 0.006*\"would\" + 0.005*\"first\" + 0.004*\"words\" + 0.003*\"letters\" + 0.003*\"like\" + 0.003*\"language\" + 0.003*\"word\" + 0.003*\"must\" + 0.003*\"used\" + 0.003*\"many\"\n",
            "2023-11-29 09:15:39,043 : INFO : topic #55 (0.010): 0.007*\"words\" + 0.006*\"sentence\" + 0.004*\"poetry\" + 0.003*\"word\" + 0.003*\"form\" + 0.003*\"language\" + 0.003*\"must\" + 0.003*\"two\" + 0.003*\"nouns\" + 0.003*\"sentences\"\n",
            "2023-11-29 09:15:39,048 : INFO : topic #71 (0.010): 0.004*\"english\" + 0.003*\"words\" + 0.003*\"every\" + 0.003*\"upon\" + 0.003*\"poetry\" + 0.003*\"language\" + 0.003*\"much\" + 0.003*\"time\" + 0.002*\"two\" + 0.002*\"well\"\n",
            "2023-11-29 09:15:39,053 : INFO : topic #92 (0.010): 0.005*\"verb\" + 0.005*\"words\" + 0.004*\"would\" + 0.004*\"person\" + 0.004*\"love\" + 0.004*\"first\" + 0.004*\"sound\" + 0.003*\"word\" + 0.003*\"must\" + 0.003*\"english\"\n",
            "2023-11-29 09:15:39,079 : INFO : topic diff=8.703310, rho=0.353553\n",
            "2023-11-29 09:15:39,114 : INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #66000/163069, outstanding queue size 19\n",
            "2023-11-29 09:15:39,389 : INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #68000/163069, outstanding queue size 20\n",
            "2023-11-29 09:15:39,677 : INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #70000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:53,086 : INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #72000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:54,578 : INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #74000/163069, outstanding queue size 21\n",
            "2023-11-29 09:15:54,818 : INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #76000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:00,259 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:16:01,083 : INFO : topic #3 (0.010): 0.004*\"english\" + 0.004*\"two\" + 0.004*\"time\" + 0.004*\"love\" + 0.003*\"use\" + 0.003*\"words\" + 0.003*\"must\" + 0.003*\"part\" + 0.003*\"first\" + 0.002*\"made\"\n",
            "2023-11-29 09:16:01,088 : INFO : topic #43 (0.010): 0.004*\"time\" + 0.004*\"english\" + 0.003*\"first\" + 0.003*\"great\" + 0.003*\"words\" + 0.003*\"man\" + 0.003*\"must\" + 0.003*\"would\" + 0.003*\"made\" + 0.003*\"also\"\n",
            "2023-11-29 09:16:01,094 : INFO : topic #99 (0.010): 0.005*\"would\" + 0.004*\"many\" + 0.004*\"must\" + 0.004*\"new\" + 0.004*\"english\" + 0.003*\"much\" + 0.003*\"upon\" + 0.003*\"great\" + 0.003*\"time\" + 0.003*\"mind\"\n",
            "2023-11-29 09:16:01,098 : INFO : topic #58 (0.010): 0.004*\"would\" + 0.004*\"upon\" + 0.003*\"must\" + 0.003*\"time\" + 0.003*\"two\" + 0.002*\"words\" + 0.002*\"voice\" + 0.002*\"part\" + 0.002*\"form\" + 0.002*\"word\"\n",
            "2023-11-29 09:16:01,105 : INFO : topic #49 (0.010): 0.007*\"words\" + 0.005*\"two\" + 0.005*\"word\" + 0.004*\"english\" + 0.003*\"part\" + 0.003*\"first\" + 0.003*\"sound\" + 0.003*\"would\" + 0.003*\"sounds\" + 0.003*\"many\"\n",
            "2023-11-29 09:16:01,133 : INFO : topic diff=1.876281, rho=0.258199\n",
            "2023-11-29 09:16:01,160 : INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #78000/163069, outstanding queue size 18\n",
            "2023-11-29 09:16:01,467 : INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #80000/163069, outstanding queue size 19\n",
            "2023-11-29 09:16:01,708 : INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #82000/163069, outstanding queue size 20\n",
            "2023-11-29 09:16:01,928 : INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #84000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:13,707 : INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #86000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:15,292 : INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #88000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:15,807 : INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #90000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:17,170 : INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #92000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:18,396 : INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #94000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:18,782 : INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #96000/163069, outstanding queue size 23\n",
            "2023-11-29 09:16:20,001 : INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #98000/163069, outstanding queue size 23\n",
            "2023-11-29 09:16:22,254 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:16:23,079 : INFO : topic #65 (0.010): 0.016*\"word\" + 0.013*\"words\" + 0.013*\"syllable\" + 0.010*\"two\" + 0.006*\"first\" + 0.006*\"accent\" + 0.005*\"vowel\" + 0.004*\"long\" + 0.004*\"english\" + 0.004*\"found\"\n",
            "2023-11-29 09:16:23,086 : INFO : topic #39 (0.010): 0.009*\"english\" + 0.005*\"words\" + 0.005*\"poetry\" + 0.004*\"latin\" + 0.003*\"upon\" + 0.003*\"french\" + 0.003*\"two\" + 0.003*\"many\" + 0.003*\"first\" + 0.003*\"word\"\n",
            "2023-11-29 09:16:23,090 : INFO : topic #46 (0.010): 0.007*\"words\" + 0.006*\"word\" + 0.006*\"der\" + 0.006*\"english\" + 0.005*\"die\" + 0.004*\"time\" + 0.004*\"good\" + 0.003*\"first\" + 0.003*\"und\" + 0.003*\"would\"\n",
            "2023-11-29 09:16:23,094 : INFO : topic #74 (0.010): 0.025*\"words\" + 0.008*\"word\" + 0.007*\"language\" + 0.005*\"found\" + 0.004*\"syllables\" + 0.004*\"sounds\" + 0.004*\"two\" + 0.004*\"long\" + 0.004*\"syllable\" + 0.004*\"like\"\n",
            "2023-11-29 09:16:23,100 : INFO : topic #50 (0.010): 0.004*\"two\" + 0.003*\"words\" + 0.003*\"great\" + 0.003*\"upon\" + 0.003*\"letters\" + 0.003*\"english\" + 0.002*\"would\" + 0.002*\"part\" + 0.002*\"fame\" + 0.002*\"many\"\n",
            "2023-11-29 09:16:23,136 : INFO : topic diff=0.491880, rho=0.213201\n",
            "2023-11-29 09:16:36,159 : INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #100000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:37,958 : INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #102000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:39,544 : INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #104000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:39,764 : INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #106000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:40,986 : INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #108000/163069, outstanding queue size 22\n",
            "2023-11-29 09:16:41,353 : INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #110000/163069, outstanding queue size 23\n",
            "2023-11-29 09:16:44,638 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:16:45,457 : INFO : topic #21 (0.010): 0.011*\"english\" + 0.006*\"poetry\" + 0.005*\"style\" + 0.004*\"language\" + 0.004*\"two\" + 0.003*\"present\" + 0.003*\"upon\" + 0.003*\"first\" + 0.003*\"work\" + 0.003*\"words\"\n",
            "2023-11-29 09:16:45,469 : INFO : topic #66 (0.010): 0.005*\"oxon\" + 0.004*\"coll\" + 0.003*\"rev\" + 0.003*\"life\" + 0.002*\"two\" + 0.002*\"king\" + 0.002*\"words\" + 0.002*\"man\" + 0.002*\"upon\" + 0.002*\"english\"\n",
            "2023-11-29 09:16:45,476 : INFO : topic #70 (0.010): 0.006*\"book\" + 0.005*\"language\" + 0.004*\"work\" + 0.004*\"must\" + 0.004*\"art\" + 0.004*\"grammar\" + 0.004*\"study\" + 0.004*\"well\" + 0.004*\"use\" + 0.003*\"school\"\n",
            "2023-11-29 09:16:45,481 : INFO : topic #42 (0.010): 0.003*\"man\" + 0.003*\"old\" + 0.003*\"must\" + 0.003*\"time\" + 0.003*\"upon\" + 0.003*\"men\" + 0.003*\"god\" + 0.003*\"would\" + 0.003*\"good\" + 0.003*\"shall\"\n",
            "2023-11-29 09:16:45,485 : INFO : topic #6 (0.010): 0.005*\"words\" + 0.004*\"sound\" + 0.004*\"teacher\" + 0.004*\"english\" + 0.004*\"sounds\" + 0.003*\"word\" + 0.003*\"letters\" + 0.003*\"many\" + 0.003*\"made\" + 0.003*\"use\"\n",
            "2023-11-29 09:16:45,514 : INFO : topic diff=0.467840, rho=0.185695\n",
            "2023-11-29 09:16:45,781 : INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #112000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:56,235 : INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #114000/163069, outstanding queue size 21\n",
            "2023-11-29 09:16:58,476 : INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #116000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:00,162 : INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #118000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:00,649 : INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #120000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:02,192 : INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #122000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:05,557 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:17:06,382 : INFO : topic #96 (0.010): 0.005*\"first\" + 0.004*\"time\" + 0.004*\"would\" + 0.003*\"new\" + 0.003*\"two\" + 0.003*\"upon\" + 0.003*\"without\" + 0.002*\"well\" + 0.002*\"part\" + 0.002*\"place\"\n",
            "2023-11-29 09:17:06,387 : INFO : topic #63 (0.010): 0.010*\"sound\" + 0.009*\"first\" + 0.007*\"word\" + 0.006*\"words\" + 0.006*\"english\" + 0.004*\"second\" + 0.003*\"represents\" + 0.003*\"two\" + 0.003*\"grammar\" + 0.003*\"language\"\n",
            "2023-11-29 09:17:06,392 : INFO : topic #16 (0.010): 0.004*\"upon\" + 0.003*\"well\" + 0.003*\"love\" + 0.003*\"like\" + 0.002*\"would\" + 0.002*\"man\" + 0.002*\"old\" + 0.002*\"two\" + 0.002*\"life\" + 0.002*\"time\"\n",
            "2023-11-29 09:17:06,398 : INFO : topic #35 (0.010): 0.007*\"verse\" + 0.006*\"words\" + 0.005*\"english\" + 0.004*\"poetry\" + 0.004*\"upon\" + 0.004*\"would\" + 0.004*\"two\" + 0.003*\"syllables\" + 0.003*\"word\" + 0.003*\"language\"\n",
            "2023-11-29 09:17:06,402 : INFO : topic #2 (0.010): 0.009*\"que\" + 0.006*\"les\" + 0.005*\"english\" + 0.004*\"qui\" + 0.003*\"dans\" + 0.003*\"che\" + 0.003*\"des\" + 0.003*\"une\" + 0.003*\"see\" + 0.003*\"los\"\n",
            "2023-11-29 09:17:06,429 : INFO : topic diff=0.445477, rho=0.166667\n",
            "2023-11-29 09:17:06,712 : INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #124000/163069, outstanding queue size 20\n",
            "2023-11-29 09:17:06,968 : INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #126000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:16,548 : INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #128000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:17,875 : INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #130000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:20,624 : INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #132000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:22,233 : INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #134000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:22,478 : INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #136000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:22,833 : INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #138000/163069, outstanding queue size 23\n",
            "2023-11-29 09:17:24,776 : INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #140000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:26,637 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:17:27,463 : INFO : topic #37 (0.010): 0.024*\"sound\" + 0.015*\"sounds\" + 0.010*\"voice\" + 0.009*\"vowels\" + 0.008*\"like\" + 0.007*\"two\" + 0.007*\"consonants\" + 0.007*\"vowel\" + 0.006*\"letters\" + 0.006*\"vocal\"\n",
            "2023-11-29 09:17:27,467 : INFO : topic #98 (0.010): 0.004*\"two\" + 0.004*\"would\" + 0.003*\"first\" + 0.003*\"verse\" + 0.003*\"time\" + 0.003*\"part\" + 0.003*\"called\" + 0.003*\"great\" + 0.003*\"upon\" + 0.003*\"vowel\"\n",
            "2023-11-29 09:17:27,473 : INFO : topic #12 (0.010): 0.007*\"words\" + 0.005*\"man\" + 0.004*\"word\" + 0.004*\"number\" + 0.004*\"gender\" + 0.004*\"male\" + 0.003*\"female\" + 0.003*\"would\" + 0.003*\"many\" + 0.003*\"part\"\n",
            "2023-11-29 09:17:27,480 : INFO : topic #27 (0.010): 0.009*\"words\" + 0.007*\"accent\" + 0.006*\"new\" + 0.005*\"word\" + 0.005*\"first\" + 0.005*\"two\" + 0.004*\"syllables\" + 0.004*\"see\" + 0.004*\"upon\" + 0.003*\"made\"\n",
            "2023-11-29 09:17:27,484 : INFO : topic #39 (0.010): 0.011*\"english\" + 0.007*\"latin\" + 0.006*\"poetry\" + 0.006*\"french\" + 0.005*\"words\" + 0.004*\"history\" + 0.003*\"first\" + 0.003*\"two\" + 0.003*\"many\" + 0.003*\"upon\"\n",
            "2023-11-29 09:17:27,512 : INFO : topic diff=0.431067, rho=0.152499\n",
            "2023-11-29 09:17:37,938 : INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #142000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:38,170 : INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #144000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:41,137 : INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #146000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:42,406 : INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #148000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:43,748 : INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #150000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:43,967 : INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #152000/163069, outstanding queue size 22\n",
            "2023-11-29 09:17:46,729 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:17:47,552 : INFO : topic #90 (0.010): 0.011*\"art\" + 0.007*\"language\" + 0.006*\"section\" + 0.005*\"must\" + 0.004*\"speech\" + 0.004*\"rules\" + 0.004*\"words\" + 0.004*\"english\" + 0.004*\"every\" + 0.004*\"principles\"\n",
            "2023-11-29 09:17:47,557 : INFO : topic #2 (0.010): 0.013*\"que\" + 0.010*\"les\" + 0.007*\"qui\" + 0.005*\"des\" + 0.004*\"dans\" + 0.004*\"english\" + 0.004*\"che\" + 0.004*\"est\" + 0.004*\"une\" + 0.003*\"del\"\n",
            "2023-11-29 09:17:47,563 : INFO : topic #97 (0.010): 0.008*\"would\" + 0.006*\"first\" + 0.005*\"letters\" + 0.004*\"words\" + 0.004*\"tense\" + 0.004*\"time\" + 0.004*\"used\" + 0.004*\"word\" + 0.004*\"like\" + 0.004*\"present\"\n",
            "2023-11-29 09:17:47,567 : INFO : topic #15 (0.010): 0.006*\"would\" + 0.004*\"god\" + 0.004*\"man\" + 0.004*\"men\" + 0.004*\"great\" + 0.004*\"earth\" + 0.003*\"life\" + 0.003*\"like\" + 0.003*\"upon\" + 0.003*\"death\"\n",
            "2023-11-29 09:17:47,574 : INFO : topic #60 (0.010): 0.005*\"man\" + 0.005*\"would\" + 0.004*\"great\" + 0.004*\"poetry\" + 0.004*\"well\" + 0.004*\"little\" + 0.004*\"life\" + 0.004*\"made\" + 0.004*\"upon\" + 0.004*\"much\"\n",
            "2023-11-29 09:17:47,607 : INFO : topic diff=0.421097, rho=0.141421\n",
            "2023-11-29 09:17:47,628 : INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #154000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:57,552 : INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #156000/163069, outstanding queue size 21\n",
            "2023-11-29 09:17:57,941 : INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #158000/163069, outstanding queue size 22\n",
            "2023-11-29 09:18:01,699 : INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #160000/163069, outstanding queue size 21\n",
            "2023-11-29 09:18:03,828 : INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #162000/163069, outstanding queue size 20\n",
            "2023-11-29 09:18:03,946 : INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #163069/163069, outstanding queue size 21\n",
            "2023-11-29 09:18:07,848 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:18:08,674 : INFO : topic #20 (0.010): 0.004*\"words\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"two\" + 0.003*\"see\" + 0.003*\"upon\" + 0.003*\"like\" + 0.002*\"reading\" + 0.002*\"would\" + 0.002*\"place\"\n",
            "2023-11-29 09:18:08,680 : INFO : topic #31 (0.010): 0.012*\"john\" + 0.009*\"english\" + 0.008*\"chapter\" + 0.008*\"william\" + 0.007*\"london\" + 0.006*\"poetry\" + 0.005*\"thomas\" + 0.005*\"new\" + 0.004*\"poems\" + 0.004*\"rev\"\n",
            "2023-11-29 09:18:08,685 : INFO : topic #93 (0.010): 0.005*\"see\" + 0.004*\"also\" + 0.004*\"like\" + 0.003*\"english\" + 0.003*\"old\" + 0.003*\"act\" + 0.003*\"first\" + 0.003*\"library\" + 0.003*\"part\" + 0.003*\"two\"\n",
            "2023-11-29 09:18:08,730 : INFO : topic #76 (0.010): 0.005*\"words\" + 0.004*\"word\" + 0.004*\"vol\" + 0.003*\"turn\" + 0.003*\"first\" + 0.003*\"loved\" + 0.002*\"part\" + 0.002*\"great\" + 0.002*\"life\" + 0.002*\"english\"\n",
            "2023-11-29 09:18:08,741 : INFO : topic #57 (0.010): 0.009*\"der\" + 0.009*\"english\" + 0.008*\"des\" + 0.007*\"von\" + 0.006*\"und\" + 0.006*\"french\" + 0.004*\"sound\" + 0.004*\"language\" + 0.003*\"poetry\" + 0.003*\"die\"\n",
            "2023-11-29 09:18:08,768 : INFO : topic diff=0.411697, rho=0.132453\n",
            "2023-11-29 09:18:23,131 : INFO : -10.025 per-word bound, 1041.9 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:18:30,333 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:18:31,172 : INFO : topic #97 (0.010): 0.009*\"would\" + 0.006*\"first\" + 0.005*\"tense\" + 0.005*\"letters\" + 0.004*\"time\" + 0.004*\"words\" + 0.004*\"shall\" + 0.004*\"present\" + 0.004*\"used\" + 0.004*\"might\"\n",
            "2023-11-29 09:18:31,178 : INFO : topic #86 (0.010): 0.014*\"non\" + 0.010*\"est\" + 0.006*\"qui\" + 0.005*\"dhe\" + 0.005*\"cum\" + 0.005*\"aut\" + 0.005*\"sed\" + 0.004*\"quod\" + 0.004*\"quam\" + 0.003*\"nec\"\n",
            "2023-11-29 09:18:31,182 : INFO : topic #27 (0.010): 0.008*\"words\" + 0.008*\"accent\" + 0.006*\"new\" + 0.005*\"word\" + 0.005*\"syllables\" + 0.005*\"first\" + 0.005*\"two\" + 0.005*\"verses\" + 0.004*\"see\" + 0.003*\"verse\"\n",
            "2023-11-29 09:18:31,186 : INFO : topic #8 (0.010): 0.004*\"thy\" + 0.003*\"like\" + 0.003*\"would\" + 0.003*\"great\" + 0.003*\"hand\" + 0.003*\"could\" + 0.003*\"right\" + 0.003*\"water\" + 0.003*\"eyes\" + 0.003*\"every\"\n",
            "2023-11-29 09:18:31,191 : INFO : topic #25 (0.010): 0.004*\"time\" + 0.004*\"two\" + 0.004*\"upon\" + 0.004*\"would\" + 0.003*\"first\" + 0.003*\"much\" + 0.003*\"old\" + 0.003*\"also\" + 0.002*\"man\" + 0.002*\"words\"\n",
            "2023-11-29 09:18:31,219 : INFO : topic diff=0.407249, rho=0.125000\n",
            "2023-11-29 09:18:44,560 : INFO : -9.988 per-word bound, 1015.6 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:18:50,440 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:18:51,263 : INFO : topic #93 (0.010): 0.005*\"see\" + 0.005*\"also\" + 0.004*\"like\" + 0.004*\"act\" + 0.003*\"old\" + 0.003*\"english\" + 0.003*\"library\" + 0.003*\"two\" + 0.003*\"first\" + 0.003*\"part\"\n",
            "2023-11-29 09:18:51,269 : INFO : topic #29 (0.010): 0.009*\"man\" + 0.005*\"good\" + 0.004*\"without\" + 0.004*\"upon\" + 0.004*\"shall\" + 0.004*\"mind\" + 0.004*\"men\" + 0.004*\"said\" + 0.004*\"taste\" + 0.004*\"life\"\n",
            "2023-11-29 09:18:51,273 : INFO : topic #94 (0.010): 0.004*\"king\" + 0.003*\"part\" + 0.003*\"upon\" + 0.003*\"language\" + 0.002*\"new\" + 0.002*\"like\" + 0.002*\"long\" + 0.002*\"kings\" + 0.002*\"old\" + 0.002*\"time\"\n",
            "2023-11-29 09:18:51,279 : INFO : topic #9 (0.010): 0.012*\"grammar\" + 0.009*\"lesson\" + 0.006*\"words\" + 0.005*\"first\" + 0.005*\"time\" + 0.005*\"english\" + 0.004*\"language\" + 0.004*\"would\" + 0.004*\"word\" + 0.003*\"use\"\n",
            "2023-11-29 09:18:51,284 : INFO : topic #62 (0.010): 0.010*\"language\" + 0.009*\"english\" + 0.007*\"grammar\" + 0.007*\"study\" + 0.005*\"work\" + 0.004*\"first\" + 0.004*\"style\" + 0.004*\"subject\" + 0.004*\"words\" + 0.004*\"use\"\n",
            "2023-11-29 09:18:51,311 : INFO : topic diff=0.402010, rho=0.118678\n",
            "2023-11-29 09:18:59,659 : INFO : -9.952 per-word bound, 990.7 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:19:02,633 : INFO : merging changes from 9069 documents into a model of 163069 documents\n",
            "2023-11-29 09:19:03,180 : INFO : topic #16 (0.010): 0.004*\"upon\" + 0.003*\"love\" + 0.003*\"þæt\" + 0.003*\"well\" + 0.002*\"man\" + 0.002*\"like\" + 0.002*\"last\" + 0.002*\"old\" + 0.002*\"life\" + 0.002*\"hit\"\n",
            "2023-11-29 09:19:03,184 : INFO : topic #10 (0.010): 0.006*\"poetry\" + 0.006*\"language\" + 0.004*\"like\" + 0.004*\"mind\" + 0.004*\"time\" + 0.003*\"words\" + 0.003*\"upon\" + 0.003*\"english\" + 0.003*\"thought\" + 0.003*\"first\"\n",
            "2023-11-29 09:19:03,186 : INFO : topic #40 (0.010): 0.009*\"upon\" + 0.005*\"great\" + 0.005*\"much\" + 0.004*\"yet\" + 0.003*\"like\" + 0.003*\"without\" + 0.003*\"shall\" + 0.003*\"many\" + 0.003*\"sublime\" + 0.003*\"nature\"\n",
            "2023-11-29 09:19:03,189 : INFO : topic #71 (0.010): 0.005*\"poetry\" + 0.004*\"poem\" + 0.003*\"every\" + 0.003*\"great\" + 0.003*\"much\" + 0.003*\"nature\" + 0.003*\"time\" + 0.003*\"upon\" + 0.003*\"professor\" + 0.002*\"well\"\n",
            "2023-11-29 09:19:03,193 : INFO : topic #2 (0.010): 0.022*\"que\" + 0.017*\"les\" + 0.012*\"qui\" + 0.008*\"des\" + 0.008*\"dans\" + 0.007*\"une\" + 0.007*\"est\" + 0.005*\"plus\" + 0.005*\"pour\" + 0.005*\"par\"\n",
            "2023-11-29 09:19:03,213 : INFO : topic diff=0.402743, rho=0.113228\n",
            "2023-11-29 09:19:06,399 : INFO : -9.821 per-word bound, 904.7 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:19:06,578 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #2000/163069, outstanding queue size 1\n",
            "2023-11-29 09:19:06,829 : INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #4000/163069, outstanding queue size 2\n",
            "2023-11-29 09:19:06,976 : INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #6000/163069, outstanding queue size 3\n",
            "2023-11-29 09:19:07,245 : INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #8000/163069, outstanding queue size 4\n",
            "2023-11-29 09:19:07,391 : INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #10000/163069, outstanding queue size 5\n",
            "2023-11-29 09:19:07,536 : INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #12000/163069, outstanding queue size 6\n",
            "2023-11-29 09:19:07,839 : INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #14000/163069, outstanding queue size 7\n",
            "2023-11-29 09:19:08,023 : INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #16000/163069, outstanding queue size 8\n",
            "2023-11-29 09:19:08,338 : INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #18000/163069, outstanding queue size 9\n",
            "2023-11-29 09:19:08,546 : INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #20000/163069, outstanding queue size 10\n",
            "2023-11-29 09:19:08,924 : INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #22000/163069, outstanding queue size 11\n",
            "2023-11-29 09:19:09,153 : INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #24000/163069, outstanding queue size 12\n",
            "2023-11-29 09:19:09,548 : INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #26000/163069, outstanding queue size 13\n",
            "2023-11-29 09:19:09,784 : INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #28000/163069, outstanding queue size 14\n",
            "2023-11-29 09:19:10,033 : INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #30000/163069, outstanding queue size 15\n",
            "2023-11-29 09:19:10,446 : INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #32000/163069, outstanding queue size 16\n",
            "2023-11-29 09:19:10,693 : INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #34000/163069, outstanding queue size 17\n",
            "2023-11-29 09:19:10,957 : INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #36000/163069, outstanding queue size 18\n",
            "2023-11-29 09:19:11,533 : INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #38000/163069, outstanding queue size 19\n",
            "2023-11-29 09:19:11,813 : INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #40000/163069, outstanding queue size 20\n",
            "2023-11-29 09:19:12,059 : INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #42000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:22,575 : INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #44000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:23,909 : INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #46000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:24,263 : INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #48000/163069, outstanding queue size 22\n",
            "2023-11-29 09:19:26,925 : INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #50000/163069, outstanding queue size 20\n",
            "2023-11-29 09:19:27,295 : INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #52000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:27,536 : INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #54000/163069, outstanding queue size 22\n",
            "2023-11-29 09:19:28,744 : INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #56000/163069, outstanding queue size 22\n",
            "2023-11-29 09:19:30,497 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:19:31,333 : INFO : topic #20 (0.010): 0.004*\"words\" + 0.004*\"time\" + 0.003*\"many\" + 0.003*\"see\" + 0.003*\"ballade\" + 0.003*\"two\" + 0.003*\"upon\" + 0.003*\"like\" + 0.003*\"reading\" + 0.002*\"would\"\n",
            "2023-11-29 09:19:31,337 : INFO : topic #58 (0.010): 0.004*\"upon\" + 0.004*\"would\" + 0.003*\"time\" + 0.003*\"must\" + 0.003*\"voice\" + 0.003*\"eyes\" + 0.002*\"whole\" + 0.002*\"thus\" + 0.002*\"tone\" + 0.002*\"body\"\n",
            "2023-11-29 09:19:31,342 : INFO : topic #98 (0.010): 0.004*\"two\" + 0.004*\"first\" + 0.004*\"would\" + 0.003*\"time\" + 0.003*\"great\" + 0.003*\"verse\" + 0.003*\"deriv\" + 0.003*\"upon\" + 0.003*\"called\" + 0.003*\"part\"\n",
            "2023-11-29 09:19:31,346 : INFO : topic #70 (0.010): 0.010*\"book\" + 0.010*\"work\" + 0.007*\"grammar\" + 0.006*\"study\" + 0.006*\"school\" + 0.006*\"reading\" + 0.006*\"schools\" + 0.006*\"language\" + 0.005*\"english\" + 0.005*\"use\"\n",
            "2023-11-29 09:19:31,351 : INFO : topic #72 (0.010): 0.006*\"words\" + 0.004*\"pro\" + 0.004*\"con\" + 0.003*\"together\" + 0.003*\"sometimes\" + 0.003*\"letter\" + 0.003*\"dis\" + 0.003*\"part\" + 0.003*\"like\" + 0.003*\"two\"\n",
            "2023-11-29 09:19:31,377 : INFO : topic diff=0.400864, rho=0.109413\n",
            "2023-11-29 09:19:41,124 : INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #58000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:42,637 : INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #60000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:42,993 : INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #62000/163069, outstanding queue size 22\n",
            "2023-11-29 09:19:43,399 : INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #64000/163069, outstanding queue size 23\n",
            "2023-11-29 09:19:45,449 : INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #66000/163069, outstanding queue size 22\n",
            "2023-11-29 09:19:45,845 : INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #68000/163069, outstanding queue size 23\n",
            "2023-11-29 09:19:48,828 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:19:49,657 : INFO : topic #87 (0.010): 0.006*\"new\" + 0.004*\"first\" + 0.004*\"work\" + 0.004*\"war\" + 0.003*\"speech\" + 0.003*\"year\" + 0.003*\"states\" + 0.003*\"two\" + 0.003*\"words\" + 0.003*\"great\"\n",
            "2023-11-29 09:19:49,664 : INFO : topic #23 (0.010): 0.004*\"english\" + 0.003*\"words\" + 0.003*\"found\" + 0.003*\"long\" + 0.003*\"also\" + 0.003*\"new\" + 0.003*\"upon\" + 0.003*\"time\" + 0.002*\"men\" + 0.002*\"poetry\"\n",
            "2023-11-29 09:19:49,669 : INFO : topic #72 (0.010): 0.006*\"words\" + 0.004*\"pro\" + 0.004*\"con\" + 0.003*\"together\" + 0.003*\"sometimes\" + 0.003*\"letter\" + 0.003*\"dis\" + 0.003*\"part\" + 0.003*\"like\" + 0.003*\"two\"\n",
            "2023-11-29 09:19:49,673 : INFO : topic #80 (0.010): 0.011*\"ideas\" + 0.009*\"language\" + 0.008*\"man\" + 0.007*\"thought\" + 0.007*\"idea\" + 0.007*\"mind\" + 0.006*\"words\" + 0.005*\"express\" + 0.005*\"without\" + 0.004*\"things\"\n",
            "2023-11-29 09:19:49,680 : INFO : topic #48 (0.010): 0.023*\"plural\" + 0.012*\"sound\" + 0.010*\"singular\" + 0.008*\"ending\" + 0.007*\"like\" + 0.006*\"words\" + 0.006*\"vowel\" + 0.005*\"plurals\" + 0.005*\"nouns\" + 0.005*\"form\"\n",
            "2023-11-29 09:19:49,707 : INFO : topic diff=0.409709, rho=0.109413\n",
            "2023-11-29 09:19:50,010 : INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #70000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:59,540 : INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #72000/163069, outstanding queue size 21\n",
            "2023-11-29 09:19:59,945 : INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #74000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:01,754 : INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #76000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:02,075 : INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #78000/163069, outstanding queue size 23\n",
            "2023-11-29 09:20:06,881 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:20:07,716 : INFO : topic #34 (0.010): 0.005*\"day\" + 0.004*\"new\" + 0.004*\"would\" + 0.004*\"upon\" + 0.004*\"art\" + 0.004*\"many\" + 0.003*\"month\" + 0.003*\"must\" + 0.003*\"time\" + 0.003*\"great\"\n",
            "2023-11-29 09:20:07,723 : INFO : topic #27 (0.010): 0.008*\"words\" + 0.008*\"accent\" + 0.006*\"new\" + 0.005*\"verses\" + 0.005*\"word\" + 0.005*\"syllables\" + 0.005*\"two\" + 0.005*\"first\" + 0.004*\"see\" + 0.004*\"verse\"\n",
            "2023-11-29 09:20:07,727 : INFO : topic #31 (0.010): 0.013*\"john\" + 0.009*\"william\" + 0.009*\"english\" + 0.007*\"london\" + 0.007*\"chapter\" + 0.006*\"thomas\" + 0.006*\"sir\" + 0.005*\"poetry\" + 0.005*\"esq\" + 0.005*\"rev\"\n",
            "2023-11-29 09:20:07,731 : INFO : topic #54 (0.010): 0.013*\"english\" + 0.009*\"language\" + 0.009*\"century\" + 0.006*\"literature\" + 0.005*\"england\" + 0.005*\"latin\" + 0.005*\"wrote\" + 0.005*\"first\" + 0.004*\"time\" + 0.004*\"written\"\n",
            "2023-11-29 09:20:07,735 : INFO : topic #99 (0.010): 0.007*\"would\" + 0.005*\"much\" + 0.005*\"must\" + 0.005*\"great\" + 0.004*\"many\" + 0.004*\"even\" + 0.004*\"upon\" + 0.004*\"time\" + 0.003*\"new\" + 0.003*\"man\"\n",
            "2023-11-29 09:20:07,762 : INFO : topic diff=0.422192, rho=0.109413\n",
            "2023-11-29 09:20:08,029 : INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #80000/163069, outstanding queue size 19\n",
            "2023-11-29 09:20:08,256 : INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #82000/163069, outstanding queue size 20\n",
            "2023-11-29 09:20:08,486 : INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #84000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:17,283 : INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #86000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:17,979 : INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #88000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:19,964 : INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #90000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:20,184 : INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #92000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:21,266 : INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #94000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:24,588 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:20:25,422 : INFO : topic #45 (0.010): 0.027*\"read\" + 0.008*\"net\" + 0.006*\"see\" + 0.005*\"crown\" + 0.004*\"table\" + 0.004*\"note\" + 0.003*\"tragedy\" + 0.003*\"lin\" + 0.003*\"first\" + 0.002*\"veritas\"\n",
            "2023-11-29 09:20:25,426 : INFO : topic #14 (0.010): 0.010*\"tion\" + 0.006*\"ing\" + 0.004*\"ble\" + 0.004*\"ver\" + 0.003*\"every\" + 0.003*\"time\" + 0.003*\"upon\" + 0.003*\"part\" + 0.003*\"make\" + 0.003*\"book\"\n",
            "2023-11-29 09:20:25,430 : INFO : topic #30 (0.010): 0.009*\"music\" + 0.008*\"poetry\" + 0.005*\"would\" + 0.005*\"words\" + 0.004*\"art\" + 0.004*\"reading\" + 0.004*\"read\" + 0.004*\"must\" + 0.004*\"voice\" + 0.004*\"many\"\n",
            "2023-11-29 09:20:25,435 : INFO : topic #88 (0.010): 0.007*\"signifies\" + 0.004*\"words\" + 0.004*\"used\" + 0.004*\"thing\" + 0.004*\"word\" + 0.003*\"night\" + 0.003*\"god\" + 0.003*\"two\" + 0.003*\"good\" + 0.003*\"like\"\n",
            "2023-11-29 09:20:25,439 : INFO : topic #22 (0.010): 0.011*\"obl\" + 0.006*\"verse\" + 0.005*\"inf\" + 0.005*\"obj\" + 0.003*\"would\" + 0.003*\"dst\" + 0.003*\"iron\" + 0.003*\"true\" + 0.003*\"time\" + 0.003*\"word\"\n",
            "2023-11-29 09:20:25,466 : INFO : topic diff=0.435568, rho=0.109413\n",
            "2023-11-29 09:20:25,485 : INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #96000/163069, outstanding queue size 20\n",
            "2023-11-29 09:20:25,740 : INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #98000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:36,379 : INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #100000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:38,499 : INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #102000/163069, outstanding queue size 20\n",
            "2023-11-29 09:20:38,918 : INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #104000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:39,146 : INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #106000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:43,354 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:20:44,169 : INFO : topic #86 (0.010): 0.020*\"non\" + 0.016*\"est\" + 0.008*\"qui\" + 0.008*\"cum\" + 0.007*\"quod\" + 0.006*\"aut\" + 0.006*\"sed\" + 0.005*\"quam\" + 0.005*\"per\" + 0.005*\"sunt\"\n",
            "2023-11-29 09:20:44,173 : INFO : topic #5 (0.010): 0.021*\"noun\" + 0.020*\"nouns\" + 0.019*\"name\" + 0.014*\"names\" + 0.013*\"case\" + 0.010*\"common\" + 0.010*\"word\" + 0.010*\"proper\" + 0.009*\"words\" + 0.008*\"number\"\n",
            "2023-11-29 09:20:44,179 : INFO : topic #53 (0.010): 0.004*\"well\" + 0.004*\"would\" + 0.004*\"great\" + 0.004*\"upon\" + 0.004*\"time\" + 0.003*\"every\" + 0.003*\"first\" + 0.003*\"yet\" + 0.003*\"words\" + 0.003*\"much\"\n",
            "2023-11-29 09:20:44,184 : INFO : topic #88 (0.010): 0.007*\"signifies\" + 0.004*\"thing\" + 0.004*\"words\" + 0.004*\"used\" + 0.003*\"god\" + 0.003*\"word\" + 0.003*\"night\" + 0.003*\"good\" + 0.003*\"two\" + 0.003*\"like\"\n",
            "2023-11-29 09:20:44,190 : INFO : topic #94 (0.010): 0.005*\"king\" + 0.003*\"part\" + 0.003*\"kings\" + 0.003*\"coast\" + 0.003*\"upon\" + 0.002*\"britain\" + 0.002*\"son\" + 0.002*\"called\" + 0.002*\"new\" + 0.002*\"long\"\n",
            "2023-11-29 09:20:44,211 : INFO : topic diff=0.448733, rho=0.109413\n",
            "2023-11-29 09:20:44,484 : INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #108000/163069, outstanding queue size 19\n",
            "2023-11-29 09:20:44,703 : INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #110000/163069, outstanding queue size 20\n",
            "2023-11-29 09:20:44,941 : INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #112000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:52,899 : INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #114000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:54,994 : INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #116000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:55,818 : INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #118000/163069, outstanding queue size 21\n",
            "2023-11-29 09:20:56,181 : INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #120000/163069, outstanding queue size 22\n",
            "2023-11-29 09:20:56,845 : INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #122000/163069, outstanding queue size 23\n",
            "2023-11-29 09:20:57,860 : INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #124000/163069, outstanding queue size 23\n",
            "2023-11-29 09:21:00,833 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:21:01,682 : INFO : topic #81 (0.010): 0.016*\"long\" + 0.014*\"short\" + 0.007*\"english\" + 0.005*\"alphabetical\" + 0.004*\"see\" + 0.004*\"words\" + 0.003*\"written\" + 0.003*\"like\" + 0.003*\"first\" + 0.003*\"word\"\n",
            "2023-11-29 09:21:01,687 : INFO : topic #97 (0.010): 0.010*\"would\" + 0.009*\"tense\" + 0.006*\"first\" + 0.006*\"shall\" + 0.006*\"time\" + 0.005*\"present\" + 0.005*\"learn\" + 0.005*\"letters\" + 0.005*\"learned\" + 0.005*\"might\"\n",
            "2023-11-29 09:21:01,693 : INFO : topic #58 (0.010): 0.005*\"upon\" + 0.004*\"eyes\" + 0.004*\"would\" + 0.003*\"voice\" + 0.003*\"time\" + 0.003*\"tone\" + 0.003*\"body\" + 0.003*\"must\" + 0.003*\"whole\" + 0.002*\"hand\"\n",
            "2023-11-29 09:21:01,697 : INFO : topic #40 (0.010): 0.010*\"upon\" + 0.006*\"great\" + 0.005*\"yet\" + 0.005*\"much\" + 0.003*\"like\" + 0.003*\"without\" + 0.003*\"many\" + 0.003*\"'tis\" + 0.003*\"shall\" + 0.003*\"sublime\"\n",
            "2023-11-29 09:21:01,701 : INFO : topic #86 (0.010): 0.021*\"non\" + 0.018*\"est\" + 0.009*\"qui\" + 0.008*\"cum\" + 0.007*\"quod\" + 0.006*\"aut\" + 0.006*\"sed\" + 0.005*\"quam\" + 0.005*\"sunt\" + 0.005*\"per\"\n",
            "2023-11-29 09:21:01,727 : INFO : topic diff=0.461362, rho=0.109413\n",
            "2023-11-29 09:21:02,005 : INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #126000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:09,714 : INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #128000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:11,259 : INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #130000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:13,010 : INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #132000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:13,419 : INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #134000/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:15,153 : INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #136000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:15,390 : INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #138000/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:17,477 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:21:18,377 : INFO : topic #93 (0.010): 0.006*\"also\" + 0.005*\"see\" + 0.004*\"act\" + 0.004*\"library\" + 0.004*\"like\" + 0.003*\"tho\" + 0.003*\"haue\" + 0.003*\"old\" + 0.003*\"university\" + 0.003*\"district\"\n",
            "2023-11-29 09:21:18,383 : INFO : topic #41 (0.010): 0.016*\"rhythm\" + 0.008*\"two\" + 0.008*\"time\" + 0.007*\"verse\" + 0.007*\"foot\" + 0.006*\"long\" + 0.006*\"feet\" + 0.006*\"latin\" + 0.006*\"greek\" + 0.006*\"first\"\n",
            "2023-11-29 09:21:18,387 : INFO : topic #51 (0.010): 0.015*\"court\" + 0.009*\"sir\" + 0.009*\"street\" + 0.006*\"king\" + 0.005*\"john\" + 0.005*\"earl\" + 0.005*\"richard\" + 0.005*\"see\" + 0.004*\"henry\" + 0.004*\"duke\"\n",
            "2023-11-29 09:21:18,392 : INFO : topic #62 (0.010): 0.011*\"language\" + 0.011*\"english\" + 0.008*\"study\" + 0.007*\"grammar\" + 0.006*\"work\" + 0.005*\"use\" + 0.005*\"composition\" + 0.005*\"subject\" + 0.005*\"writing\" + 0.005*\"style\"\n",
            "2023-11-29 09:21:18,396 : INFO : topic #6 (0.010): 0.005*\"words\" + 0.005*\"power\" + 0.005*\"xxxvii\" + 0.004*\"xxxv\" + 0.004*\"many\" + 0.004*\"sound\" + 0.004*\"xxxiv\" + 0.004*\"xxxvi\" + 0.004*\"xxxiii\" + 0.004*\"xxxii\"\n",
            "2023-11-29 09:21:18,424 : INFO : topic diff=0.473772, rho=0.109413\n",
            "2023-11-29 09:21:18,673 : INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #140000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:28,148 : INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #142000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:28,526 : INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #144000/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:31,094 : INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #146000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:31,463 : INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #148000/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:31,739 : INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #150000/163069, outstanding queue size 23\n",
            "2023-11-29 09:21:34,985 : INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #152000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:35,219 : INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #154000/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:37,152 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:21:37,998 : INFO : topic #35 (0.010): 0.027*\"verse\" + 0.008*\"verses\" + 0.007*\"blank\" + 0.007*\"syllables\" + 0.005*\"english\" + 0.005*\"feet\" + 0.005*\"poetry\" + 0.005*\"two\" + 0.004*\"ear\" + 0.004*\"lines\"\n",
            "2023-11-29 09:21:38,002 : INFO : topic #42 (0.010): 0.004*\"old\" + 0.004*\"time\" + 0.004*\"man\" + 0.004*\"men\" + 0.004*\"people\" + 0.003*\"great\" + 0.003*\"upon\" + 0.003*\"country\" + 0.003*\"years\" + 0.003*\"would\"\n",
            "2023-11-29 09:21:38,008 : INFO : topic #73 (0.010): 0.005*\"first\" + 0.005*\"part\" + 0.004*\"two\" + 0.003*\"english\" + 0.003*\"good\" + 0.003*\"time\" + 0.003*\"many\" + 0.003*\"made\" + 0.003*\"every\" + 0.003*\"use\"\n",
            "2023-11-29 09:21:38,013 : INFO : topic #98 (0.010): 0.004*\"two\" + 0.003*\"first\" + 0.003*\"would\" + 0.003*\"time\" + 0.003*\"upon\" + 0.003*\"finger\" + 0.003*\"great\" + 0.003*\"called\" + 0.002*\"libre\" + 0.002*\"left\"\n",
            "2023-11-29 09:21:38,017 : INFO : topic #55 (0.010): 0.025*\"sentence\" + 0.020*\"words\" + 0.018*\"sentences\" + 0.009*\"word\" + 0.008*\"chapter\" + 0.008*\"used\" + 0.007*\"use\" + 0.007*\"subject\" + 0.007*\"exercises\" + 0.007*\"parts\"\n",
            "2023-11-29 09:21:38,044 : INFO : topic diff=0.484726, rho=0.109413\n",
            "2023-11-29 09:21:46,374 : INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #156000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:47,639 : INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #158000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:48,948 : INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #160000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:50,355 : INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #162000/163069, outstanding queue size 21\n",
            "2023-11-29 09:21:50,474 : INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #163069/163069, outstanding queue size 22\n",
            "2023-11-29 09:21:54,984 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:21:55,831 : INFO : topic #13 (0.010): 0.010*\"dryden\" + 0.010*\"poets\" + 0.009*\"milton\" + 0.009*\"spenser\" + 0.008*\"poetry\" + 0.008*\"pope\" + 0.007*\"poems\" + 0.006*\"poet\" + 0.005*\"style\" + 0.005*\"verse\"\n",
            "2023-11-29 09:21:55,837 : INFO : topic #26 (0.010): 0.005*\"first\" + 0.003*\"english\" + 0.003*\"see\" + 0.003*\"like\" + 0.003*\"well\" + 0.003*\"work\" + 0.003*\"two\" + 0.003*\"would\" + 0.003*\"many\" + 0.002*\"great\"\n",
            "2023-11-29 09:21:55,841 : INFO : topic #92 (0.010): 0.028*\"verb\" + 0.018*\"person\" + 0.017*\"verbs\" + 0.012*\"pronouns\" + 0.012*\"gender\" + 0.011*\"nouns\" + 0.010*\"noun\" + 0.009*\"pronoun\" + 0.009*\"plural\" + 0.008*\"number\"\n",
            "2023-11-29 09:21:55,847 : INFO : topic #82 (0.010): 0.022*\"der\" + 0.021*\"und\" + 0.016*\"die\" + 0.011*\"als\" + 0.011*\"nicht\" + 0.009*\"ist\" + 0.009*\"dass\" + 0.008*\"hat\" + 0.006*\"wie\" + 0.006*\"eine\"\n",
            "2023-11-29 09:21:55,852 : INFO : topic #42 (0.010): 0.004*\"time\" + 0.004*\"old\" + 0.004*\"man\" + 0.004*\"men\" + 0.004*\"people\" + 0.003*\"years\" + 0.003*\"upon\" + 0.003*\"great\" + 0.003*\"country\" + 0.003*\"would\"\n",
            "2023-11-29 09:21:55,879 : INFO : topic diff=0.495035, rho=0.109413\n",
            "2023-11-29 09:22:08,415 : INFO : -9.677 per-word bound, 818.6 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:22:15,376 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:22:16,237 : INFO : topic #39 (0.010): 0.014*\"latin\" + 0.011*\"french\" + 0.009*\"greek\" + 0.009*\"english\" + 0.008*\"poetry\" + 0.007*\"history\" + 0.006*\"roman\" + 0.006*\"eloquence\" + 0.005*\"ancient\" + 0.004*\"first\"\n",
            "2023-11-29 09:22:16,242 : INFO : topic #23 (0.010): 0.003*\"english\" + 0.003*\"men\" + 0.003*\"found\" + 0.003*\"abba\" + 0.003*\"also\" + 0.003*\"baldwin\" + 0.003*\"ane\" + 0.003*\"upon\" + 0.003*\"words\" + 0.002*\"man\"\n",
            "2023-11-29 09:22:16,246 : INFO : topic #34 (0.010): 0.006*\"day\" + 0.005*\"month\" + 0.004*\"new\" + 0.004*\"upon\" + 0.004*\"would\" + 0.004*\"many\" + 0.004*\"art\" + 0.003*\"time\" + 0.003*\"must\" + 0.003*\"pass\"\n",
            "2023-11-29 09:22:16,251 : INFO : topic #43 (0.010): 0.005*\"rime\" + 0.005*\"time\" + 0.004*\"great\" + 0.004*\"first\" + 0.004*\"minstrels\" + 0.004*\"written\" + 0.003*\"year\" + 0.003*\"songs\" + 0.003*\"king\" + 0.003*\"poetry\"\n",
            "2023-11-29 09:22:16,254 : INFO : topic #20 (0.010): 0.004*\"ballade\" + 0.003*\"horn\" + 0.003*\"words\" + 0.003*\"see\" + 0.003*\"time\" + 0.003*\"two\" + 0.003*\"many\" + 0.002*\"upon\" + 0.002*\"like\" + 0.002*\"reading\"\n",
            "2023-11-29 09:22:16,283 : INFO : topic diff=0.505305, rho=0.109413\n",
            "2023-11-29 09:22:28,963 : INFO : -9.673 per-word bound, 816.3 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:22:35,198 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:22:36,048 : INFO : topic #26 (0.010): 0.005*\"first\" + 0.003*\"english\" + 0.003*\"see\" + 0.003*\"like\" + 0.003*\"well\" + 0.003*\"work\" + 0.003*\"two\" + 0.003*\"would\" + 0.003*\"many\" + 0.002*\"years\"\n",
            "2023-11-29 09:22:36,055 : INFO : topic #87 (0.010): 0.007*\"new\" + 0.005*\"year\" + 0.004*\"states\" + 0.004*\"war\" + 0.004*\"first\" + 0.004*\"two\" + 0.003*\"speech\" + 0.003*\"united\" + 0.003*\"government\" + 0.003*\"school\"\n",
            "2023-11-29 09:22:36,059 : INFO : topic #2 (0.010): 0.028*\"les\" + 0.028*\"que\" + 0.016*\"qui\" + 0.015*\"des\" + 0.013*\"dans\" + 0.011*\"est\" + 0.010*\"une\" + 0.009*\"par\" + 0.009*\"plus\" + 0.008*\"pour\"\n",
            "2023-11-29 09:22:36,066 : INFO : topic #98 (0.010): 0.004*\"first\" + 0.004*\"two\" + 0.003*\"would\" + 0.003*\"finger\" + 0.003*\"time\" + 0.003*\"harp\" + 0.003*\"libre\" + 0.003*\"upon\" + 0.002*\"great\" + 0.002*\"called\"\n",
            "2023-11-29 09:22:36,076 : INFO : topic #17 (0.010): 0.015*\"lat\" + 0.013*\"adj\" + 0.006*\"inst\" + 0.006*\"town\" + 0.005*\"name\" + 0.004*\"miles\" + 0.004*\"part\" + 0.004*\"north\" + 0.003*\"sub\" + 0.003*\"lib\"\n",
            "2023-11-29 09:22:36,102 : INFO : topic diff=0.516366, rho=0.109413\n",
            "2023-11-29 09:22:42,796 : INFO : -9.661 per-word bound, 809.8 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:22:45,705 : INFO : merging changes from 9069 documents into a model of 163069 documents\n",
            "2023-11-29 09:22:46,271 : INFO : topic #15 (0.010): 0.008*\"god\" + 0.006*\"earth\" + 0.005*\"life\" + 0.005*\"man\" + 0.005*\"death\" + 0.005*\"heaven\" + 0.005*\"would\" + 0.004*\"men\" + 0.004*\"world\" + 0.004*\"soul\"\n",
            "2023-11-29 09:22:46,274 : INFO : topic #98 (0.010): 0.007*\"deriv\" + 0.004*\"finger\" + 0.004*\"two\" + 0.003*\"first\" + 0.003*\"would\" + 0.003*\"tapping\" + 0.003*\"libre\" + 0.003*\"harp\" + 0.003*\"upon\" + 0.003*\"time\"\n",
            "2023-11-29 09:22:46,277 : INFO : topic #42 (0.010): 0.004*\"time\" + 0.004*\"old\" + 0.004*\"man\" + 0.004*\"years\" + 0.004*\"men\" + 0.004*\"people\" + 0.004*\"great\" + 0.004*\"country\" + 0.004*\"upon\" + 0.003*\"son\"\n",
            "2023-11-29 09:22:46,279 : INFO : topic #31 (0.010): 0.015*\"john\" + 0.010*\"william\" + 0.009*\"english\" + 0.008*\"london\" + 0.007*\"thomas\" + 0.007*\"rev\" + 0.007*\"sir\" + 0.006*\"esq\" + 0.006*\"george\" + 0.006*\"james\"\n",
            "2023-11-29 09:22:46,283 : INFO : topic #9 (0.010): 0.016*\"lesson\" + 0.015*\"grammar\" + 0.006*\"first\" + 0.006*\"time\" + 0.006*\"master\" + 0.005*\"words\" + 0.005*\"class\" + 0.004*\"part\" + 0.004*\"english\" + 0.003*\"book\"\n",
            "2023-11-29 09:22:46,304 : INFO : topic diff=0.531752, rho=0.109413\n",
            "2023-11-29 09:22:49,405 : INFO : -9.578 per-word bound, 764.2 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:22:49,578 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #2000/163069, outstanding queue size 1\n",
            "2023-11-29 09:22:49,822 : INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #4000/163069, outstanding queue size 2\n",
            "2023-11-29 09:22:50,051 : INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #6000/163069, outstanding queue size 3\n",
            "2023-11-29 09:22:50,302 : INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #8000/163069, outstanding queue size 4\n",
            "2023-11-29 09:22:50,553 : INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #10000/163069, outstanding queue size 5\n",
            "2023-11-29 09:22:50,706 : INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #12000/163069, outstanding queue size 6\n",
            "2023-11-29 09:22:51,011 : INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #14000/163069, outstanding queue size 7\n",
            "2023-11-29 09:22:51,234 : INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #16000/163069, outstanding queue size 8\n",
            "2023-11-29 09:22:51,619 : INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #18000/163069, outstanding queue size 9\n",
            "2023-11-29 09:22:51,833 : INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #20000/163069, outstanding queue size 10\n",
            "2023-11-29 09:22:52,045 : INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #22000/163069, outstanding queue size 11\n",
            "2023-11-29 09:22:52,423 : INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #24000/163069, outstanding queue size 12\n",
            "2023-11-29 09:22:52,813 : INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #26000/163069, outstanding queue size 13\n",
            "2023-11-29 09:22:53,053 : INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #28000/163069, outstanding queue size 14\n",
            "2023-11-29 09:22:53,456 : INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #30000/163069, outstanding queue size 15\n",
            "2023-11-29 09:22:53,688 : INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #32000/163069, outstanding queue size 16\n",
            "2023-11-29 09:22:53,921 : INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #34000/163069, outstanding queue size 17\n",
            "2023-11-29 09:22:54,178 : INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #36000/163069, outstanding queue size 18\n",
            "2023-11-29 09:22:54,387 : INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #38000/163069, outstanding queue size 19\n",
            "2023-11-29 09:22:54,640 : INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #40000/163069, outstanding queue size 20\n",
            "2023-11-29 09:22:54,892 : INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #42000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:05,375 : INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #44000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:06,035 : INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #46000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:07,534 : INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #48000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:11,050 : INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #50000/163069, outstanding queue size 19\n",
            "2023-11-29 09:23:12,668 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:23:13,524 : INFO : topic #20 (0.010): 0.006*\"ballade\" + 0.003*\"see\" + 0.003*\"words\" + 0.003*\"horn\" + 0.003*\"time\" + 0.003*\"reading\" + 0.002*\"two\" + 0.002*\"many\" + 0.002*\"judgments\" + 0.002*\"upon\"\n",
            "2023-11-29 09:23:13,530 : INFO : topic #34 (0.010): 0.006*\"day\" + 0.005*\"month\" + 0.004*\"new\" + 0.004*\"upon\" + 0.004*\"would\" + 0.004*\"many\" + 0.004*\"art\" + 0.003*\"time\" + 0.003*\"must\" + 0.003*\"man\"\n",
            "2023-11-29 09:23:13,534 : INFO : topic #35 (0.010): 0.034*\"verse\" + 0.010*\"verses\" + 0.008*\"blank\" + 0.007*\"syllables\" + 0.006*\"feet\" + 0.006*\"ear\" + 0.005*\"english\" + 0.005*\"lines\" + 0.005*\"two\" + 0.005*\"would\"\n",
            "2023-11-29 09:23:13,539 : INFO : topic #82 (0.010): 0.025*\"der\" + 0.024*\"und\" + 0.019*\"die\" + 0.014*\"nicht\" + 0.013*\"als\" + 0.012*\"ist\" + 0.012*\"dass\" + 0.011*\"hat\" + 0.008*\"wie\" + 0.008*\"sie\"\n",
            "2023-11-29 09:23:13,543 : INFO : topic #98 (0.010): 0.006*\"deriv\" + 0.004*\"finger\" + 0.004*\"first\" + 0.003*\"two\" + 0.003*\"would\" + 0.003*\"libre\" + 0.003*\"harp\" + 0.003*\"upon\" + 0.003*\"tapping\" + 0.003*\"time\"\n",
            "2023-11-29 09:23:13,569 : INFO : topic diff=0.537291, rho=0.108763\n",
            "2023-11-29 09:23:13,866 : INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #52000/163069, outstanding queue size 19\n",
            "2023-11-29 09:23:14,106 : INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #54000/163069, outstanding queue size 20\n",
            "2023-11-29 09:23:14,318 : INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #56000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:21,801 : INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #58000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:24,281 : INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #60000/163069, outstanding queue size 20\n",
            "2023-11-29 09:23:24,502 : INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #62000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:24,859 : INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #64000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:25,780 : INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #66000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:26,365 : INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #68000/163069, outstanding queue size 23\n",
            "2023-11-29 09:23:29,583 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:23:30,438 : INFO : topic #28 (0.010): 0.047*\"poetry\" + 0.021*\"prose\" + 0.016*\"verse\" + 0.009*\"poetic\" + 0.008*\"poets\" + 0.007*\"lyric\" + 0.006*\"poet\" + 0.006*\"metre\" + 0.006*\"form\" + 0.005*\"art\"\n",
            "2023-11-29 09:23:30,441 : INFO : topic #98 (0.010): 0.006*\"deriv\" + 0.004*\"finger\" + 0.003*\"first\" + 0.003*\"two\" + 0.003*\"would\" + 0.003*\"libre\" + 0.003*\"harp\" + 0.003*\"upon\" + 0.003*\"tapping\" + 0.003*\"time\"\n",
            "2023-11-29 09:23:30,446 : INFO : topic #31 (0.010): 0.015*\"john\" + 0.010*\"william\" + 0.009*\"english\" + 0.007*\"london\" + 0.007*\"thomas\" + 0.007*\"sir\" + 0.007*\"rev\" + 0.006*\"george\" + 0.006*\"mrs\" + 0.006*\"james\"\n",
            "2023-11-29 09:23:30,451 : INFO : topic #32 (0.010): 0.011*\"new\" + 0.011*\"professor\" + 0.009*\"york\" + 0.008*\"english\" + 0.007*\"district\" + 0.006*\"work\" + 0.006*\"university\" + 0.005*\"book\" + 0.004*\"first\" + 0.004*\"greek\"\n",
            "2023-11-29 09:23:30,455 : INFO : topic #2 (0.010): 0.032*\"les\" + 0.030*\"que\" + 0.018*\"qui\" + 0.017*\"des\" + 0.013*\"dans\" + 0.011*\"est\" + 0.010*\"une\" + 0.010*\"par\" + 0.010*\"plus\" + 0.009*\"pour\"\n",
            "2023-11-29 09:23:30,483 : INFO : topic diff=0.536247, rho=0.108763\n",
            "2023-11-29 09:23:30,787 : INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #70000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:39,553 : INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #72000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:39,936 : INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #74000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:42,325 : INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #76000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:42,565 : INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #78000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:42,957 : INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #80000/163069, outstanding queue size 23\n",
            "2023-11-29 09:23:46,727 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:23:47,592 : INFO : topic #49 (0.010): 0.005*\"word\" + 0.005*\"obs\" + 0.005*\"two\" + 0.005*\"much\" + 0.004*\"many\" + 0.004*\"words\" + 0.004*\"term\" + 0.004*\"first\" + 0.003*\"among\" + 0.003*\"english\"\n",
            "2023-11-29 09:23:47,597 : INFO : topic #35 (0.010): 0.036*\"verse\" + 0.010*\"verses\" + 0.010*\"blank\" + 0.007*\"syllables\" + 0.006*\"feet\" + 0.006*\"ear\" + 0.005*\"english\" + 0.005*\"lines\" + 0.005*\"two\" + 0.005*\"would\"\n",
            "2023-11-29 09:23:47,601 : INFO : topic #39 (0.010): 0.015*\"latin\" + 0.012*\"greek\" + 0.011*\"french\" + 0.009*\"roman\" + 0.008*\"history\" + 0.008*\"poetry\" + 0.008*\"english\" + 0.007*\"eloquence\" + 0.007*\"ancient\" + 0.005*\"hebrew\"\n",
            "2023-11-29 09:23:47,605 : INFO : topic #70 (0.010): 0.013*\"book\" + 0.012*\"work\" + 0.010*\"school\" + 0.008*\"schools\" + 0.008*\"grammar\" + 0.008*\"reading\" + 0.007*\"study\" + 0.007*\"english\" + 0.007*\"teacher\" + 0.007*\"teachers\"\n",
            "2023-11-29 09:23:47,609 : INFO : topic #47 (0.010): 0.014*\"text\" + 0.013*\"edition\" + 0.008*\"work\" + 0.007*\"many\" + 0.007*\"notes\" + 0.007*\"volume\" + 0.006*\"volumes\" + 0.006*\"present\" + 0.005*\"made\" + 0.005*\"time\"\n",
            "2023-11-29 09:23:47,645 : INFO : topic diff=0.542812, rho=0.108763\n",
            "2023-11-29 09:23:47,904 : INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #82000/163069, outstanding queue size 20\n",
            "2023-11-29 09:23:48,120 : INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #84000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:56,119 : INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #86000/163069, outstanding queue size 21\n",
            "2023-11-29 09:23:56,375 : INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #88000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:57,510 : INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #90000/163069, outstanding queue size 22\n",
            "2023-11-29 09:23:57,871 : INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #92000/163069, outstanding queue size 23\n",
            "2023-11-29 09:24:01,289 : INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #94000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:02,465 : INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #96000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:02,704 : INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #98000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:04,567 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:24:05,455 : INFO : topic #87 (0.010): 0.008*\"new\" + 0.005*\"year\" + 0.005*\"states\" + 0.005*\"war\" + 0.004*\"two\" + 0.004*\"united\" + 0.004*\"american\" + 0.004*\"first\" + 0.004*\"committee\" + 0.003*\"city\"\n",
            "2023-11-29 09:24:05,459 : INFO : topic #67 (0.010): 0.009*\"printed\" + 0.009*\"edition\" + 0.007*\"sir\" + 0.006*\"new\" + 0.006*\"first\" + 0.005*\"london\" + 0.005*\"book\" + 0.005*\"published\" + 0.004*\"vol\" + 0.004*\"henry\"\n",
            "2023-11-29 09:24:05,465 : INFO : topic #95 (0.010): 0.019*\"taste\" + 0.007*\"virtue\" + 0.006*\"mind\" + 0.006*\"nature\" + 0.006*\"man\" + 0.005*\"good\" + 0.005*\"men\" + 0.005*\"human\" + 0.004*\"great\" + 0.004*\"sense\"\n",
            "2023-11-29 09:24:05,469 : INFO : topic #28 (0.010): 0.052*\"poetry\" + 0.022*\"prose\" + 0.017*\"verse\" + 0.010*\"poetic\" + 0.008*\"poets\" + 0.007*\"poet\" + 0.007*\"lyric\" + 0.007*\"metre\" + 0.006*\"form\" + 0.005*\"art\"\n",
            "2023-11-29 09:24:05,475 : INFO : topic #5 (0.010): 0.027*\"noun\" + 0.026*\"name\" + 0.025*\"nouns\" + 0.021*\"names\" + 0.014*\"word\" + 0.013*\"case\" + 0.012*\"proper\" + 0.012*\"common\" + 0.012*\"used\" + 0.011*\"words\"\n",
            "2023-11-29 09:24:05,501 : INFO : topic diff=0.546122, rho=0.108763\n",
            "2023-11-29 09:24:13,439 : INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #100000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:13,887 : INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #102000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:15,585 : INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #104000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:15,838 : INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #106000/163069, outstanding queue size 23\n",
            "2023-11-29 09:24:20,483 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:24:21,408 : INFO : topic #1 (0.010): 0.005*\"first\" + 0.004*\"man\" + 0.003*\"ancient\" + 0.003*\"greek\" + 0.003*\"english\" + 0.003*\"many\" + 0.003*\"god\" + 0.003*\"great\" + 0.003*\"two\" + 0.003*\"latin\"\n",
            "2023-11-29 09:24:21,414 : INFO : topic #48 (0.010): 0.041*\"plural\" + 0.019*\"singular\" + 0.015*\"nouns\" + 0.011*\"ending\" + 0.009*\"like\" + 0.009*\"form\" + 0.008*\"sound\" + 0.008*\"words\" + 0.007*\"plurals\" + 0.007*\"gen\"\n",
            "2023-11-29 09:24:21,419 : INFO : topic #16 (0.010): 0.005*\"mid\" + 0.005*\"metaphor\" + 0.004*\"hit\" + 0.004*\"þæt\" + 0.004*\"swa\" + 0.003*\"love\" + 0.003*\"hyperbole\" + 0.003*\"death\" + 0.003*\"last\" + 0.003*\"upon\"\n",
            "2023-11-29 09:24:21,424 : INFO : topic #32 (0.010): 0.013*\"professor\" + 0.013*\"new\" + 0.010*\"york\" + 0.008*\"english\" + 0.008*\"district\" + 0.007*\"university\" + 0.006*\"book\" + 0.006*\"work\" + 0.005*\"act\" + 0.004*\"year\"\n",
            "2023-11-29 09:24:21,429 : INFO : topic #49 (0.010): 0.005*\"word\" + 0.005*\"much\" + 0.005*\"obs\" + 0.005*\"two\" + 0.004*\"term\" + 0.004*\"many\" + 0.004*\"words\" + 0.004*\"first\" + 0.003*\"among\" + 0.003*\"even\"\n",
            "2023-11-29 09:24:21,457 : INFO : topic diff=0.552530, rho=0.108763\n",
            "2023-11-29 09:24:21,477 : INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #108000/163069, outstanding queue size 19\n",
            "2023-11-29 09:24:21,709 : INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #110000/163069, outstanding queue size 20\n",
            "2023-11-29 09:24:21,949 : INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #112000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:28,817 : INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #114000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:30,495 : INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #116000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:31,828 : INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #118000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:32,222 : INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #120000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:35,588 : INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #122000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:36,888 : INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #124000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:37,262 : INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #126000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:38,557 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:24:39,436 : INFO : topic #28 (0.010): 0.056*\"poetry\" + 0.023*\"prose\" + 0.018*\"verse\" + 0.011*\"poetic\" + 0.009*\"poets\" + 0.008*\"poet\" + 0.007*\"lyric\" + 0.007*\"form\" + 0.006*\"metre\" + 0.006*\"art\"\n",
            "2023-11-29 09:24:39,441 : INFO : topic #89 (0.010): 0.018*\"voice\" + 0.011*\"speaking\" + 0.010*\"speaker\" + 0.010*\"expression\" + 0.008*\"gesture\" + 0.008*\"elocution\" + 0.008*\"must\" + 0.007*\"words\" + 0.007*\"reading\" + 0.006*\"speech\"\n",
            "2023-11-29 09:24:39,446 : INFO : topic #42 (0.010): 0.005*\"time\" + 0.004*\"years\" + 0.004*\"old\" + 0.004*\"great\" + 0.004*\"people\" + 0.004*\"country\" + 0.004*\"men\" + 0.004*\"man\" + 0.004*\"upon\" + 0.003*\"son\"\n",
            "2023-11-29 09:24:39,450 : INFO : topic #79 (0.010): 0.012*\"description\" + 0.011*\"chapter\" + 0.011*\"composition\" + 0.011*\"paragraph\" + 0.010*\"style\" + 0.008*\"theme\" + 0.008*\"subject\" + 0.007*\"unity\" + 0.007*\"exposition\" + 0.007*\"general\"\n",
            "2023-11-29 09:24:39,456 : INFO : topic #65 (0.010): 0.033*\"syllable\" + 0.028*\"word\" + 0.028*\"words\" + 0.021*\"vowel\" + 0.019*\"accent\" + 0.016*\"two\" + 0.016*\"syllables\" + 0.014*\"long\" + 0.011*\"vowels\" + 0.011*\"consonant\"\n",
            "2023-11-29 09:24:39,482 : INFO : topic diff=0.555630, rho=0.108763\n",
            "2023-11-29 09:24:45,067 : INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #128000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:45,631 : INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #130000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:48,521 : INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #132000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:49,123 : INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #134000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:51,961 : INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #136000/163069, outstanding queue size 21\n",
            "2023-11-29 09:24:52,298 : INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #138000/163069, outstanding queue size 22\n",
            "2023-11-29 09:24:55,719 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:24:56,603 : INFO : topic #74 (0.010): 0.062*\"words\" + 0.016*\"word\" + 0.016*\"spelling\" + 0.015*\"pronunciation\" + 0.010*\"language\" + 0.009*\"sounds\" + 0.008*\"found\" + 0.006*\"would\" + 0.006*\"letters\" + 0.006*\"written\"\n",
            "2023-11-29 09:24:56,607 : INFO : topic #8 (0.010): 0.006*\"like\" + 0.004*\"eyes\" + 0.004*\"night\" + 0.003*\"hand\" + 0.003*\"day\" + 0.003*\"head\" + 0.003*\"long\" + 0.003*\"little\" + 0.003*\"light\" + 0.003*\"old\"\n",
            "2023-11-29 09:24:56,612 : INFO : topic #4 (0.010): 0.026*\"english\" + 0.011*\"latin\" + 0.011*\"words\" + 0.011*\"languages\" + 0.010*\"german\" + 0.010*\"language\" + 0.010*\"pronunciation\" + 0.009*\"french\" + 0.008*\"sound\" + 0.008*\"old\"\n",
            "2023-11-29 09:24:56,618 : INFO : topic #10 (0.010): 0.009*\"poetry\" + 0.007*\"wordsworth\" + 0.007*\"coleridge\" + 0.005*\"language\" + 0.005*\"like\" + 0.004*\"mind\" + 0.004*\"upon\" + 0.004*\"life\" + 0.004*\"nature\" + 0.004*\"new\"\n",
            "2023-11-29 09:24:56,626 : INFO : topic #57 (0.010): 0.016*\"der\" + 0.014*\"des\" + 0.013*\"french\" + 0.013*\"von\" + 0.010*\"english\" + 0.010*\"und\" + 0.009*\"see\" + 0.007*\"german\" + 0.006*\"paris\" + 0.005*\"die\"\n",
            "2023-11-29 09:24:56,657 : INFO : topic diff=0.556428, rho=0.108763\n",
            "2023-11-29 09:24:56,676 : INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #140000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:02,243 : INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #142000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:02,487 : INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #144000/163069, outstanding queue size 22\n",
            "2023-11-29 09:25:04,794 : INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #146000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:06,072 : INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #148000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:07,553 : INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #150000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:08,933 : INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #152000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:12,642 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:25:13,498 : INFO : topic #77 (0.010): 0.025*\"verse\" + 0.011*\"chap\" + 0.010*\"english\" + 0.009*\"stress\" + 0.009*\"poetry\" + 0.008*\"quantity\" + 0.007*\"accent\" + 0.007*\"greek\" + 0.007*\"syllables\" + 0.006*\"music\"\n",
            "2023-11-29 09:25:13,502 : INFO : topic #43 (0.010): 0.007*\"songs\" + 0.007*\"minstrels\" + 0.005*\"time\" + 0.005*\"great\" + 0.004*\"king\" + 0.004*\"first\" + 0.004*\"year\" + 0.004*\"written\" + 0.004*\"minstrel\" + 0.004*\"song\"\n",
            "2023-11-29 09:25:13,508 : INFO : topic #49 (0.010): 0.006*\"obs\" + 0.005*\"word\" + 0.005*\"much\" + 0.005*\"term\" + 0.004*\"two\" + 0.004*\"many\" + 0.004*\"among\" + 0.003*\"words\" + 0.003*\"first\" + 0.003*\"even\"\n",
            "2023-11-29 09:25:13,513 : INFO : topic #63 (0.010): 0.015*\"first\" + 0.011*\"sound\" + 0.009*\"second\" + 0.009*\"represents\" + 0.007*\"impression\" + 0.007*\"word\" + 0.005*\"upon\" + 0.004*\"also\" + 0.004*\"image\" + 0.004*\"made\"\n",
            "2023-11-29 09:25:13,519 : INFO : topic #66 (0.010): 0.010*\"hav\" + 0.010*\"wil\" + 0.007*\"dat\" + 0.005*\"coll\" + 0.004*\"hiz\" + 0.004*\"oxon\" + 0.004*\"wel\" + 0.004*\"haz\" + 0.004*\"bin\" + 0.003*\"dis\"\n",
            "2023-11-29 09:25:13,547 : INFO : topic diff=0.557649, rho=0.108763\n",
            "2023-11-29 09:25:13,565 : INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #154000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:18,742 : INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #156000/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:19,013 : INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #158000/163069, outstanding queue size 22\n",
            "2023-11-29 09:25:23,051 : INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #160000/163069, outstanding queue size 19\n",
            "2023-11-29 09:25:23,314 : INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #162000/163069, outstanding queue size 20\n",
            "2023-11-29 09:25:23,606 : INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #163069/163069, outstanding queue size 21\n",
            "2023-11-29 09:25:30,511 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:25:31,415 : INFO : topic #95 (0.010): 0.019*\"taste\" + 0.008*\"virtue\" + 0.007*\"mind\" + 0.006*\"nature\" + 0.006*\"good\" + 0.006*\"man\" + 0.005*\"men\" + 0.005*\"human\" + 0.005*\"sense\" + 0.004*\"great\"\n",
            "2023-11-29 09:25:31,419 : INFO : topic #41 (0.010): 0.015*\"rhythm\" + 0.011*\"two\" + 0.009*\"time\" + 0.007*\"foot\" + 0.007*\"first\" + 0.007*\"verse\" + 0.006*\"feet\" + 0.006*\"long\" + 0.005*\"number\" + 0.005*\"would\"\n",
            "2023-11-29 09:25:31,424 : INFO : topic #61 (0.010): 0.009*\"like\" + 0.009*\"venice\" + 0.005*\"rome\" + 0.004*\"plato\" + 0.004*\"english\" + 0.004*\"herodotus\" + 0.004*\"hoy\" + 0.003*\"plutarch\" + 0.003*\"grandeur\" + 0.003*\"florence\"\n",
            "2023-11-29 09:25:31,428 : INFO : topic #0 (0.010): 0.007*\"every\" + 0.007*\"would\" + 0.006*\"great\" + 0.006*\"much\" + 0.006*\"upon\" + 0.005*\"rules\" + 0.005*\"art\" + 0.005*\"nature\" + 0.005*\"manner\" + 0.005*\"must\"\n",
            "2023-11-29 09:25:31,432 : INFO : topic #58 (0.010): 0.007*\"eyes\" + 0.005*\"upon\" + 0.005*\"body\" + 0.004*\"motion\" + 0.004*\"eye\" + 0.004*\"hand\" + 0.004*\"voice\" + 0.004*\"tone\" + 0.003*\"whole\" + 0.003*\"time\"\n",
            "2023-11-29 09:25:31,461 : INFO : topic diff=0.554639, rho=0.108763\n",
            "2023-11-29 09:25:43,909 : INFO : -9.539 per-word bound, 743.7 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:25:49,801 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:25:50,681 : INFO : topic #34 (0.010): 0.008*\"day\" + 0.007*\"month\" + 0.004*\"eclogues\" + 0.004*\"new\" + 0.004*\"pass\" + 0.004*\"eclogue\" + 0.004*\"upon\" + 0.004*\"many\" + 0.003*\"art\" + 0.003*\"rest\"\n",
            "2023-11-29 09:25:50,686 : INFO : topic #13 (0.010): 0.013*\"poets\" + 0.011*\"dryden\" + 0.011*\"pope\" + 0.011*\"milton\" + 0.011*\"spenser\" + 0.010*\"poetry\" + 0.009*\"poet\" + 0.008*\"poems\" + 0.007*\"age\" + 0.006*\"style\"\n",
            "2023-11-29 09:25:50,690 : INFO : topic #98 (0.010): 0.005*\"finger\" + 0.004*\"libre\" + 0.004*\"instruments\" + 0.003*\"first\" + 0.003*\"two\" + 0.003*\"deriv\" + 0.003*\"lap\" + 0.003*\"harp\" + 0.003*\"tapping\" + 0.003*\"gramophone\"\n",
            "2023-11-29 09:25:50,694 : INFO : topic #16 (0.010): 0.006*\"metaphor\" + 0.005*\"mid\" + 0.004*\"hyperbole\" + 0.004*\"hit\" + 0.004*\"death\" + 0.004*\"personification\" + 0.004*\"love\" + 0.003*\"figures\" + 0.003*\"irony\" + 0.003*\"swa\"\n",
            "2023-11-29 09:25:50,698 : INFO : topic #11 (0.010): 0.004*\"first\" + 0.003*\"yet\" + 0.003*\"two\" + 0.003*\"every\" + 0.003*\"man\" + 0.003*\"nature\" + 0.003*\"like\" + 0.003*\"great\" + 0.003*\"without\" + 0.003*\"richter\"\n",
            "2023-11-29 09:25:50,725 : INFO : topic diff=0.554851, rho=0.108763\n",
            "2023-11-29 09:26:02,873 : INFO : -9.540 per-word bound, 744.6 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:26:07,476 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:26:08,357 : INFO : topic #3 (0.010): 0.014*\"sect\" + 0.011*\"muscles\" + 0.009*\"breathing\" + 0.009*\"part\" + 0.009*\"air\" + 0.009*\"chest\" + 0.008*\"lungs\" + 0.008*\"body\" + 0.006*\"fig\" + 0.006*\"breath\"\n",
            "2023-11-29 09:26:08,361 : INFO : topic #99 (0.010): 0.007*\"would\" + 0.005*\"great\" + 0.005*\"even\" + 0.005*\"must\" + 0.005*\"much\" + 0.005*\"life\" + 0.004*\"upon\" + 0.004*\"men\" + 0.004*\"time\" + 0.004*\"many\"\n",
            "2023-11-29 09:26:08,368 : INFO : topic #75 (0.010): 0.014*\"words\" + 0.014*\"word\" + 0.011*\"sentence\" + 0.011*\"subject\" + 0.009*\"tell\" + 0.009*\"sentences\" + 0.009*\"following\" + 0.008*\"write\" + 0.008*\"predicate\" + 0.007*\"lesson\"\n",
            "2023-11-29 09:26:08,371 : INFO : topic #30 (0.010): 0.016*\"music\" + 0.006*\"art\" + 0.006*\"poetry\" + 0.006*\"musical\" + 0.005*\"must\" + 0.005*\"singing\" + 0.004*\"reading\" + 0.004*\"beauty\" + 0.004*\"would\" + 0.004*\"read\"\n",
            "2023-11-29 09:26:08,375 : INFO : topic #52 (0.010): 0.083*\"see\" + 0.004*\"angular\" + 0.002*\"place\" + 0.002*\"vessel\" + 0.002*\"post\" + 0.002*\"words\" + 0.002*\"boo\" + 0.002*\"imogen\" + 0.001*\"also\" + 0.001*\"wale\"\n",
            "2023-11-29 09:26:08,402 : INFO : topic diff=0.553651, rho=0.108763\n",
            "2023-11-29 09:26:14,520 : INFO : -9.540 per-word bound, 744.5 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:26:17,831 : INFO : merging changes from 9069 documents into a model of 163069 documents\n",
            "2023-11-29 09:26:18,426 : INFO : topic #46 (0.010): 0.059*\"der\" + 0.056*\"die\" + 0.042*\"und\" + 0.023*\"von\" + 0.023*\"den\" + 0.019*\"das\" + 0.019*\"des\" + 0.012*\"mit\" + 0.011*\"dem\" + 0.011*\"sich\"\n",
            "2023-11-29 09:26:18,429 : INFO : topic #64 (0.010): 0.071*\"sound\" + 0.024*\"short\" + 0.020*\"long\" + 0.008*\"sounds\" + 0.007*\"letter\" + 0.007*\"broad\" + 0.007*\"heard\" + 0.006*\"french\" + 0.005*\"pronunciation\" + 0.005*\"like\"\n",
            "2023-11-29 09:26:18,433 : INFO : topic #74 (0.010): 0.064*\"words\" + 0.017*\"pronunciation\" + 0.016*\"word\" + 0.015*\"spelling\" + 0.011*\"language\" + 0.010*\"sounds\" + 0.008*\"found\" + 0.007*\"would\" + 0.006*\"letters\" + 0.006*\"written\"\n",
            "2023-11-29 09:26:18,436 : INFO : topic #50 (0.010): 0.008*\"reverend\" + 0.005*\"two\" + 0.004*\"church\" + 0.003*\"though\" + 0.003*\"papal\" + 0.003*\"good\" + 0.002*\"present\" + 0.002*\"soph\" + 0.002*\"would\" + 0.002*\"gach\"\n",
            "2023-11-29 09:26:18,438 : INFO : topic #89 (0.010): 0.023*\"voice\" + 0.012*\"speaking\" + 0.011*\"speaker\" + 0.010*\"expression\" + 0.009*\"elocution\" + 0.009*\"reading\" + 0.008*\"must\" + 0.008*\"gesture\" + 0.008*\"emphasis\" + 0.007*\"words\"\n",
            "2023-11-29 09:26:18,459 : INFO : topic diff=0.557127, rho=0.108763\n",
            "2023-11-29 09:26:21,550 : INFO : -9.474 per-word bound, 711.3 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:26:21,729 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #2000/163069, outstanding queue size 1\n",
            "2023-11-29 09:26:21,977 : INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #4000/163069, outstanding queue size 2\n",
            "2023-11-29 09:26:22,124 : INFO : PROGRESS: pass 3, dispatched chunk #2 = documents up to #6000/163069, outstanding queue size 3\n",
            "2023-11-29 09:26:22,373 : INFO : PROGRESS: pass 3, dispatched chunk #3 = documents up to #8000/163069, outstanding queue size 4\n",
            "2023-11-29 09:26:22,524 : INFO : PROGRESS: pass 3, dispatched chunk #4 = documents up to #10000/163069, outstanding queue size 5\n",
            "2023-11-29 09:26:22,793 : INFO : PROGRESS: pass 3, dispatched chunk #5 = documents up to #12000/163069, outstanding queue size 6\n",
            "2023-11-29 09:26:22,966 : INFO : PROGRESS: pass 3, dispatched chunk #6 = documents up to #14000/163069, outstanding queue size 7\n",
            "2023-11-29 09:26:23,273 : INFO : PROGRESS: pass 3, dispatched chunk #7 = documents up to #16000/163069, outstanding queue size 8\n",
            "2023-11-29 09:26:23,648 : INFO : PROGRESS: pass 3, dispatched chunk #8 = documents up to #18000/163069, outstanding queue size 9\n",
            "2023-11-29 09:26:24,000 : INFO : PROGRESS: pass 3, dispatched chunk #9 = documents up to #20000/163069, outstanding queue size 10\n",
            "2023-11-29 09:26:24,222 : INFO : PROGRESS: pass 3, dispatched chunk #10 = documents up to #22000/163069, outstanding queue size 11\n",
            "2023-11-29 09:26:24,460 : INFO : PROGRESS: pass 3, dispatched chunk #11 = documents up to #24000/163069, outstanding queue size 12\n",
            "2023-11-29 09:26:24,679 : INFO : PROGRESS: pass 3, dispatched chunk #12 = documents up to #26000/163069, outstanding queue size 13\n",
            "2023-11-29 09:26:25,092 : INFO : PROGRESS: pass 3, dispatched chunk #13 = documents up to #28000/163069, outstanding queue size 14\n",
            "2023-11-29 09:26:25,340 : INFO : PROGRESS: pass 3, dispatched chunk #14 = documents up to #30000/163069, outstanding queue size 15\n",
            "2023-11-29 09:26:25,707 : INFO : PROGRESS: pass 3, dispatched chunk #15 = documents up to #32000/163069, outstanding queue size 16\n",
            "2023-11-29 09:26:25,950 : INFO : PROGRESS: pass 3, dispatched chunk #16 = documents up to #34000/163069, outstanding queue size 17\n",
            "2023-11-29 09:26:26,203 : INFO : PROGRESS: pass 3, dispatched chunk #17 = documents up to #36000/163069, outstanding queue size 18\n",
            "2023-11-29 09:26:26,601 : INFO : PROGRESS: pass 3, dispatched chunk #18 = documents up to #38000/163069, outstanding queue size 19\n",
            "2023-11-29 09:26:26,853 : INFO : PROGRESS: pass 3, dispatched chunk #19 = documents up to #40000/163069, outstanding queue size 20\n",
            "2023-11-29 09:26:27,105 : INFO : PROGRESS: pass 3, dispatched chunk #20 = documents up to #42000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:38,288 : INFO : PROGRESS: pass 3, dispatched chunk #21 = documents up to #44000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:39,977 : INFO : PROGRESS: pass 3, dispatched chunk #22 = documents up to #46000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:40,215 : INFO : PROGRESS: pass 3, dispatched chunk #23 = documents up to #48000/163069, outstanding queue size 22\n",
            "2023-11-29 09:26:45,508 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:26:46,402 : INFO : topic #2 (0.010): 0.036*\"les\" + 0.034*\"que\" + 0.019*\"des\" + 0.019*\"qui\" + 0.014*\"dans\" + 0.013*\"est\" + 0.012*\"par\" + 0.011*\"une\" + 0.010*\"plus\" + 0.010*\"pour\"\n",
            "2023-11-29 09:26:46,407 : INFO : topic #94 (0.010): 0.008*\"part\" + 0.007*\"king\" + 0.005*\"fide\" + 0.005*\"coast\" + 0.004*\"kings\" + 0.004*\"called\" + 0.004*\"britain\" + 0.004*\"sea\" + 0.004*\"son\" + 0.004*\"name\"\n",
            "2023-11-29 09:26:46,411 : INFO : topic #28 (0.010): 0.070*\"poetry\" + 0.025*\"prose\" + 0.019*\"verse\" + 0.012*\"poetic\" + 0.012*\"poets\" + 0.010*\"poet\" + 0.008*\"form\" + 0.008*\"lyric\" + 0.006*\"art\" + 0.006*\"metre\"\n",
            "2023-11-29 09:26:46,417 : INFO : topic #40 (0.010): 0.009*\"upon\" + 0.007*\"great\" + 0.006*\"yet\" + 0.006*\"much\" + 0.005*\"'tis\" + 0.005*\"fame\" + 0.004*\"poet\" + 0.004*\"never\" + 0.004*\"without\" + 0.004*\"like\"\n",
            "2023-11-29 09:26:46,423 : INFO : topic #80 (0.010): 0.019*\"ideas\" + 0.014*\"mind\" + 0.013*\"language\" + 0.013*\"idea\" + 0.011*\"man\" + 0.010*\"thought\" + 0.010*\"things\" + 0.008*\"words\" + 0.008*\"objects\" + 0.006*\"express\"\n",
            "2023-11-29 09:26:46,449 : INFO : topic diff=0.548617, rho=0.108126\n",
            "2023-11-29 09:26:46,717 : INFO : PROGRESS: pass 3, dispatched chunk #24 = documents up to #50000/163069, outstanding queue size 18\n",
            "2023-11-29 09:26:46,969 : INFO : PROGRESS: pass 3, dispatched chunk #25 = documents up to #52000/163069, outstanding queue size 19\n",
            "2023-11-29 09:26:47,237 : INFO : PROGRESS: pass 3, dispatched chunk #26 = documents up to #54000/163069, outstanding queue size 20\n",
            "2023-11-29 09:26:47,471 : INFO : PROGRESS: pass 3, dispatched chunk #27 = documents up to #56000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:54,566 : INFO : PROGRESS: pass 3, dispatched chunk #28 = documents up to #58000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:56,402 : INFO : PROGRESS: pass 3, dispatched chunk #29 = documents up to #60000/163069, outstanding queue size 21\n",
            "2023-11-29 09:26:56,634 : INFO : PROGRESS: pass 3, dispatched chunk #30 = documents up to #62000/163069, outstanding queue size 22\n",
            "2023-11-29 09:26:57,022 : INFO : PROGRESS: pass 3, dispatched chunk #31 = documents up to #64000/163069, outstanding queue size 23\n",
            "2023-11-29 09:26:59,157 : INFO : PROGRESS: pass 3, dispatched chunk #32 = documents up to #66000/163069, outstanding queue size 22\n",
            "2023-11-29 09:27:00,434 : INFO : PROGRESS: pass 3, dispatched chunk #33 = documents up to #68000/163069, outstanding queue size 22\n",
            "2023-11-29 09:27:03,276 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:27:04,162 : INFO : topic #31 (0.010): 0.017*\"john\" + 0.012*\"william\" + 0.008*\"thomas\" + 0.008*\"english\" + 0.008*\"rev\" + 0.007*\"mrs\" + 0.007*\"sir\" + 0.007*\"george\" + 0.007*\"james\" + 0.007*\"esq\"\n",
            "2023-11-29 09:27:04,167 : INFO : topic #21 (0.010): 0.025*\"poetry\" + 0.016*\"poems\" + 0.013*\"english\" + 0.012*\"poem\" + 0.011*\"epic\" + 0.010*\"style\" + 0.009*\"century\" + 0.008*\"literature\" + 0.007*\"chapter\" + 0.007*\"history\"\n",
            "2023-11-29 09:27:04,173 : INFO : topic #23 (0.010): 0.006*\"ane\" + 0.004*\"baldwin\" + 0.003*\"men\" + 0.003*\"johnson\" + 0.003*\"law\" + 0.003*\"davis\" + 0.003*\"man\" + 0.003*\"thair\" + 0.003*\"þat\" + 0.003*\"english\"\n",
            "2023-11-29 09:27:04,179 : INFO : topic #43 (0.010): 0.009*\"songs\" + 0.008*\"minstrels\" + 0.006*\"minstrel\" + 0.005*\"time\" + 0.005*\"song\" + 0.005*\"king\" + 0.005*\"great\" + 0.005*\"year\" + 0.004*\"old\" + 0.004*\"written\"\n",
            "2023-11-29 09:27:04,191 : INFO : topic #99 (0.010): 0.007*\"would\" + 0.005*\"even\" + 0.005*\"great\" + 0.005*\"life\" + 0.005*\"must\" + 0.005*\"much\" + 0.004*\"upon\" + 0.004*\"men\" + 0.004*\"time\" + 0.004*\"many\"\n",
            "2023-11-29 09:27:04,220 : INFO : topic diff=0.537948, rho=0.108126\n",
            "2023-11-29 09:27:04,241 : INFO : PROGRESS: pass 3, dispatched chunk #34 = documents up to #70000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:12,665 : INFO : PROGRESS: pass 3, dispatched chunk #35 = documents up to #72000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:13,820 : INFO : PROGRESS: pass 3, dispatched chunk #36 = documents up to #74000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:15,019 : INFO : PROGRESS: pass 3, dispatched chunk #37 = documents up to #76000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:17,449 : INFO : PROGRESS: pass 3, dispatched chunk #38 = documents up to #78000/163069, outstanding queue size 20\n",
            "2023-11-29 09:27:17,694 : INFO : PROGRESS: pass 3, dispatched chunk #39 = documents up to #80000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:17,925 : INFO : PROGRESS: pass 3, dispatched chunk #40 = documents up to #82000/163069, outstanding queue size 22\n",
            "2023-11-29 09:27:19,007 : INFO : PROGRESS: pass 3, dispatched chunk #41 = documents up to #84000/163069, outstanding queue size 22\n",
            "2023-11-29 09:27:20,553 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:27:21,445 : INFO : topic #63 (0.010): 0.016*\"first\" + 0.010*\"represents\" + 0.009*\"second\" + 0.009*\"impression\" + 0.008*\"sound\" + 0.007*\"word\" + 0.006*\"upon\" + 0.005*\"also\" + 0.005*\"made\" + 0.004*\"eye\"\n",
            "2023-11-29 09:27:21,451 : INFO : topic #45 (0.010): 0.050*\"read\" + 0.018*\"net\" + 0.007*\"note\" + 0.004*\"crown\" + 0.004*\"see\" + 0.004*\"veritas\" + 0.003*\"table\" + 0.003*\"scientia\" + 0.003*\"van\" + 0.003*\"bib\"\n",
            "2023-11-29 09:27:21,456 : INFO : topic #96 (0.010): 0.005*\"first\" + 0.005*\"time\" + 0.004*\"brutus\" + 0.004*\"part\" + 0.004*\"speech\" + 0.004*\"senate\" + 0.004*\"place\" + 0.003*\"upon\" + 0.003*\"ancient\" + 0.003*\"scipio\"\n",
            "2023-11-29 09:27:21,460 : INFO : topic #69 (0.010): 0.010*\"ter\" + 0.008*\"per\" + 0.006*\"con\" + 0.006*\"ing\" + 0.004*\"ble\" + 0.004*\"ment\" + 0.004*\"ful\" + 0.004*\"der\" + 0.004*\"man\" + 0.004*\"com\"\n",
            "2023-11-29 09:27:21,465 : INFO : topic #27 (0.010): 0.014*\"4th\" + 0.013*\"verses\" + 0.010*\"accent\" + 0.009*\"6th\" + 0.009*\"5th\" + 0.008*\"syllables\" + 0.008*\"7th\" + 0.007*\"pause\" + 0.007*\"verse\" + 0.007*\"last\"\n",
            "2023-11-29 09:27:21,492 : INFO : topic diff=0.531899, rho=0.108126\n",
            "2023-11-29 09:27:29,070 : INFO : PROGRESS: pass 3, dispatched chunk #42 = documents up to #86000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:30,109 : INFO : PROGRESS: pass 3, dispatched chunk #43 = documents up to #88000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:33,197 : INFO : PROGRESS: pass 3, dispatched chunk #44 = documents up to #90000/163069, outstanding queue size 20\n",
            "2023-11-29 09:27:33,412 : INFO : PROGRESS: pass 3, dispatched chunk #45 = documents up to #92000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:35,879 : INFO : PROGRESS: pass 3, dispatched chunk #46 = documents up to #94000/163069, outstanding queue size 20\n",
            "2023-11-29 09:27:36,095 : INFO : PROGRESS: pass 3, dispatched chunk #47 = documents up to #96000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:38,249 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:27:39,152 : INFO : topic #60 (0.010): 0.009*\"would\" + 0.008*\"man\" + 0.007*\"little\" + 0.006*\"much\" + 0.006*\"well\" + 0.006*\"life\" + 0.006*\"good\" + 0.006*\"could\" + 0.005*\"know\" + 0.005*\"like\"\n",
            "2023-11-29 09:27:39,156 : INFO : topic #16 (0.010): 0.007*\"metaphor\" + 0.005*\"mid\" + 0.005*\"figures\" + 0.005*\"simile\" + 0.004*\"þæt\" + 0.004*\"hyperbole\" + 0.004*\"swa\" + 0.004*\"ond\" + 0.004*\"hit\" + 0.004*\"death\"\n",
            "2023-11-29 09:27:39,162 : INFO : topic #12 (0.010): 0.018*\"gender\" + 0.018*\"feminine\" + 0.018*\"female\" + 0.018*\"lady\" + 0.015*\"masculine\" + 0.015*\"male\" + 0.013*\"sex\" + 0.013*\"woman\" + 0.011*\"man\" + 0.011*\"wife\"\n",
            "2023-11-29 09:27:39,168 : INFO : topic #25 (0.010): 0.003*\"also\" + 0.003*\"two\" + 0.003*\"old\" + 0.003*\"upon\" + 0.003*\"king\" + 0.002*\"see\" + 0.002*\"time\" + 0.002*\"pam\" + 0.002*\"mari\" + 0.002*\"would\"\n",
            "2023-11-29 09:27:39,173 : INFO : topic #89 (0.010): 0.027*\"voice\" + 0.013*\"speaking\" + 0.012*\"speaker\" + 0.010*\"expression\" + 0.010*\"reading\" + 0.010*\"elocution\" + 0.009*\"emphasis\" + 0.009*\"must\" + 0.008*\"speech\" + 0.007*\"gesture\"\n",
            "2023-11-29 09:27:39,205 : INFO : topic diff=0.523330, rho=0.108126\n",
            "2023-11-29 09:27:39,226 : INFO : PROGRESS: pass 3, dispatched chunk #48 = documents up to #98000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:47,301 : INFO : PROGRESS: pass 3, dispatched chunk #49 = documents up to #100000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:48,614 : INFO : PROGRESS: pass 3, dispatched chunk #50 = documents up to #102000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:49,838 : INFO : PROGRESS: pass 3, dispatched chunk #51 = documents up to #104000/163069, outstanding queue size 21\n",
            "2023-11-29 09:27:50,099 : INFO : PROGRESS: pass 3, dispatched chunk #52 = documents up to #106000/163069, outstanding queue size 22\n",
            "2023-11-29 09:27:56,209 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:27:57,118 : INFO : topic #63 (0.010): 0.016*\"first\" + 0.010*\"represents\" + 0.010*\"second\" + 0.009*\"impression\" + 0.008*\"sound\" + 0.007*\"upon\" + 0.006*\"word\" + 0.005*\"also\" + 0.004*\"made\" + 0.004*\"image\"\n",
            "2023-11-29 09:27:57,125 : INFO : topic #3 (0.010): 0.014*\"sect\" + 0.010*\"muscles\" + 0.009*\"air\" + 0.009*\"body\" + 0.009*\"breathing\" + 0.009*\"part\" + 0.009*\"lungs\" + 0.008*\"chest\" + 0.007*\"fig\" + 0.007*\"right\"\n",
            "2023-11-29 09:27:57,129 : INFO : topic #0 (0.010): 0.008*\"every\" + 0.008*\"would\" + 0.007*\"much\" + 0.007*\"great\" + 0.007*\"upon\" + 0.005*\"rules\" + 0.005*\"art\" + 0.005*\"manner\" + 0.005*\"without\" + 0.005*\"must\"\n",
            "2023-11-29 09:27:57,134 : INFO : topic #48 (0.010): 0.049*\"plural\" + 0.025*\"singular\" + 0.022*\"nouns\" + 0.012*\"ending\" + 0.011*\"form\" + 0.009*\"number\" + 0.008*\"gen\" + 0.008*\"like\" + 0.008*\"adding\" + 0.008*\"words\"\n",
            "2023-11-29 09:27:57,139 : INFO : topic #79 (0.010): 0.015*\"chapter\" + 0.014*\"description\" + 0.014*\"composition\" + 0.013*\"style\" + 0.012*\"paragraph\" + 0.009*\"subject\" + 0.009*\"theme\" + 0.008*\"general\" + 0.008*\"unity\" + 0.007*\"exposition\"\n",
            "2023-11-29 09:27:57,167 : INFO : topic diff=0.519121, rho=0.108126\n",
            "2023-11-29 09:27:57,190 : INFO : PROGRESS: pass 3, dispatched chunk #53 = documents up to #108000/163069, outstanding queue size 19\n",
            "2023-11-29 09:27:57,418 : INFO : PROGRESS: pass 3, dispatched chunk #54 = documents up to #110000/163069, outstanding queue size 20\n",
            "2023-11-29 09:27:57,655 : INFO : PROGRESS: pass 3, dispatched chunk #55 = documents up to #112000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:04,478 : INFO : PROGRESS: pass 3, dispatched chunk #56 = documents up to #114000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:06,197 : INFO : PROGRESS: pass 3, dispatched chunk #57 = documents up to #116000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:08,085 : INFO : PROGRESS: pass 3, dispatched chunk #58 = documents up to #118000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:09,266 : INFO : PROGRESS: pass 3, dispatched chunk #59 = documents up to #120000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:11,024 : INFO : PROGRESS: pass 3, dispatched chunk #60 = documents up to #122000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:12,653 : INFO : PROGRESS: pass 3, dispatched chunk #61 = documents up to #124000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:13,024 : INFO : PROGRESS: pass 3, dispatched chunk #62 = documents up to #126000/163069, outstanding queue size 22\n",
            "2023-11-29 09:28:14,486 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:28:15,384 : INFO : topic #40 (0.010): 0.010*\"upon\" + 0.007*\"great\" + 0.007*\"yet\" + 0.006*\"much\" + 0.005*\"'tis\" + 0.005*\"fame\" + 0.004*\"poet\" + 0.004*\"never\" + 0.004*\"like\" + 0.004*\"without\"\n",
            "2023-11-29 09:28:15,389 : INFO : topic #61 (0.010): 0.009*\"like\" + 0.009*\"venice\" + 0.007*\"plato\" + 0.006*\"rome\" + 0.005*\"herodotus\" + 0.005*\"plutarch\" + 0.005*\"hoy\" + 0.004*\"catullus\" + 0.004*\"thucydides\" + 0.003*\"tacitus\"\n",
            "2023-11-29 09:28:15,395 : INFO : topic #41 (0.010): 0.014*\"rhythm\" + 0.012*\"two\" + 0.010*\"time\" + 0.008*\"first\" + 0.007*\"foot\" + 0.006*\"number\" + 0.006*\"long\" + 0.006*\"feet\" + 0.006*\"verse\" + 0.005*\"would\"\n",
            "2023-11-29 09:28:15,399 : INFO : topic #82 (0.010): 0.024*\"und\" + 0.022*\"der\" + 0.018*\"als\" + 0.018*\"die\" + 0.016*\"nicht\" + 0.016*\"ist\" + 0.015*\"hat\" + 0.014*\"dass\" + 0.012*\"sie\" + 0.010*\"wie\"\n",
            "2023-11-29 09:28:15,405 : INFO : topic #52 (0.010): 0.121*\"see\" + 0.003*\"angular\" + 0.002*\"place\" + 0.002*\"ayr\" + 0.002*\"also\" + 0.002*\"strait\" + 0.002*\"table\" + 0.002*\"post\" + 0.002*\"sce\" + 0.002*\"desert\"\n",
            "2023-11-29 09:28:15,433 : INFO : topic diff=0.512906, rho=0.108126\n",
            "2023-11-29 09:28:21,301 : INFO : PROGRESS: pass 3, dispatched chunk #63 = documents up to #128000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:21,703 : INFO : PROGRESS: pass 3, dispatched chunk #64 = documents up to #130000/163069, outstanding queue size 22\n",
            "2023-11-29 09:28:24,764 : INFO : PROGRESS: pass 3, dispatched chunk #65 = documents up to #132000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:25,934 : INFO : PROGRESS: pass 3, dispatched chunk #66 = documents up to #134000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:28,619 : INFO : PROGRESS: pass 3, dispatched chunk #67 = documents up to #136000/163069, outstanding queue size 20\n",
            "2023-11-29 09:28:28,940 : INFO : PROGRESS: pass 3, dispatched chunk #68 = documents up to #138000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:32,571 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:28:33,475 : INFO : topic #28 (0.010): 0.081*\"poetry\" + 0.025*\"prose\" + 0.019*\"verse\" + 0.014*\"poetic\" + 0.014*\"poets\" + 0.012*\"poet\" + 0.009*\"form\" + 0.008*\"lyric\" + 0.007*\"art\" + 0.005*\"metre\"\n",
            "2023-11-29 09:28:33,480 : INFO : topic #59 (0.010): 0.037*\"language\" + 0.035*\"grammar\" + 0.025*\"letters\" + 0.024*\"words\" + 0.018*\"english\" + 0.011*\"latin\" + 0.010*\"parts\" + 0.010*\"part\" + 0.008*\"languages\" + 0.008*\"tongue\"\n",
            "2023-11-29 09:28:33,486 : INFO : topic #8 (0.010): 0.006*\"like\" + 0.004*\"night\" + 0.004*\"eyes\" + 0.004*\"day\" + 0.003*\"head\" + 0.003*\"little\" + 0.003*\"light\" + 0.003*\"old\" + 0.003*\"hand\" + 0.003*\"long\"\n",
            "2023-11-29 09:28:33,491 : INFO : topic #62 (0.010): 0.012*\"english\" + 0.011*\"study\" + 0.010*\"language\" + 0.008*\"work\" + 0.007*\"use\" + 0.007*\"writing\" + 0.006*\"composition\" + 0.006*\"student\" + 0.006*\"must\" + 0.006*\"good\"\n",
            "2023-11-29 09:28:33,494 : INFO : topic #30 (0.010): 0.018*\"music\" + 0.007*\"art\" + 0.006*\"musical\" + 0.005*\"poetry\" + 0.005*\"singing\" + 0.005*\"song\" + 0.005*\"must\" + 0.005*\"beauty\" + 0.004*\"even\" + 0.004*\"would\"\n",
            "2023-11-29 09:28:33,521 : INFO : topic diff=0.500883, rho=0.108126\n",
            "2023-11-29 09:28:33,546 : INFO : PROGRESS: pass 3, dispatched chunk #69 = documents up to #140000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:38,615 : INFO : PROGRESS: pass 3, dispatched chunk #70 = documents up to #142000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:38,997 : INFO : PROGRESS: pass 3, dispatched chunk #71 = documents up to #144000/163069, outstanding queue size 22\n",
            "2023-11-29 09:28:41,652 : INFO : PROGRESS: pass 3, dispatched chunk #72 = documents up to #146000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:43,039 : INFO : PROGRESS: pass 3, dispatched chunk #73 = documents up to #148000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:45,054 : INFO : PROGRESS: pass 3, dispatched chunk #74 = documents up to #150000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:46,662 : INFO : PROGRESS: pass 3, dispatched chunk #75 = documents up to #152000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:50,143 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:28:51,046 : INFO : topic #13 (0.010): 0.015*\"poets\" + 0.013*\"pope\" + 0.013*\"milton\" + 0.013*\"dryden\" + 0.011*\"spenser\" + 0.010*\"poetry\" + 0.009*\"poet\" + 0.008*\"poems\" + 0.008*\"age\" + 0.007*\"style\"\n",
            "2023-11-29 09:28:51,052 : INFO : topic #51 (0.010): 0.022*\"sir\" + 0.020*\"court\" + 0.015*\"king\" + 0.010*\"john\" + 0.010*\"earl\" + 0.010*\"lord\" + 0.010*\"street\" + 0.009*\"henry\" + 0.009*\"richard\" + 0.007*\"william\"\n",
            "2023-11-29 09:28:51,057 : INFO : topic #99 (0.010): 0.007*\"would\" + 0.005*\"great\" + 0.005*\"even\" + 0.005*\"life\" + 0.005*\"must\" + 0.005*\"much\" + 0.005*\"men\" + 0.004*\"upon\" + 0.004*\"time\" + 0.004*\"man\"\n",
            "2023-11-29 09:28:51,066 : INFO : topic #93 (0.010): 0.008*\"also\" + 0.006*\"tho\" + 0.006*\"haue\" + 0.005*\"see\" + 0.004*\"god\" + 0.004*\"men\" + 0.004*\"iii\" + 0.004*\"good\" + 0.004*\"hath\" + 0.004*\"man\"\n",
            "2023-11-29 09:28:51,072 : INFO : topic #64 (0.010): 0.093*\"sound\" + 0.029*\"short\" + 0.023*\"long\" + 0.009*\"sounds\" + 0.008*\"broad\" + 0.008*\"letter\" + 0.007*\"heard\" + 0.007*\"french\" + 0.007*\"like\" + 0.006*\"pronunciation\"\n",
            "2023-11-29 09:28:51,099 : INFO : topic diff=0.493960, rho=0.108126\n",
            "2023-11-29 09:28:51,135 : INFO : PROGRESS: pass 3, dispatched chunk #76 = documents up to #154000/163069, outstanding queue size 21\n",
            "2023-11-29 09:28:56,509 : INFO : PROGRESS: pass 3, dispatched chunk #77 = documents up to #156000/163069, outstanding queue size 20\n",
            "2023-11-29 09:28:56,770 : INFO : PROGRESS: pass 3, dispatched chunk #78 = documents up to #158000/163069, outstanding queue size 21\n",
            "2023-11-29 09:29:01,449 : INFO : PROGRESS: pass 3, dispatched chunk #79 = documents up to #160000/163069, outstanding queue size 19\n",
            "2023-11-29 09:29:01,729 : INFO : PROGRESS: pass 3, dispatched chunk #80 = documents up to #162000/163069, outstanding queue size 20\n",
            "2023-11-29 09:29:01,848 : INFO : PROGRESS: pass 3, dispatched chunk #81 = documents up to #163069/163069, outstanding queue size 21\n",
            "2023-11-29 09:29:09,048 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:29:09,954 : INFO : topic #7 (0.010): 0.012*\"homer\" + 0.006*\"iliad\" + 0.006*\"like\" + 0.005*\"translation\" + 0.005*\"less\" + 0.004*\"least\" + 0.004*\"even\" + 0.004*\"much\" + 0.003*\"never\" + 0.003*\"yet\"\n",
            "2023-11-29 09:29:09,960 : INFO : topic #35 (0.010): 0.058*\"verse\" + 0.016*\"verses\" + 0.014*\"blank\" + 0.011*\"feet\" + 0.010*\"syllables\" + 0.008*\"ear\" + 0.006*\"lines\" + 0.006*\"two\" + 0.006*\"pause\" + 0.006*\"milton\"\n",
            "2023-11-29 09:29:09,964 : INFO : topic #30 (0.010): 0.018*\"music\" + 0.007*\"art\" + 0.006*\"musical\" + 0.005*\"singing\" + 0.005*\"song\" + 0.005*\"beauty\" + 0.005*\"poetry\" + 0.005*\"must\" + 0.004*\"even\" + 0.004*\"would\"\n",
            "2023-11-29 09:29:09,969 : INFO : topic #51 (0.010): 0.023*\"sir\" + 0.019*\"court\" + 0.015*\"king\" + 0.011*\"lord\" + 0.011*\"john\" + 0.010*\"earl\" + 0.010*\"street\" + 0.009*\"henry\" + 0.009*\"richard\" + 0.007*\"william\"\n",
            "2023-11-29 09:29:09,974 : INFO : topic #74 (0.010): 0.068*\"words\" + 0.019*\"word\" + 0.019*\"pronunciation\" + 0.018*\"spelling\" + 0.011*\"language\" + 0.010*\"sounds\" + 0.007*\"would\" + 0.007*\"dictionary\" + 0.007*\"letters\" + 0.007*\"found\"\n",
            "2023-11-29 09:29:10,010 : INFO : topic diff=0.480987, rho=0.108126\n",
            "2023-11-29 09:29:22,663 : INFO : -9.487 per-word bound, 717.7 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:29:30,116 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:29:31,148 : INFO : topic #98 (0.010): 0.007*\"finger\" + 0.005*\"instruments\" + 0.005*\"major\" + 0.004*\"libre\" + 0.004*\"lap\" + 0.004*\"deriv\" + 0.004*\"moses\" + 0.004*\"harmonic\" + 0.003*\"record\" + 0.003*\"disc\"\n",
            "2023-11-29 09:29:31,154 : INFO : topic #36 (0.010): 0.031*\"vowel\" + 0.024*\"vowels\" + 0.013*\"tongue\" + 0.012*\"back\" + 0.012*\"front\" + 0.011*\"found\" + 0.009*\"consonant\" + 0.009*\"sound\" + 0.009*\"voiced\" + 0.007*\"position\"\n",
            "2023-11-29 09:29:31,159 : INFO : topic #83 (0.010): 0.020*\"play\" + 0.018*\"plays\" + 0.012*\"drama\" + 0.011*\"shakespeare\" + 0.011*\"stage\" + 0.007*\"dramatic\" + 0.006*\"first\" + 0.006*\"scene\" + 0.006*\"tragedy\" + 0.005*\"shakespeare's\"\n",
            "2023-11-29 09:29:31,164 : INFO : topic #10 (0.010): 0.015*\"wordsworth\" + 0.014*\"coleridge\" + 0.008*\"poetry\" + 0.006*\"life\" + 0.006*\"byron\" + 0.005*\"shelley\" + 0.005*\"upon\" + 0.005*\"like\" + 0.005*\"mind\" + 0.005*\"coleridge's\"\n",
            "2023-11-29 09:29:31,170 : INFO : topic #0 (0.010): 0.009*\"every\" + 0.008*\"would\" + 0.007*\"much\" + 0.007*\"great\" + 0.007*\"upon\" + 0.005*\"rules\" + 0.005*\"art\" + 0.005*\"without\" + 0.005*\"manner\" + 0.005*\"must\"\n",
            "2023-11-29 09:29:31,196 : INFO : topic diff=0.471878, rho=0.108126\n",
            "2023-11-29 09:29:42,281 : INFO : -9.488 per-word bound, 717.9 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:29:51,055 : INFO : merging changes from 23069 documents into a model of 163069 documents\n",
            "2023-11-29 09:29:51,666 : INFO : topic #8 (0.010): 0.006*\"like\" + 0.004*\"night\" + 0.004*\"eyes\" + 0.004*\"day\" + 0.003*\"head\" + 0.003*\"little\" + 0.003*\"old\" + 0.003*\"light\" + 0.003*\"long\" + 0.003*\"hand\"\n",
            "2023-11-29 09:29:51,670 : INFO : topic #66 (0.010): 0.012*\"wil\" + 0.011*\"hav\" + 0.009*\"dat\" + 0.006*\"dhi\" + 0.005*\"wel\" + 0.005*\"hiz\" + 0.005*\"coll\" + 0.005*\"haz\" + 0.004*\"bin\" + 0.004*\"dis\"\n",
            "2023-11-29 09:29:51,672 : INFO : topic #30 (0.010): 0.019*\"music\" + 0.007*\"art\" + 0.007*\"musical\" + 0.005*\"singing\" + 0.005*\"song\" + 0.005*\"beauty\" + 0.005*\"must\" + 0.004*\"poetry\" + 0.004*\"even\" + 0.004*\"would\"\n",
            "2023-11-29 09:29:51,675 : INFO : topic #0 (0.010): 0.009*\"every\" + 0.008*\"would\" + 0.007*\"much\" + 0.007*\"great\" + 0.007*\"upon\" + 0.005*\"rules\" + 0.005*\"art\" + 0.005*\"without\" + 0.005*\"manner\" + 0.005*\"must\"\n",
            "2023-11-29 09:29:51,679 : INFO : topic #62 (0.010): 0.012*\"english\" + 0.011*\"study\" + 0.010*\"language\" + 0.008*\"work\" + 0.008*\"use\" + 0.007*\"writing\" + 0.006*\"composition\" + 0.006*\"student\" + 0.006*\"must\" + 0.006*\"good\"\n",
            "2023-11-29 09:29:51,703 : INFO : topic diff=0.463612, rho=0.108126\n",
            "2023-11-29 09:29:54,783 : INFO : -9.459 per-word bound, 704.0 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:29:54,957 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #2000/163069, outstanding queue size 1\n",
            "2023-11-29 09:29:55,201 : INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #4000/163069, outstanding queue size 2\n",
            "2023-11-29 09:29:55,345 : INFO : PROGRESS: pass 4, dispatched chunk #2 = documents up to #6000/163069, outstanding queue size 3\n",
            "2023-11-29 09:29:55,683 : INFO : PROGRESS: pass 4, dispatched chunk #3 = documents up to #8000/163069, outstanding queue size 4\n",
            "2023-11-29 09:29:55,828 : INFO : PROGRESS: pass 4, dispatched chunk #4 = documents up to #10000/163069, outstanding queue size 5\n",
            "2023-11-29 09:29:55,981 : INFO : PROGRESS: pass 4, dispatched chunk #5 = documents up to #12000/163069, outstanding queue size 6\n",
            "2023-11-29 09:29:56,248 : INFO : PROGRESS: pass 4, dispatched chunk #6 = documents up to #14000/163069, outstanding queue size 7\n",
            "2023-11-29 09:29:56,426 : INFO : PROGRESS: pass 4, dispatched chunk #7 = documents up to #16000/163069, outstanding queue size 8\n",
            "2023-11-29 09:29:56,768 : INFO : PROGRESS: pass 4, dispatched chunk #8 = documents up to #18000/163069, outstanding queue size 9\n",
            "2023-11-29 09:29:56,944 : INFO : PROGRESS: pass 4, dispatched chunk #9 = documents up to #20000/163069, outstanding queue size 10\n",
            "2023-11-29 09:29:57,151 : INFO : PROGRESS: pass 4, dispatched chunk #10 = documents up to #22000/163069, outstanding queue size 11\n",
            "2023-11-29 09:29:57,522 : INFO : PROGRESS: pass 4, dispatched chunk #11 = documents up to #24000/163069, outstanding queue size 12\n",
            "2023-11-29 09:29:57,731 : INFO : PROGRESS: pass 4, dispatched chunk #12 = documents up to #26000/163069, outstanding queue size 13\n",
            "2023-11-29 09:29:57,965 : INFO : PROGRESS: pass 4, dispatched chunk #13 = documents up to #28000/163069, outstanding queue size 14\n",
            "2023-11-29 09:29:58,347 : INFO : PROGRESS: pass 4, dispatched chunk #14 = documents up to #30000/163069, outstanding queue size 15\n",
            "2023-11-29 09:29:58,568 : INFO : PROGRESS: pass 4, dispatched chunk #15 = documents up to #32000/163069, outstanding queue size 16\n",
            "2023-11-29 09:29:58,808 : INFO : PROGRESS: pass 4, dispatched chunk #16 = documents up to #34000/163069, outstanding queue size 17\n",
            "2023-11-29 09:29:59,216 : INFO : PROGRESS: pass 4, dispatched chunk #17 = documents up to #36000/163069, outstanding queue size 18\n",
            "2023-11-29 09:29:59,441 : INFO : PROGRESS: pass 4, dispatched chunk #18 = documents up to #38000/163069, outstanding queue size 19\n",
            "2023-11-29 09:29:59,722 : INFO : PROGRESS: pass 4, dispatched chunk #19 = documents up to #40000/163069, outstanding queue size 20\n",
            "2023-11-29 09:30:00,131 : INFO : PROGRESS: pass 4, dispatched chunk #20 = documents up to #42000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:10,149 : INFO : PROGRESS: pass 4, dispatched chunk #21 = documents up to #44000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:11,670 : INFO : PROGRESS: pass 4, dispatched chunk #22 = documents up to #46000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:13,085 : INFO : PROGRESS: pass 4, dispatched chunk #23 = documents up to #48000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:13,294 : INFO : PROGRESS: pass 4, dispatched chunk #24 = documents up to #50000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:14,577 : INFO : PROGRESS: pass 4, dispatched chunk #25 = documents up to #52000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:14,964 : INFO : PROGRESS: pass 4, dispatched chunk #26 = documents up to #54000/163069, outstanding queue size 23\n",
            "2023-11-29 09:30:16,962 : INFO : PROGRESS: pass 4, dispatched chunk #27 = documents up to #56000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:18,350 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:30:19,249 : INFO : topic #92 (0.010): 0.041*\"verb\" + 0.027*\"verbs\" + 0.021*\"person\" + 0.017*\"pronouns\" + 0.014*\"noun\" + 0.012*\"pronoun\" + 0.011*\"nouns\" + 0.011*\"case\" + 0.010*\"nominative\" + 0.010*\"number\"\n",
            "2023-11-29 09:30:19,254 : INFO : topic #68 (0.010): 0.048*\"thou\" + 0.039*\"thy\" + 0.030*\"love\" + 0.023*\"thee\" + 0.016*\"shall\" + 0.011*\"let\" + 0.007*\"lord\" + 0.006*\"loved\" + 0.006*\"yet\" + 0.006*\"doth\"\n",
            "2023-11-29 09:30:19,258 : INFO : topic #74 (0.010): 0.068*\"words\" + 0.019*\"pronunciation\" + 0.019*\"word\" + 0.017*\"spelling\" + 0.011*\"language\" + 0.010*\"sounds\" + 0.007*\"would\" + 0.007*\"dictionary\" + 0.007*\"letters\" + 0.006*\"found\"\n",
            "2023-11-29 09:30:19,262 : INFO : topic #6 (0.010): 0.015*\"xxiii\" + 0.015*\"xxii\" + 0.015*\"xxiv\" + 0.014*\"xxv\" + 0.013*\"xix\" + 0.013*\"xvii\" + 0.013*\"xxi\" + 0.013*\"xxvii\" + 0.012*\"xxvi\" + 0.011*\"xxx\"\n",
            "2023-11-29 09:30:19,266 : INFO : topic #38 (0.010): 0.016*\"government\" + 0.011*\"war\" + 0.009*\"political\" + 0.008*\"policy\" + 0.007*\"law\" + 0.006*\"letter\" + 0.005*\"party\" + 0.005*\"british\" + 0.004*\"state\" + 0.004*\"towards\"\n",
            "2023-11-29 09:30:19,293 : INFO : topic diff=0.455456, rho=0.107499\n",
            "2023-11-29 09:30:25,735 : INFO : PROGRESS: pass 4, dispatched chunk #28 = documents up to #58000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:28,305 : INFO : PROGRESS: pass 4, dispatched chunk #29 = documents up to #60000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:28,673 : INFO : PROGRESS: pass 4, dispatched chunk #30 = documents up to #62000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:29,941 : INFO : PROGRESS: pass 4, dispatched chunk #31 = documents up to #64000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:31,502 : INFO : PROGRESS: pass 4, dispatched chunk #32 = documents up to #66000/163069, outstanding queue size 22\n",
            "2023-11-29 09:30:31,779 : INFO : PROGRESS: pass 4, dispatched chunk #33 = documents up to #68000/163069, outstanding queue size 23\n",
            "2023-11-29 09:30:35,659 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:30:36,529 : INFO : topic #16 (0.010): 0.009*\"metaphor\" + 0.007*\"figures\" + 0.006*\"mid\" + 0.006*\"simile\" + 0.005*\"hyperbole\" + 0.005*\"swa\" + 0.005*\"allegory\" + 0.005*\"tropes\" + 0.005*\"personification\" + 0.005*\"ond\"\n",
            "2023-11-29 09:30:36,533 : INFO : topic #35 (0.010): 0.061*\"verse\" + 0.016*\"verses\" + 0.014*\"blank\" + 0.011*\"feet\" + 0.010*\"syllables\" + 0.009*\"ear\" + 0.006*\"lines\" + 0.006*\"pause\" + 0.006*\"iambic\" + 0.006*\"two\"\n",
            "2023-11-29 09:30:36,537 : INFO : topic #6 (0.010): 0.015*\"xxiii\" + 0.015*\"xxiv\" + 0.015*\"xxii\" + 0.014*\"xxv\" + 0.014*\"xix\" + 0.014*\"xxi\" + 0.014*\"xvii\" + 0.013*\"xxvii\" + 0.013*\"xxvi\" + 0.012*\"xxx\"\n",
            "2023-11-29 09:30:36,542 : INFO : topic #69 (0.010): 0.011*\"ter\" + 0.008*\"per\" + 0.006*\"con\" + 0.006*\"ing\" + 0.004*\"ble\" + 0.004*\"ful\" + 0.004*\"ment\" + 0.004*\"man\" + 0.004*\"der\" + 0.004*\"com\"\n",
            "2023-11-29 09:30:36,547 : INFO : topic #14 (0.010): 0.017*\"tion\" + 0.016*\"ing\" + 0.008*\"con\" + 0.006*\"com\" + 0.005*\"pro\" + 0.005*\"thing\" + 0.005*\"tions\" + 0.005*\"pre\" + 0.005*\"ver\" + 0.005*\"make\"\n",
            "2023-11-29 09:30:36,569 : INFO : topic diff=0.440458, rho=0.107499\n",
            "2023-11-29 09:30:37,054 : INFO : PROGRESS: pass 4, dispatched chunk #34 = documents up to #70000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:42,780 : INFO : PROGRESS: pass 4, dispatched chunk #35 = documents up to #72000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:43,514 : INFO : PROGRESS: pass 4, dispatched chunk #36 = documents up to #74000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:49,310 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:30:50,339 : INFO : topic #16 (0.010): 0.009*\"metaphor\" + 0.007*\"mid\" + 0.007*\"figures\" + 0.006*\"simile\" + 0.005*\"hyperbole\" + 0.005*\"allegory\" + 0.005*\"swa\" + 0.005*\"ond\" + 0.005*\"personification\" + 0.005*\"tropes\"\n",
            "2023-11-29 09:30:50,347 : INFO : topic #87 (0.010): 0.010*\"new\" + 0.006*\"year\" + 0.006*\"states\" + 0.005*\"american\" + 0.005*\"committee\" + 0.004*\"united\" + 0.004*\"two\" + 0.004*\"president\" + 0.004*\"america\" + 0.004*\"war\"\n",
            "2023-11-29 09:30:50,351 : INFO : topic #25 (0.010): 0.003*\"pam\" + 0.003*\"mari\" + 0.003*\"also\" + 0.002*\"old\" + 0.002*\"king\" + 0.002*\"hine\" + 0.002*\"two\" + 0.002*\"upon\" + 0.002*\"see\" + 0.002*\"cons\"\n",
            "2023-11-29 09:30:50,357 : INFO : topic #41 (0.010): 0.014*\"rhythm\" + 0.012*\"two\" + 0.011*\"time\" + 0.008*\"first\" + 0.007*\"foot\" + 0.006*\"number\" + 0.006*\"second\" + 0.006*\"would\" + 0.006*\"long\" + 0.005*\"three\"\n",
            "2023-11-29 09:30:50,362 : INFO : topic #11 (0.010): 0.003*\"first\" + 0.003*\"like\" + 0.003*\"yet\" + 0.003*\"every\" + 0.003*\"richter\" + 0.003*\"many\" + 0.003*\"nature\" + 0.003*\"indeed\" + 0.003*\"man\" + 0.003*\"much\"\n",
            "2023-11-29 09:30:50,388 : INFO : topic diff=0.429099, rho=0.107499\n",
            "2023-11-29 09:30:50,417 : INFO : PROGRESS: pass 4, dispatched chunk #37 = documents up to #76000/163069, outstanding queue size 17\n",
            "2023-11-29 09:30:50,663 : INFO : PROGRESS: pass 4, dispatched chunk #38 = documents up to #78000/163069, outstanding queue size 18\n",
            "2023-11-29 09:30:50,903 : INFO : PROGRESS: pass 4, dispatched chunk #39 = documents up to #80000/163069, outstanding queue size 19\n",
            "2023-11-29 09:30:51,135 : INFO : PROGRESS: pass 4, dispatched chunk #40 = documents up to #82000/163069, outstanding queue size 20\n",
            "2023-11-29 09:30:51,364 : INFO : PROGRESS: pass 4, dispatched chunk #41 = documents up to #84000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:57,031 : INFO : PROGRESS: pass 4, dispatched chunk #42 = documents up to #86000/163069, outstanding queue size 21\n",
            "2023-11-29 09:30:58,404 : INFO : PROGRESS: pass 4, dispatched chunk #43 = documents up to #88000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:02,617 : INFO : PROGRESS: pass 4, dispatched chunk #44 = documents up to #90000/163069, outstanding queue size 20\n",
            "2023-11-29 09:31:03,278 : INFO : PROGRESS: pass 4, dispatched chunk #45 = documents up to #92000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:06,826 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:31:07,793 : INFO : topic #90 (0.010): 0.020*\"art\" + 0.010*\"rhetoric\" + 0.009*\"science\" + 0.008*\"principles\" + 0.007*\"must\" + 0.006*\"knowledge\" + 0.006*\"nature\" + 0.006*\"speech\" + 0.005*\"subject\" + 0.005*\"language\"\n",
            "2023-11-29 09:31:07,797 : INFO : topic #30 (0.010): 0.019*\"music\" + 0.008*\"art\" + 0.007*\"musical\" + 0.005*\"singing\" + 0.005*\"beauty\" + 0.005*\"song\" + 0.005*\"must\" + 0.004*\"even\" + 0.004*\"would\" + 0.004*\"poetry\"\n",
            "2023-11-29 09:31:07,801 : INFO : topic #15 (0.010): 0.010*\"god\" + 0.007*\"earth\" + 0.007*\"life\" + 0.006*\"heaven\" + 0.006*\"world\" + 0.006*\"man\" + 0.006*\"death\" + 0.006*\"soul\" + 0.005*\"love\" + 0.004*\"like\"\n",
            "2023-11-29 09:31:07,805 : INFO : topic #20 (0.010): 0.010*\"horn\" + 0.008*\"ballade\" + 0.005*\"idg\" + 0.004*\"estimate\" + 0.004*\"image\" + 0.004*\"sec\" + 0.004*\"visual\" + 0.004*\"auditory\" + 0.003*\"syllogism\" + 0.003*\"hadde\"\n",
            "2023-11-29 09:31:07,809 : INFO : topic #94 (0.010): 0.010*\"part\" + 0.008*\"sea\" + 0.008*\"king\" + 0.007*\"coast\" + 0.006*\"island\" + 0.005*\"called\" + 0.005*\"asia\" + 0.005*\"river\" + 0.005*\"britain\" + 0.005*\"name\"\n",
            "2023-11-29 09:31:07,837 : INFO : topic diff=0.416116, rho=0.107499\n",
            "2023-11-29 09:31:07,858 : INFO : PROGRESS: pass 4, dispatched chunk #46 = documents up to #94000/163069, outstanding queue size 19\n",
            "2023-11-29 09:31:08,097 : INFO : PROGRESS: pass 4, dispatched chunk #47 = documents up to #96000/163069, outstanding queue size 20\n",
            "2023-11-29 09:31:08,340 : INFO : PROGRESS: pass 4, dispatched chunk #48 = documents up to #98000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:13,899 : INFO : PROGRESS: pass 4, dispatched chunk #49 = documents up to #100000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:15,283 : INFO : PROGRESS: pass 4, dispatched chunk #50 = documents up to #102000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:19,387 : INFO : PROGRESS: pass 4, dispatched chunk #51 = documents up to #104000/163069, outstanding queue size 20\n",
            "2023-11-29 09:31:19,691 : INFO : PROGRESS: pass 4, dispatched chunk #52 = documents up to #106000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:21,773 : INFO : PROGRESS: pass 4, dispatched chunk #53 = documents up to #108000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:22,098 : INFO : PROGRESS: pass 4, dispatched chunk #54 = documents up to #110000/163069, outstanding queue size 22\n",
            "2023-11-29 09:31:24,790 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:31:25,683 : INFO : topic #62 (0.010): 0.012*\"english\" + 0.011*\"study\" + 0.009*\"language\" + 0.008*\"work\" + 0.008*\"use\" + 0.007*\"writing\" + 0.007*\"must\" + 0.006*\"composition\" + 0.006*\"good\" + 0.006*\"student\"\n",
            "2023-11-29 09:31:25,687 : INFO : topic #53 (0.010): 0.006*\"every\" + 0.006*\"composition\" + 0.005*\"well\" + 0.005*\"never\" + 0.005*\"great\" + 0.004*\"kal\" + 0.004*\"age\" + 0.004*\"attention\" + 0.004*\"much\" + 0.004*\"public\"\n",
            "2023-11-29 09:31:25,691 : INFO : topic #65 (0.010): 0.034*\"syllable\" + 0.032*\"words\" + 0.027*\"word\" + 0.026*\"vowel\" + 0.018*\"accent\" + 0.017*\"syllables\" + 0.016*\"two\" + 0.016*\"vowels\" + 0.015*\"long\" + 0.014*\"consonant\"\n",
            "2023-11-29 09:31:25,695 : INFO : topic #8 (0.010): 0.006*\"like\" + 0.004*\"night\" + 0.004*\"day\" + 0.004*\"eyes\" + 0.003*\"little\" + 0.003*\"old\" + 0.003*\"light\" + 0.003*\"head\" + 0.003*\"long\" + 0.003*\"wind\"\n",
            "2023-11-29 09:31:25,699 : INFO : topic #54 (0.010): 0.023*\"century\" + 0.017*\"english\" + 0.015*\"literature\" + 0.014*\"england\" + 0.013*\"latin\" + 0.013*\"french\" + 0.009*\"period\" + 0.009*\"time\" + 0.008*\"first\" + 0.008*\"france\"\n",
            "2023-11-29 09:31:25,726 : INFO : topic diff=0.407894, rho=0.107499\n",
            "2023-11-29 09:31:25,984 : INFO : PROGRESS: pass 4, dispatched chunk #55 = documents up to #112000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:30,939 : INFO : PROGRESS: pass 4, dispatched chunk #56 = documents up to #114000/163069, outstanding queue size 20\n",
            "2023-11-29 09:31:31,419 : INFO : PROGRESS: pass 4, dispatched chunk #57 = documents up to #116000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:36,275 : INFO : PROGRESS: pass 4, dispatched chunk #58 = documents up to #118000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:38,099 : INFO : PROGRESS: pass 4, dispatched chunk #59 = documents up to #120000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:38,316 : INFO : PROGRESS: pass 4, dispatched chunk #60 = documents up to #122000/163069, outstanding queue size 22\n",
            "2023-11-29 09:31:39,647 : INFO : PROGRESS: pass 4, dispatched chunk #61 = documents up to #124000/163069, outstanding queue size 22\n",
            "2023-11-29 09:31:40,769 : INFO : PROGRESS: pass 4, dispatched chunk #62 = documents up to #126000/163069, outstanding queue size 22\n",
            "2023-11-29 09:31:42,519 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:31:43,419 : INFO : topic #95 (0.010): 0.018*\"taste\" + 0.010*\"virtue\" + 0.010*\"mind\" + 0.007*\"nature\" + 0.007*\"good\" + 0.006*\"human\" + 0.006*\"man\" + 0.006*\"men\" + 0.005*\"passions\" + 0.005*\"life\"\n",
            "2023-11-29 09:31:43,425 : INFO : topic #65 (0.010): 0.034*\"syllable\" + 0.032*\"words\" + 0.027*\"word\" + 0.027*\"vowel\" + 0.018*\"accent\" + 0.017*\"syllables\" + 0.016*\"two\" + 0.016*\"vowels\" + 0.015*\"long\" + 0.014*\"consonant\"\n",
            "2023-11-29 09:31:43,429 : INFO : topic #41 (0.010): 0.014*\"rhythm\" + 0.013*\"two\" + 0.011*\"time\" + 0.008*\"first\" + 0.007*\"foot\" + 0.006*\"number\" + 0.006*\"second\" + 0.006*\"would\" + 0.006*\"long\" + 0.005*\"three\"\n",
            "2023-11-29 09:31:43,433 : INFO : topic #50 (0.010): 0.012*\"reverend\" + 0.008*\"church\" + 0.008*\"hymns\" + 0.008*\"hymn\" + 0.005*\"two\" + 0.004*\"pub\" + 0.003*\"though\" + 0.003*\"papal\" + 0.003*\"brit\" + 0.003*\"good\"\n",
            "2023-11-29 09:31:43,438 : INFO : topic #42 (0.010): 0.005*\"time\" + 0.005*\"years\" + 0.005*\"great\" + 0.004*\"country\" + 0.004*\"people\" + 0.004*\"upon\" + 0.004*\"men\" + 0.004*\"king\" + 0.004*\"son\" + 0.004*\"father\"\n",
            "2023-11-29 09:31:43,465 : INFO : topic diff=0.398372, rho=0.107499\n",
            "2023-11-29 09:31:46,206 : INFO : PROGRESS: pass 4, dispatched chunk #63 = documents up to #128000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:46,587 : INFO : PROGRESS: pass 4, dispatched chunk #64 = documents up to #130000/163069, outstanding queue size 22\n",
            "2023-11-29 09:31:54,917 : INFO : PROGRESS: pass 4, dispatched chunk #65 = documents up to #132000/163069, outstanding queue size 18\n",
            "2023-11-29 09:31:55,184 : INFO : PROGRESS: pass 4, dispatched chunk #66 = documents up to #134000/163069, outstanding queue size 19\n",
            "2023-11-29 09:31:55,603 : INFO : PROGRESS: pass 4, dispatched chunk #67 = documents up to #136000/163069, outstanding queue size 20\n",
            "2023-11-29 09:31:55,832 : INFO : PROGRESS: pass 4, dispatched chunk #68 = documents up to #138000/163069, outstanding queue size 21\n",
            "2023-11-29 09:31:58,593 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:31:59,486 : INFO : topic #33 (0.010): 0.032*\"taste\" + 0.015*\"nature\" + 0.011*\"genius\" + 0.011*\"arts\" + 0.009*\"men\" + 0.007*\"reason\" + 0.006*\"pleasure\" + 0.006*\"beauties\" + 0.005*\"beauty\" + 0.005*\"human\"\n",
            "2023-11-29 09:31:59,491 : INFO : topic #87 (0.010): 0.009*\"new\" + 0.006*\"year\" + 0.006*\"american\" + 0.006*\"states\" + 0.004*\"committee\" + 0.004*\"two\" + 0.004*\"president\" + 0.004*\"america\" + 0.004*\"united\" + 0.004*\"association\"\n",
            "2023-11-29 09:31:59,498 : INFO : topic #53 (0.010): 0.007*\"composition\" + 0.007*\"every\" + 0.005*\"well\" + 0.005*\"never\" + 0.005*\"great\" + 0.004*\"age\" + 0.004*\"much\" + 0.004*\"attention\" + 0.004*\"public\" + 0.004*\"study\"\n",
            "2023-11-29 09:31:59,503 : INFO : topic #21 (0.010): 0.024*\"poetry\" + 0.019*\"poems\" + 0.015*\"poem\" + 0.013*\"english\" + 0.011*\"epic\" + 0.011*\"chapter\" + 0.010*\"literature\" + 0.010*\"century\" + 0.009*\"history\" + 0.008*\"style\"\n",
            "2023-11-29 09:31:59,510 : INFO : topic #70 (0.010): 0.015*\"book\" + 0.014*\"school\" + 0.013*\"work\" + 0.011*\"schools\" + 0.011*\"teacher\" + 0.009*\"grammar\" + 0.009*\"teachers\" + 0.009*\"exercises\" + 0.009*\"reading\" + 0.009*\"teaching\"\n",
            "2023-11-29 09:31:59,536 : INFO : topic diff=0.383452, rho=0.107499\n",
            "2023-11-29 09:31:59,558 : INFO : PROGRESS: pass 4, dispatched chunk #69 = documents up to #140000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:02,683 : INFO : PROGRESS: pass 4, dispatched chunk #70 = documents up to #142000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:02,909 : INFO : PROGRESS: pass 4, dispatched chunk #71 = documents up to #144000/163069, outstanding queue size 22\n",
            "2023-11-29 09:32:08,606 : INFO : PROGRESS: pass 4, dispatched chunk #72 = documents up to #146000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:12,018 : INFO : PROGRESS: pass 4, dispatched chunk #73 = documents up to #148000/163069, outstanding queue size 19\n",
            "2023-11-29 09:32:12,334 : INFO : PROGRESS: pass 4, dispatched chunk #74 = documents up to #150000/163069, outstanding queue size 20\n",
            "2023-11-29 09:32:12,577 : INFO : PROGRESS: pass 4, dispatched chunk #75 = documents up to #152000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:15,316 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:32:16,272 : INFO : topic #67 (0.010): 0.014*\"printed\" + 0.010*\"edition\" + 0.010*\"london\" + 0.007*\"book\" + 0.007*\"vol\" + 0.006*\"first\" + 0.006*\"sir\" + 0.006*\"poems\" + 0.006*\"new\" + 0.006*\"8vo\"\n",
            "2023-11-29 09:32:16,277 : INFO : topic #68 (0.010): 0.050*\"thou\" + 0.042*\"thy\" + 0.032*\"love\" + 0.024*\"thee\" + 0.016*\"shall\" + 0.011*\"let\" + 0.007*\"lord\" + 0.007*\"loved\" + 0.006*\"yet\" + 0.006*\"doth\"\n",
            "2023-11-29 09:32:16,281 : INFO : topic #26 (0.010): 0.004*\"first\" + 0.003*\"vienna\" + 0.003*\"spanish\" + 0.003*\"marriage\" + 0.003*\"see\" + 0.002*\"well\" + 0.002*\"two\" + 0.002*\"satire\" + 0.002*\"like\" + 0.002*\"louis\"\n",
            "2023-11-29 09:32:16,285 : INFO : topic #12 (0.010): 0.026*\"gender\" + 0.023*\"feminine\" + 0.021*\"lady\" + 0.020*\"female\" + 0.019*\"masculine\" + 0.016*\"male\" + 0.015*\"sex\" + 0.015*\"woman\" + 0.013*\"wife\" + 0.012*\"father\"\n",
            "2023-11-29 09:32:16,289 : INFO : topic #96 (0.010): 0.007*\"speech\" + 0.006*\"brutus\" + 0.006*\"first\" + 0.005*\"time\" + 0.005*\"part\" + 0.005*\"oration\" + 0.005*\"senate\" + 0.004*\"cæsar\" + 0.004*\"cicero\" + 0.004*\"upon\"\n",
            "2023-11-29 09:32:16,317 : INFO : topic diff=0.374905, rho=0.107499\n",
            "2023-11-29 09:32:16,337 : INFO : PROGRESS: pass 4, dispatched chunk #76 = documents up to #154000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:19,279 : INFO : PROGRESS: pass 4, dispatched chunk #77 = documents up to #156000/163069, outstanding queue size 20\n",
            "2023-11-29 09:32:19,537 : INFO : PROGRESS: pass 4, dispatched chunk #78 = documents up to #158000/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:26,940 : INFO : PROGRESS: pass 4, dispatched chunk #79 = documents up to #160000/163069, outstanding queue size 19\n",
            "2023-11-29 09:32:27,210 : INFO : PROGRESS: pass 4, dispatched chunk #80 = documents up to #162000/163069, outstanding queue size 20\n",
            "2023-11-29 09:32:27,358 : INFO : PROGRESS: pass 4, dispatched chunk #81 = documents up to #163069/163069, outstanding queue size 21\n",
            "2023-11-29 09:32:32,384 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:32:33,409 : INFO : topic #71 (0.010): 0.028*\"sublime\" + 0.008*\"sublimity\" + 0.007*\"grandeur\" + 0.007*\"great\" + 0.006*\"awful\" + 0.006*\"darkness\" + 0.005*\"nature\" + 0.005*\"ideas\" + 0.005*\"objects\" + 0.005*\"mind\"\n",
            "2023-11-29 09:32:33,414 : INFO : topic #44 (0.010): 0.050*\"12mo\" + 0.047*\"god\" + 0.029*\"london\" + 0.028*\"grammar\" + 0.027*\"english\" + 0.024*\"1st\" + 0.019*\"18mo\" + 0.012*\"boston\" + 0.010*\"8vo\" + 0.010*\"lord\"\n",
            "2023-11-29 09:32:33,422 : INFO : topic #73 (0.010): 0.006*\"part\" + 0.005*\"art\" + 0.004*\"abbot\" + 0.004*\"good\" + 0.004*\"two\" + 0.004*\"abbey\" + 0.004*\"studies\" + 0.004*\"criticism\" + 0.003*\"arts\" + 0.003*\"knowledge\"\n",
            "2023-11-29 09:32:33,427 : INFO : topic #45 (0.010): 0.053*\"read\" + 0.019*\"net\" + 0.007*\"note\" + 0.004*\"van\" + 0.003*\"veritas\" + 0.003*\"artes\" + 0.003*\"pop\" + 0.003*\"sewn\" + 0.003*\"bib\" + 0.003*\"crown\"\n",
            "2023-11-29 09:32:33,434 : INFO : topic #81 (0.010): 0.030*\"long\" + 0.020*\"short\" + 0.013*\"like\" + 0.008*\"sounds\" + 0.006*\"broad\" + 0.006*\"alphabetical\" + 0.006*\"sound\" + 0.005*\"heard\" + 0.004*\"words\" + 0.004*\"pronounced\"\n",
            "2023-11-29 09:32:33,462 : INFO : topic diff=0.360098, rho=0.107499\n",
            "2023-11-29 09:32:45,440 : INFO : -9.492 per-word bound, 720.3 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:32:52,043 : INFO : merging changes from 14000 documents into a model of 163069 documents\n",
            "2023-11-29 09:32:52,990 : INFO : topic #74 (0.010): 0.070*\"words\" + 0.022*\"pronunciation\" + 0.021*\"word\" + 0.018*\"spelling\" + 0.012*\"language\" + 0.010*\"sounds\" + 0.008*\"dictionary\" + 0.008*\"would\" + 0.007*\"letters\" + 0.006*\"written\"\n",
            "2023-11-29 09:32:52,994 : INFO : topic #75 (0.010): 0.015*\"words\" + 0.014*\"word\" + 0.013*\"sentence\" + 0.011*\"sentences\" + 0.010*\"subject\" + 0.010*\"write\" + 0.010*\"tell\" + 0.010*\"following\" + 0.008*\"lesson\" + 0.007*\"predicate\"\n",
            "2023-11-29 09:32:52,999 : INFO : topic #38 (0.010): 0.017*\"government\" + 0.014*\"war\" + 0.010*\"political\" + 0.010*\"law\" + 0.008*\"policy\" + 0.006*\"party\" + 0.006*\"letter\" + 0.005*\"state\" + 0.005*\"british\" + 0.004*\"foreign\"\n",
            "2023-11-29 09:32:53,003 : INFO : topic #88 (0.010): 0.010*\"thing\" + 0.009*\"degree\" + 0.008*\"good\" + 0.007*\"quality\" + 0.007*\"positive\" + 0.007*\"comparative\" + 0.007*\"superlative\" + 0.006*\"signifies\" + 0.005*\"bad\" + 0.005*\"comparison\"\n",
            "2023-11-29 09:32:53,007 : INFO : topic #44 (0.010): 0.049*\"god\" + 0.049*\"12mo\" + 0.029*\"london\" + 0.026*\"grammar\" + 0.026*\"english\" + 0.023*\"1st\" + 0.019*\"18mo\" + 0.012*\"boston\" + 0.011*\"8vo\" + 0.010*\"lord\"\n",
            "2023-11-29 09:32:53,034 : INFO : topic diff=0.350720, rho=0.107499\n",
            "2023-11-29 09:33:04,797 : INFO : -9.495 per-word bound, 721.7 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:33:11,213 : INFO : merging changes from 18000 documents into a model of 163069 documents\n",
            "2023-11-29 09:33:12,097 : INFO : topic #47 (0.010): 0.015*\"edition\" + 0.012*\"text\" + 0.011*\"work\" + 0.010*\"volume\" + 0.009*\"first\" + 0.008*\"many\" + 0.008*\"notes\" + 0.008*\"published\" + 0.008*\"book\" + 0.007*\"made\"\n",
            "2023-11-29 09:33:12,101 : INFO : topic #90 (0.010): 0.021*\"art\" + 0.011*\"rhetoric\" + 0.010*\"science\" + 0.009*\"principles\" + 0.007*\"must\" + 0.006*\"knowledge\" + 0.006*\"nature\" + 0.006*\"subject\" + 0.006*\"speech\" + 0.005*\"system\"\n",
            "2023-11-29 09:33:12,103 : INFO : topic #37 (0.010): 0.035*\"sound\" + 0.035*\"sounds\" + 0.017*\"voice\" + 0.012*\"vowels\" + 0.011*\"mouth\" + 0.011*\"consonants\" + 0.011*\"vocal\" + 0.010*\"tongue\" + 0.010*\"vowel\" + 0.009*\"organs\"\n",
            "2023-11-29 09:33:12,107 : INFO : topic #18 (0.010): 0.031*\"fame\" + 0.013*\"ibid\" + 0.009*\"thle\" + 0.007*\"tile\" + 0.006*\"ill\" + 0.006*\"tie\" + 0.005*\"tle\" + 0.004*\"lie\" + 0.004*\"ion\" + 0.004*\"thie\"\n",
            "2023-11-29 09:33:12,110 : INFO : topic #13 (0.010): 0.015*\"pope\" + 0.015*\"milton\" + 0.014*\"poets\" + 0.013*\"dryden\" + 0.012*\"spenser\" + 0.010*\"poetry\" + 0.009*\"poet\" + 0.009*\"age\" + 0.009*\"poems\" + 0.007*\"style\"\n",
            "2023-11-29 09:33:12,135 : INFO : topic diff=0.341271, rho=0.107499\n",
            "2023-11-29 09:33:15,765 : INFO : -9.492 per-word bound, 720.1 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:33:17,504 : INFO : merging changes from 5069 documents into a model of 163069 documents\n",
            "2023-11-29 09:33:18,086 : INFO : topic #5 (0.010): 0.034*\"name\" + 0.032*\"noun\" + 0.030*\"nouns\" + 0.028*\"names\" + 0.024*\"word\" + 0.018*\"used\" + 0.016*\"words\" + 0.014*\"proper\" + 0.014*\"common\" + 0.014*\"article\"\n",
            "2023-11-29 09:33:18,090 : INFO : topic #70 (0.010): 0.015*\"book\" + 0.014*\"school\" + 0.013*\"work\" + 0.012*\"schools\" + 0.011*\"teacher\" + 0.010*\"grammar\" + 0.009*\"teachers\" + 0.009*\"exercises\" + 0.009*\"reading\" + 0.009*\"teaching\"\n",
            "2023-11-29 09:33:18,093 : INFO : topic #83 (0.010): 0.022*\"play\" + 0.020*\"plays\" + 0.014*\"shakespeare\" + 0.014*\"drama\" + 0.012*\"stage\" + 0.008*\"tragedy\" + 0.008*\"dramatic\" + 0.007*\"scene\" + 0.007*\"comedy\" + 0.006*\"first\"\n",
            "2023-11-29 09:33:18,095 : INFO : topic #75 (0.010): 0.015*\"words\" + 0.014*\"word\" + 0.013*\"sentence\" + 0.012*\"sentences\" + 0.011*\"subject\" + 0.010*\"tell\" + 0.010*\"write\" + 0.010*\"following\" + 0.008*\"lesson\" + 0.007*\"predicate\"\n",
            "2023-11-29 09:33:18,098 : INFO : topic #54 (0.010): 0.024*\"century\" + 0.016*\"literature\" + 0.016*\"english\" + 0.013*\"england\" + 0.013*\"french\" + 0.013*\"latin\" + 0.009*\"period\" + 0.009*\"time\" + 0.009*\"first\" + 0.008*\"france\"\n",
            "2023-11-29 09:33:18,118 : INFO : topic diff=0.343058, rho=0.107499\n",
            "2023-11-29 09:33:21,231 : INFO : -9.412 per-word bound, 681.0 perplexity estimate based on a held-out corpus of 1069 documents with 138032 words\n",
            "2023-11-29 09:33:21,379 : INFO : LdaMulticore lifecycle event {'msg': 'trained LdaMulticore<num_terms=100000, num_topics=100, decay=0.5, chunksize=2000> in 1122.66s', 'datetime': '2023-11-29T09:33:21.379134', 'gensim': '4.3.2', 'python': '3.12.0 (main, Nov  7 2023, 18:55:06) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.0-x86_64-i386-64bit', 'event': 'created'}\n",
            "2023-11-29 09:33:21,380 : INFO : LdaState lifecycle event {'fname_or_handle': '/Users/ryanheuser/ppa_data/solrcorpus/data.gensim.lda_model.ntopic=100.pkl.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-29T09:33:21.380032', 'gensim': '4.3.2', 'python': '3.12.0 (main, Nov  7 2023, 18:55:06) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.0-x86_64-i386-64bit', 'event': 'saving'}\n",
            "2023-11-29 09:33:21,410 : INFO : saved /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.lda_model.ntopic=100.pkl.state\n",
            "2023-11-29 09:33:21,488 : INFO : LdaMulticore lifecycle event {'fname_or_handle': '/Users/ryanheuser/ppa_data/solrcorpus/data.gensim.lda_model.ntopic=100.pkl', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2023-11-29T09:33:21.488376', 'gensim': '4.3.2', 'python': '3.12.0 (main, Nov  7 2023, 18:55:06) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.0-x86_64-i386-64bit', 'event': 'saving'}\n",
            "2023-11-29 09:33:21,489 : INFO : storing np array 'expElogbeta' to /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.lda_model.ntopic=100.pkl.expElogbeta.npy\n",
            "2023-11-29 09:33:21,503 : INFO : not storing attribute state\n",
            "2023-11-29 09:33:21,503 : INFO : not storing attribute dispatcher\n",
            "2023-11-29 09:33:21,504 : INFO : not storing attribute id2word\n",
            "2023-11-29 09:33:21,511 : INFO : saved /Users/ryanheuser/ppa_data/solrcorpus/data.gensim.lda_model.ntopic=100.pkl\n"
          ]
        }
      ],
      "source": [
        "lda_model = topic_model(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jExj9SYRRyE",
        "outputId": "98ab31fc-8248-4c77-a4f0-53b236c60289"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 09:34:27,384 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
            "2023-11-29 09:34:27,489 : INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
            "2023-11-29 09:34:27,594 : INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
            "2023-11-29 09:34:27,699 : INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
            "2023-11-29 09:34:27,805 : INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
            "2023-11-29 09:34:27,913 : INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
            "2023-11-29 09:34:28,010 : INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
            "2023-11-29 09:34:28,125 : INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
            "2023-11-29 09:34:28,238 : INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
            "2023-11-29 09:34:28,337 : INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
            "2023-11-29 09:34:28,451 : INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
            "2023-11-29 09:34:28,552 : INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
            "2023-11-29 09:34:28,678 : INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
            "2023-11-29 09:34:28,792 : INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
            "2023-11-29 09:34:28,900 : INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
            "2023-11-29 09:34:29,012 : INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
            "2023-11-29 09:34:29,133 : INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
            "2023-11-29 09:34:29,263 : INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
            "2023-11-29 09:34:29,364 : INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
            "2023-11-29 09:34:29,473 : INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
            "2023-11-29 09:34:29,578 : INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
            "2023-11-29 09:34:29,690 : INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
            "2023-11-29 09:34:29,795 : INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
            "2023-11-29 09:34:29,932 : INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
            "2023-11-29 09:34:30,039 : INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
            "2023-11-29 09:34:30,153 : INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
            "2023-11-29 09:34:30,280 : INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
            "2023-11-29 09:34:30,392 : INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
            "2023-11-29 09:34:30,520 : INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
            "2023-11-29 09:34:30,635 : INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
            "2023-11-29 09:34:30,747 : INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
            "2023-11-29 09:34:30,865 : INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
            "2023-11-29 09:34:30,993 : INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
            "2023-11-29 09:34:31,100 : INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
            "2023-11-29 09:34:31,223 : INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
            "2023-11-29 09:34:31,343 : INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
            "2023-11-29 09:34:31,446 : INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
            "2023-11-29 09:34:31,548 : INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
            "2023-11-29 09:34:31,663 : INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
            "2023-11-29 09:34:31,783 : INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
            "2023-11-29 09:34:31,885 : INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
            "2023-11-29 09:34:31,998 : INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
            "2023-11-29 09:34:32,107 : INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
            "2023-11-29 09:34:32,220 : INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
            "2023-11-29 09:34:32,351 : INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
            "2023-11-29 09:34:32,477 : INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
            "2023-11-29 09:34:32,590 : INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
            "2023-11-29 09:34:32,704 : INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
            "2023-11-29 09:34:32,811 : INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
            "2023-11-29 09:34:32,917 : INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
            "2023-11-29 09:34:33,047 : INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
            "2023-11-29 09:34:33,158 : INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
            "2023-11-29 09:34:33,276 : INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
            "2023-11-29 09:34:33,419 : INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
            "2023-11-29 09:34:33,534 : INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
            "2023-11-29 09:34:33,640 : INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
            "2023-11-29 09:34:33,754 : INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
            "2023-11-29 09:34:33,874 : INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
            "2023-11-29 09:34:33,997 : INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
            "2023-11-29 09:34:34,116 : INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
            "2023-11-29 09:34:34,224 : INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
            "2023-11-29 09:34:34,323 : INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
            "2023-11-29 09:34:34,427 : INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
            "2023-11-29 09:34:34,546 : INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
            "2023-11-29 09:34:34,653 : INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
            "2023-11-29 09:34:34,758 : INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
            "2023-11-29 09:34:34,864 : INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
            "2023-11-29 09:34:35,001 : INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
            "2023-11-29 09:34:35,138 : INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
            "2023-11-29 09:34:35,281 : INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
            "2023-11-29 09:34:35,394 : INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
            "2023-11-29 09:34:35,537 : INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
            "2023-11-29 09:34:35,654 : INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
            "2023-11-29 09:34:35,775 : INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
            "2023-11-29 09:34:35,889 : INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
            "2023-11-29 09:34:36,001 : INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
            "2023-11-29 09:34:36,111 : INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
            "2023-11-29 09:34:36,261 : INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
            "2023-11-29 09:34:36,376 : INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
            "2023-11-29 09:34:36,504 : INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
            "2023-11-29 09:34:36,620 : INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
            "2023-11-29 09:34:36,748 : INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
            "2023-11-29 09:34:36,865 : INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
            "2023-11-29 09:34:36,973 : INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
            "2023-11-29 09:34:37,090 : INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
            "2023-11-29 09:34:37,197 : INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
            "2023-11-29 09:34:37,296 : INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
            "2023-11-29 09:34:37,410 : INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
            "2023-11-29 09:34:37,509 : INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
            "2023-11-29 09:34:37,617 : INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
            "2023-11-29 09:34:37,725 : INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
            "2023-11-29 09:34:37,825 : INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
            "2023-11-29 09:34:37,928 : INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
            "2023-11-29 09:34:38,044 : INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
            "2023-11-29 09:34:38,150 : INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
            "2023-11-29 09:34:38,254 : INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
            "2023-11-29 09:34:38,374 : INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
            "2023-11-29 09:34:38,493 : INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
            "2023-11-29 09:34:38,607 : INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
            "2023-11-29 09:34:38,718 : INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
            "2023-11-29 09:34:38,816 : INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
            "2023-11-29 09:34:38,926 : INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
            "2023-11-29 09:34:39,027 : INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
            "2023-11-29 09:34:39,156 : INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
            "2023-11-29 09:34:39,255 : INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
            "2023-11-29 09:34:39,369 : INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
            "2023-11-29 09:34:39,494 : INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
            "2023-11-29 09:34:39,608 : INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
            "2023-11-29 09:34:39,711 : INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
            "2023-11-29 09:34:39,821 : INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
            "2023-11-29 09:34:39,941 : INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
            "2023-11-29 09:34:40,044 : INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
            "2023-11-29 09:34:40,153 : INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
            "2023-11-29 09:34:40,267 : INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
            "2023-11-29 09:34:40,382 : INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
            "2023-11-29 09:34:40,477 : INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
            "2023-11-29 09:34:40,579 : INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
            "2023-11-29 09:34:40,689 : INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
            "2023-11-29 09:34:40,794 : INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
            "2023-11-29 09:34:40,902 : INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
            "2023-11-29 09:34:41,000 : INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
            "2023-11-29 09:34:41,105 : INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
            "2023-11-29 09:34:41,223 : INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
            "2023-11-29 09:34:41,349 : INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
            "2023-11-29 09:34:41,463 : INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
            "2023-11-29 09:34:41,574 : INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
            "2023-11-29 09:34:41,678 : INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
            "2023-11-29 09:34:41,783 : INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
            "2023-11-29 09:34:41,894 : INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
            "2023-11-29 09:34:42,010 : INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
            "2023-11-29 09:34:42,126 : INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
            "2023-11-29 09:34:42,233 : INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
            "2023-11-29 09:34:42,361 : INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
            "2023-11-29 09:34:42,483 : INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
            "2023-11-29 09:34:42,592 : INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
            "2023-11-29 09:34:42,718 : INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
            "2023-11-29 09:34:42,839 : INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
            "2023-11-29 09:34:42,944 : INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
            "2023-11-29 09:34:43,040 : INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
            "2023-11-29 09:34:43,171 : INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
            "2023-11-29 09:34:43,280 : INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
            "2023-11-29 09:34:43,399 : INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
            "2023-11-29 09:34:43,496 : INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
            "2023-11-29 09:34:43,624 : INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
            "2023-11-29 09:34:43,742 : INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
            "2023-11-29 09:34:43,861 : INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
            "2023-11-29 09:34:43,963 : INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
            "2023-11-29 09:34:44,076 : INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
            "2023-11-29 09:34:44,202 : INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
            "2023-11-29 09:34:44,324 : INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
            "2023-11-29 09:34:44,435 : INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
            "2023-11-29 09:34:44,543 : INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
            "2023-11-29 09:34:44,657 : INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
            "2023-11-29 09:34:44,778 : INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
            "2023-11-29 09:34:44,884 : INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
            "2023-11-29 09:34:44,999 : INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
            "2023-11-29 09:34:45,108 : INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
            "2023-11-29 09:34:45,236 : INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
            "2023-11-29 09:34:45,355 : INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
            "2023-11-29 09:34:45,471 : INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
            "2023-11-29 09:34:45,592 : INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
            "2023-11-29 09:34:45,723 : INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
            "2023-11-29 09:34:45,830 : INFO : CorpusAccumulator accumulated stats from 163000 documents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg. Coherence Score: -2.9346454377173643\n"
          ]
        }
      ],
      "source": [
        "topics_coherences = lda_model.top_topics(corpus, topn=20)\n",
        "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
        "print('Avg. Coherence Score:', avg_coherence_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCJwa9dnjvDx"
      },
      "source": [
        "Topic coherence is a complex topic in its own and it can be used to measure the\n",
        "quality of topic models to some extent. Typically, a set of statements is said to be\n",
        "coherent if they support each other. Topic models are unsupervised learning based\n",
        "models that are trained on unstructured text data, making it difficult to measure the\n",
        "quality of outputs.\n",
        "\n",
        "Refer to Text Analytics with Python 2nd Edition for more detail on this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lda_model.num_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azEdB08qRX4z",
        "outputId": "4ff567d9-98da-4d3b-ba17-35777bbbf953"
      },
      "outputs": [],
      "source": [
        "def get_topic_terms(lda_model, topn=100):\n",
        "    return {\n",
        "        topicid:', '.join([dictionary[i] for i,j in lda_model.get_topic_terms(topicid,topn=topn)])\n",
        "        for topicid in range(lda_model.num_topics)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'every, would, much, great, upon, without, rules, art, must, manner, ought, well, nature, language, many, though, time, proper, general, genius, indeed, read, others, make, therefore, even, good, cannot, first, knowledge, different, attention, give, part, propriety, either, study, yet, among, fame, made, public, necessary, order, however, kind, learned, use, several, composition, writing, regard, method, perhaps, point, men, particular, far, often, able, natural, never, whether, might, writers, learning, reason, find, persons, speak, found, man, degree, way, advantage, words, little, variety, view, want, could, ever, long, less, whole, best, reading, thing, useful, always, another, agreeable, means, capable, greatest, parts, former, true, care, nothing',\n",
              " 1: 'first, greek, ancient, learning, man, scaliger, says, son, among, latin, much, account, god, bentley, combe, age, death, latter, though, author, would, editions, also, planus, writers, part, tardus, called, velox, great, find, upon, whose, several, made, like, two, odin, truth, many, cretic, times, scholars, well, original, antiquity, use, irish, vol, given, written, mentioned, published, see, former, men, stephens, seems, known, second, authors, father, turnebus, dissertation, perhaps, poets, according, druids, vossius, bard, name, book, life, odes, however, soul, muretus, victorius, pitt, remarks, verse, casaubon, celebrated, epistles, andronicus, chap, translation, history, elegant, equal, ought, poetica, century, divine, univ, erudition, kind, critics, never, might',\n",
              " 2: \"les, que, des, qui, dans, est, par, une, plus, pour, pas, son, che, vous, lui, comme, del, nous, sur, tout, mais, ses, los, bien, avec, mon, aux, c'est, sans, elle, por, vers, fait, ces, qu'il, cette, d'un, sont, leur, nos, non, dont, deux, tous, paris, moi, ils, las, con, même, french, vie, una, bon, peut, n'est, vos, faire, ont, d'une, dit, point, mots, quand, mes, langue, etc, dire, qu'on, temps, mot, soit, sous, della, notre, madrid, grand, leurs, aussi, peu, mal, font, donne, jour, rien, faut, mort, car, monde, trop, roi, n'a, été, quant, encore, homme, mas, entre, autres, toujours\",\n",
              " 3: 'sect, body, air, muscles, breathing, part, right, position, fig, chest, lungs, breath, left, movements, back, movement, head, action, side, exercise, hand, arm, arms, use, larynx, forward, lower, apparatus, front, upon, two, chap, first, hands, muscular, time, diaphragm, exercises, vocal, point, cords, fingers, iii, respiration, upper, full, nerve, shoulders, place, curve, straight, nerves, pressure, weight, without, foot, control, times, made, free, organs, tension, move, motion, keep, voice, practice, draw, must, little, physical, slowly, nervous, possible, parts, strength, take, motor, second, act, erect, abdominal, repeat, sides, direction, condition, ribs, leg, horizontal, effort, thus, inspiration, also, muscle, curves, distance, feet, power, abdomen, tube',\n",
              " 4: 'english, latin, languages, french, german, language, words, old, forms, form, pronunciation, etc, also, greek, modern, phonetic, dialects, word, dialect, sound, used, different, like, sounds, spoken, would, still, gothic, speech, thus, see, change, alphabet, dutch, many, written, probably, use, two, common, first, teutonic, italian, vowel, changes, origin, however, root, find, original, spanish, seems, present, sanskrit, germanic, european, family, cases, persian, new, found, scotch, saxon, often, though, must, case, people, generally, even, long, meaning, indo, welsh, roots, germ, compare, later, irish, vowels, phonetics, place, high, following, called, well, southern, middle, derived, northern, pronounced, originally, occurs, similar, become, much, hence, represented, certain, low',\n",
              " 5: 'name, noun, nouns, names, word, used, words, proper, common, article, called, number, man, person, case, thing, things, many, class, book, particular, adjectives, kind, adjective, two, john, use, say, means, persons, object, pronoun, capital, objects, articles, boy, possessive, singular, classes, thus, meaning, denote, speech, begin, following, mean, letter, also, every, place, denotes, first, three, different, general, spoken, indefinite, capitals, definite, pronouns, rule, give, abstract, individual, write, without, quality, applied, form, james, sense, good, men, etc, instead, belongs, call, either, city, individuals, kinds, another, substantive, letters, example, parts, placed, plural, sometimes, distinguish, second, house, examples, william, places, express, collective, horse, qualities, titles',\n",
              " 6: 'xix, xvii, xvi, xxii, xviii, xxi, xxiii, xxiv, xxv, xiv, xiii, xii, xxvii, xxvi, xxx, xxix, xxviii, xxxi, xxxiv, standard, xxxii, xxxvii, power, xxxv, screw, xxxiii, xxxvi, xxxviii, webster, vii, xxxix, xli, xliv, xlv, xliii, viii, international, gives, man, xlii, xlvii, speech, creator, xlvi, continued, key, xlviii, xlix, peculiar, words, page, iii, faculty, many, purposes, forts, extent, liv, pervert, letter, tas, fort, give, uses, excellent, namely, belonging, pronunciation, variable, etc, respect, agrees, chart, lii, greatest, bestowed, kind, use, part, overestimated, table, commencing, nickel, writing, general, meridian, sound, beneficent, second, screws, alas, implied, worst, confined, first, inglis, often, letters, review, member',\n",
              " 7: \"homer, translation, iliad, like, less, much, even, least, original, homer's, passage, translator, found, yet, homeric, cowper, many, arnold, two, great, never, poet, troy, though, pope, lines, version, well, man, head, compared, better, odyssey, thus, genius, pope's, could, far, simplicity, without, newman, words, good, best, gods, shall, perhaps, wise, take, famous, sense, last, achilles, translating, degree, translated, would, part, also, time, men, little, worst, still, know, worse, half, another, literal, former, must, ever, hector, greek, true, cannot, character, translations, country, poetry, says, virgil, among, dante, mere, poets, comparison, place, indeed, old, plain, things, whole, manner, dead, say, high, work, effect, dunciad\",\n",
              " 8: \"like, night, day, eyes, old, little, head, light, sea, wind, round, long, o'er, green, white, hand, face, away, air, water, high, upon, dark, bright, red, see, sun, way, blue, came, come, deep, sweet, back, still, along, saw, fair, heart, ground, wild, spring, feet, sky, black, lay, stood, far, cold, fire, every, eye, great, rose, side, around, make, could, hill, would, door, look, left, tree, made, gold, seen, soft, hair, trees, morning, thus, sleep, till, wood, river, golden, hear, last, yet, low, bed, half, leaves, rest, full, behind, young, dead, arms, soon, stream, field, moon, near, beneath, flowers, said, shore, winds\",\n",
              " 9: \"lesson, master, class, grammar, memory, first, part, time, let, made, lily, book, cushing, diagrams, short, method, exercises, whistle, small, words, taken, till, taught, thus, well, compilation, written, portion, work, year, authors, many, grammarian, letter, making, says, every, lyon, repeat, according, several, committed, take, definitions, use, latin, gram, half, plan, second, age, word, scheme, less, common, murray, best, laureat, also, four, scholar, neither, proceeds, others, ever, new, next, number, lily's, learning, able, learners, difficult, yet, books, padua, apologize, original, circ, teacher, gone, english, nature, found, language, kept, man, copied, understanding, upon, third, elude, study, alternately, schools, mode, murray's, omitting, grammatical, could\",\n",
              " 10: \"wordsworth, coleridge, byron, shelley, life, browning, poetry, tennyson, coleridge's, upon, new, mind, like, wordsworth's, nature, keats, thought, criticism, southey, yet, language, science, first, spirit, time, far, poets, without, movement, mariner, imagination, byron's, thus, burns, true, philosophy, man, even, power, long, poet, poems, growth, psychical, genius, philosophic, though, logic, lyrical, light, beauty, subjects, generally, mere, men, form, christabel, abstract, see, found, thoughts, become, least, less, idealism, old, milton, last, biographia, judgment, course, world, fine, truth, step, poe, habit, common, many, delightful, longfellow, ballads, childe, let, poetic, often, english, words, faculties, great, ancient, consciousness, would, make, generation, conception, literaria, principles, fancy, made\",\n",
              " 11: 'richter, many, και, nature, like, man, without, every, lxi, yet, first, lvi, lxii, indeed, lix, much, also, light, last, faust, mirabeau, nay, things, ends, lxiv, lviii, little, lxx, state, see, truth, works, lxiii, liii, προς, lvii, living, lxxi, lxvi, lxvii, though, study, streak, lxv, lxxii, imagination, lxviii, lxix, lxxvi, γάρ, often, lxxx, lxxxv, vision, perhaps, lxxiii, affectation, lxxvii, ever, words, lii, forth, fable, lxxv, part, end, lxxiv, two, lxxix, lxxviii, true, lxxxiii, helena, aristotle, whole, lxxxvi, humorist, lxxxii, vain, sense, lxxxiv, time, great, common, cannot, xci, adapted, must, false, fiction, word, illusion, old, necklace, men, declamations, squares, new, mind, brooding',\n",
              " 12: 'gender, feminine, lady, masculine, female, male, sex, woman, wife, father, mother, man, child, daughter, son, neuter, girl, husband, brother, king, servant, sister, maid, cock, boy, nouns, females, cow, horse, prince, uncle, males, queen, words, said, ladies, aunt, goat, three, women, hen, married, genders, goose, lord, neither, princess, widow, young, cousin, lion, parent, animals, sparrow, house, either, ess, sexes, word, mistress, bride, hero, duchess, common, heroine, countess, duke, marquis, friend, bull, duck, heir, different, nephew, dog, count, niece, friar, master, thus, termination, emperor, witch, poet, two, distinction, mare, buck, number, bachelor, baron, without, moon, lad, madam, life, hunter, priest, jew, ram',\n",
              " 13: \"pope, poets, milton, dryden, spenser, poetry, poet, age, poems, style, poem, jonson, works, satire, writers, horace, milton's, character, paradise, first, ode, ben, verse, prose, lost, poetical, waller, pastoral, genius, sidney, spenser's, pope's, dryden's, elizabethan, addison, sonnets, virgil, queen, wrote, life, shakspeare, fletcher, cowley, english, donne, odes, written, great, verses, author, edst, essay, love, shakespeare, dramatic, satires, imitation, chaucer, ovid, translation, swift, tragedy, among, sir, pindar, death, pieces, two, beaumont, johnson, poesy, wit, comedy, homer, jonson's, heroic, general, seems, birth, time, restoration, plays, later, drayton, faerie, queene, preface, contemporaries, harvey, sidney's, period, lyric, says, moral, tragedies, influence, praise, though, second, drummond\",\n",
              " 14: 'tion, ing, con, com, tions, pro, thing, pre, ver, per, fame, dis, make, ion, fay, ment, upon, ble, book, part, account, time, ers, every, ther, great, thould, whole, many, par, made, sion, able, kind, ture, muff, yet, ness, much, might, several, little, fort, ance, ter, give, ters, ence, taken, fore, fee, read, ous, particularly, given, ments, vnd, books, manner, particular, new, another, ties, added, cafe, author, sub, two, would, use, found, ent, persons, ions, therefore, ten, original, three, general, appear, proper, parts, uni, hath, things, take, reader, age, tive, ought, wherein, shall, could, person, men, vers, rea, first, fubje, pieces',\n",
              " 15: 'god, earth, life, world, heaven, man, death, soul, love, heart, like, men, day, whose, shall, yet, nature, great, would, ever, divine, light, still, upon, song, glory, spirit, power, praise, gods, fire, forth, let, sacred, joy, holy, human, things, beauty, peace, every, land, thus, mighty, long, eternal, deep, never, first, faith, hell, sun, night, lord, fate, sorrow, stars, angels, away, good, true, king, sea, pride, heavenly, mortal, hope, made, fear, mind, time, glorious, vain, truth, must, thoughts, wisdom, many, thy, hearts, days, even, last, old, thought, darkness, free, fair, come, sweet, eyes, war, see, blood, new, dead, high, happy, voice, born',\n",
              " 16: \"metaphor, mid, figures, þæt, simile, hyperbole, ond, swa, allegory, personification, tropes, apostrophe, death, trope, irony, soliloquy, heo, hit, figure, last, antithesis, synecdoche, love, wars, page, speech, metonymy, life, vision, richter, climax, hercules, far, wit, biography, beauty, first, godes, funeral, miser, long, metaphors, man, indeed, seem, gif, heora, next, well, three, doo, battle, works, marco, www, saying, examples, however, cato's, hamlet's, quare, part, medea, ealle, zour, edipus, lamenting, another, bozzaris, liti, grave, rhetorical, blou, londini, hire, studley, bombast, waterloo, thing, mode, various, like, god, exercise, satan, name, sentence, douglas, except, jasper, doctor, fjr, readers, country, nearly, greater, daz, english, four, clausula\",\n",
              " 17: \"lat, adj, town, name, inst, adv, miles, new, belonging, ibe, greek, heb, market, sax, kind, sub, county, int, two, north, made, hinc, bailey, man's, illa, part, cole, aided, botany, lon, ohg, adi, london, near, obsolete, lately, word, illum, chaucer, quidam, great, place, nice, natural, day, nine, time, spelling, roller, ocean, aid, ibus, goth, mille, old, rhetorica, hic, newly, nick, librum, scott, space, whence, river, puer, borough, non, hence, perf, mea, eye, cbaucer, thing, city, fuerit, vitam, grad, immediately, noise, eight, hominum, point, comp, quatuor, horse, tria, come, land, neck, fate, meum, nox, medio, degree, illis, altera, aiding, sermo, fere, narrow\",\n",
              " 18: \"fame, ibid, thle, tile, ill, tie, tle, ion, lie, thie, thi, ith, con, of', found, arc, ihe, cafe, fay, inl, end, fee, tlhe, tha, ther, pro, ing, hich, ind, andl, ted, anld, fir, tihe, time, thc, andi, like, great, many, two, ard, 'the, anid, ire, yet, molt, hat, ere, the', son, thr, ini, upon, fiom, tht, ani, ave, ich, others, ame, fort, ome, ofthe, lit, thl, ile, thort, ail, kind, ite, letter, little, age, anl, part, les, thre, ant, t'he, also, ore, made, well, lame, int, thei, fromn, till, let, iin, every, aid, ithe, ors, mult, ation, 'and, tion, ord\",\n",
              " 19: \"eng, sea, see, used, water, name, lat, state, called, part, small, act, kind, genus, also, plant, two, form, species, body, pref, plants, adv, fish, law, person, manner, applied, term, place, ing, iron, order, shak, large, par, sax, pertaining, made, cut, like, ship, animal, land, common, adj, quality, belonging, parts, seed, wood, tree, given, acid, air, make, skin, stone, thin, thing, family, animals, another, surface, vessel, fruit, piece, without, bot, metal, food, colour, found, oil, instrument, tion, one's, substance, pret, add, away, consisting, formed, sometimes, dis, seal, bodies, rare, containing, four, scotch, leaves, salt, coal, heat, shakesp, hence, pine, white, word\",\n",
              " 20: 'horn, sec, ballade, judgments, image, visual, estimate, distraction, salle, syllogism, auditory, idg, enthymem, beves, whyte, personal, pues, conclusion, count, minor, substitutions, hadde, course, reading, contingent, major, induction, villon, sqq, rate, somers, triolet, enthymeme, make, sense, really, fallacy, premiss, see, alsace, syll, confounds, ultra, rondel, two, yllables, alma, lorraine, words, come, universal, development, logue, short, hundredth, like, matter, crafts, another, ferrers, indogermanic, process, certes, veneris, importance, divisor, dooth, sometimes, great, spak, boasting, gestis, enough, dismissing, word, also, exaggerated, toga, horned, device, particular, poetarum, upon, troilus, tenues, maur, nolde, sede, goon, syllabis, adduces, villanelle, pronou, man, fallacies, truism, genu, something, though, dignum',\n",
              " 21: \"poetry, poems, poem, english, chapter, epic, literature, century, history, style, chaucer, romance, poets, romances, early, iii, prose, character, literary, criticism, historical, later, general, french, period, poetical, part, works, poet, work, narrative, critical, ballads, modern, writers, first, metrical, ballad, verse, page, romantic, middle, beowulf, heroic, popular, influence, subject, age, vii, chaucer's, story, specimens, chivalry, lyric, earlier, tales, introduction, section, form, two, old, also, translation, great, songs, origin, passages, language, critics, though, religious, author, remarks, date, essays, cynewulf, original, writings, fiction, dramatic, vol, written, homer, viii, legends, eighteenth, found, translations, present, tale, contents, times, fragment, among, legend, poetic, fifteenth, novels, pieces, minor\",\n",
              " 22: \"inf, obl, true, dst, much, yet, say, even, man, iron, lessing, less, whole, many, every, must, obj, style, manner, taste, let, indeed, richter's, pen, little, qualities, genuine, literature, though, might, pure, forth, without, nose, nay, strange, nature, life, known, orbit, things, forms, new, works, art, word, would, still, among, language, light, except, good, neither, peculiar, nearly, age, writers, spirit, form, natural, culture, last, like, humor, richter, point, humour, similar, often, chiefly, great, writing, men, sponge, poet, parts, old, mind, perhaps, fashion, least, transparent, first, best, fifths, completely, merely, harmonics, alive, general, work, show, attained, either, bad, french, gracefulness, laws, wrong\",\n",
              " 23: 'ane, baldwin, bot, men, þat, law, davis, johnson, abba, lang, man, rose, owen, thair, yow, payne, alle, herte, white, dilly, deth, fro, grene, hym, davies, nocht, nichols, printed, evans, ainm, hem, upon, hit, wilkie, english, bew, works, king, piers, becket, nicol, dally, fox, volume, poet, thame, robson, conant, algebra, love, leigh, samuel, twa, sons, robinson, murray, newbery, mak, crowder, dodsley, biographical, cott, calender, lowndes, long, fals, longman, cadell, bathurst, strahan, minot, full, gude, day, oper, critical, spirit, sic, alex, also, abab, sotheby, bowen, life, syne, lyne, forth, argumentum, clene, wele, age, gold, hire, ridley, quhen, thay, first, thus, buckland, gif',\n",
              " 24: 'sicut, would, book, carmen, regis, far, concert, church, potest, recital, thay, though, facta, much, morhof, set, concerts, commonly, thys, populi, garland, subject, illa, erigena, better, persius, day, eos, locum, bede, nomine, hur, ordo, since, great, itaque, domini, habent, usus, order, ther, suum, first, many, measure, best, quint, use, received, jesuit, finnish, readily, verbo, generally, indeed, make, fit, romani, wyth, lector, train, leges, hymn, method, time, quum, pianoforte, see, veterum, pueri, fiunt, livy, pyro, part, art, mare, schal, communi, parma, harum, oratio, recte, could, tone, viribus, inquit, dixit, illis, partem, words, writers, factum, ver, dedit, might, mea, others, let, corpore, good',\n",
              " 25: \"hine, mari, bors, pam, fulle, occ, ben, lil, also, old, wvas, king, cons, two, tlhat, ponne, sem, hij, upon, ofr, pil, 'i', see, nance, blondel, find, mani, zee, marc, thnt, man, thus, vre, sen, ther, son, rai, time, pic, kay, winkle, french, ravens, dios, seyd, first, ashmole, thf, bodl, much, many, zon, called, day, said, rith, chester, burly, ric, tof, seynt, sir, would, ree, come, umbrian, men, castra, without, gos, woll, twenti, deu, wið, bij, este, made, ged, know, found, shi, buffoonery, thin, bonne, alle, oscan, atr, syr, thut, tli, fol, wvould, monody, thoug, sam, mette, des, three, sou, nul\",\n",
              " 26: \"first, l'avare, marriage, vienna, spanish, satire, well, see, school, paris, two, louis, dante's, like, paul, ewald, vol, also, name, whose, rabelais, frau, years, dumas, later, laura, parody, roxburghe, washburn, doubt, aeschylus, geben, century, known, place, master, birthplace, decades, professor, study, ibsen, english, lambinus, work, herrn, four, roumanian, trevelyan, many, classical, munich, hartley, however, note, long, distichs, dein, polly, though, treatment, krieg, curme, sagt, great, copenhagen, abbé, diametrically, cartes, poems, thus, would, dealer, echoes, elmer, commedia, author, grotesque, connection, seems, burlesque, titchener, vergil's, french, bassanio, francaise, bayle, literary, wortes, hopkins, jusserand, mention, wife, gleich, bloomfield, dagegen, jahr, michel, literature, probably, perhaps\",\n",
              " 27: \"4th, verses, 6th, accent, pause, syllables, 5th, 7th, last, verse, syllable, dryd, wall, two, seat, save, word, ought, made, heroick, 8th, accented, second, happens, whose, words, nothing, like, first, number, next, cowl, following, example, 2nd, sense, 3rd, thus, penn, secondary, double, conclude, kellogg, must, take, reed, amb, reverse, tho', 1st, observed, poems, require, equal, rhyme, thy, 9th, disagreeable, others, triplet, accents, examples, foregoing, fame, things, always, alonzo, poem, lyrick, many, reason, worth, rules, upon, great, us'd, blac, anim, use, see, certain, day, true, end, poetry, ånd, frequent, fault, sth, a'tion, due, new, observation, burlesque, measure, taught, preceding, either, thể, seen\",\n",
              " 28: \"poetry, prose, verse, poets, poet, poetic, form, lyric, art, imagination, poetical, expression, metre, nature, beauty, rhythm, poem, must, feeling, thought, sense, poet's, language, quality, poems, element, true, diction, first, matter, forms, imaginative, words, like, lyrical, different, dramatic, emotion, great, english, free, passion, yet, life, even, style, two, less, upon, drama, character, metrical, criticism, natural, artistic, versification, movement, elements, modern, high, always, certain, new, though, general, imitation, literature, order, would, time, genius, song, manner, much, rhyme, also, means, best, beautiful, subject, kinds, creative, without, substance, kind, theory, truth, spirit, use, critic, essential, find, essence, something, harmony, lyrics, strophes, mere, distinction, power\",\n",
              " 29: \"man, good, make, know, upon, shall, without, well, would, sir, said, mind, life, think, reason, every, take, men, must, right, give, duty, happiness, first, friend, power, never, say, hope, nothing, pleasure, let, done, says, sense, great, much, yet, better, dear, world, cannot, wrong, johnson, health, could, put, truth, things, thus, true, come, time, sure, place, ought, therefore, happy, gentleman, others, whether, answer, lord, i'll, an', cause, tell, even, makes, fine, means, pain, opinion, yes, thing, another, hear, ever, virtue, heart, though, often, find, fellow, believe, want, pride, question, order, little, consider, like, young, less, desire, body, way, state, truly, able\",\n",
              " 30: 'music, art, musical, singing, song, beauty, must, even, would, ear, expression, nature, read, voice, like, many, melody, poet, sense, reading, feeling, words, life, without, well, upon, work, beautiful, poetry, every, tone, often, true, artist, much, voices, pleasure, time, eye, emotion, thought, form, spirit, singer, poem, sing, natural, also, find, effect, less, first, yet, fact, certain, harmony, say, great, almost, perhaps, power, make, reader, soul, always, best, never, fine, hear, instrument, painting, emotional, whole, new, good, human, notes, high, feel, charm, though, could, cannot, far, perfect, artistic, rather, still, however, people, nothing, mind, made, singers, way, meaning, musician, tones, might, quality',\n",
              " 31: 'john, william, rev, esq, thomas, sir, george, mrs, james, charles, henry, robert, page, english, iii, new, london, miss, lord, edward, life, song, smith, hon, richard, poems, professor, scott, walter, samuel, history, american, college, england, francis, vii, mary, old, literature, death, joseph, chapter, johnson, taylor, book, university, alexander, poetry, arthur, poet, alfred, lady, moore, viii, shakespeare, tennyson, browning, hall, edmund, campbell, brown, young, byron, gray, letters, prof, extract, arnold, wilson, street, oxford, york, oliver, cambridge, herbert, ode, david, longfellow, lowell, elizabeth, daniel, school, matthew, louis, books, contents, hill, edwin, modern, notes, green, hamilton, part, jones, lewis, company, emerson, wordsworth, russell, burns',\n",
              " 32: \"university, professor, new, york, english, library, district, book, college, act, books, company, states, united, year, edition, boston, office, published, author, congress, harvard, street, entered, use, according, chicago, press, publishers, california, copies, school, paper, philadelphia, revised, court, work, first, columbia, cambridge, printed, academy, entitled, also, authors, public, thanks, clerk's, language, department, greek, yale, schools, american, maps, permission, principal, learning, suggestions, pennsylvania, encouragement, series, charts, following, latin, grammar, southern, city, etc, state, proprietors, volume, indebted, mentioned, due, read, times, dictionary, massachusetts, composition, many, printing, july, america, pages, literature, stereotyped, hopkins, kindly, instructor, historical, exercises, copyright, valuable, mass, study, messrs, page, studies, enlarged\",\n",
              " 33: 'taste, nature, arts, genius, men, reason, pleasure, beauties, beauty, works, human, criticism, general, pleasures, fine, upon, sense, natural, principles, objects, part, concerning, every, must, nations, different, imagination, ages, manners, standard, others, poetry, among, give, great, founded, yet, beautiful, characters, subject, considered, found, powers, first, authority, true, proper, sentiments, power, mankind, well, therefore, whether, good, far, composition, judgment, common, improved, less, internal, understanding, experience, respect, hence, refined, several, owing, refinement, man, truth, order, homer, though, without, examine, time, shall, rude, painting, degree, receive, object, would, even, discover, discovered, thus, indeed, made, productions, improvement, sensibility, universal, poem, foundation, source, perfection, education, greater',\n",
              " 34: \"day, month, eclogue, eclogues, pass, theocritus, shepherds, keep, classic, high, rest, fifth, meditation, prayer, always, many, lover, order, upon, aeneid, new, georgics, work, primrose, hinders, hills, opera, art, telephone, great, adnominal, ascended, holy, yet, man, time, play'd, lydia, love, must, life, bagdad, would, which', warbling, alexis, spedding, summer's, complaint, good, bion, atrides, alway, two, meaning, come, phoebe, well, let, read, never, unacceptable, canto, best, excellent, dirae, subjeet, thus, pastoral, whose, moon's, woeful, labour, traditionally, bucolic, last, hops, stript, flocks, pipe, women, sicilian, wakes, thing, chamberlayne, angell, made, poet, pass'd, said, twenty, mantuan, deeper, first, several, 'ft, gives, sixth, master, filed\",\n",
              " 35: \"verse, verses, blank, feet, syllables, ear, iambic, measure, lines, two, pause, line, milton, foot, rhyme, poets, metre, numbers, english, elision, versification, sense, first, without, spondee, heroic, caesura, syllable, would, called, pauses, variety, following, prose, harmony, hexameter, place, last, though, use, kind, short, thus, number, even, words, make, must, poet, long, well, poetry, trochee, milton's, fourth, metrical, made, like, rules, used, many, trochaic, therefore, end, much, ode, yet, often, cadence, every, upon, also, dactylic, sometimes, run, third, different, couplet, cannot, measures, iambus, places, lost, found, reader, kinds, example, paradise, examples, frequently, rhime, certain, great, shall, hexameters, second, sixth, metres, especially, composed\",\n",
              " 36: 'vowel, vowels, tongue, back, front, found, consonant, voiced, sound, position, consonants, open, thus, high, low, sounds, words, eth, wide, mixed, mid, stop, voiceless, formed, irish, narrow, english, pronounce, teeth, round, final, lips, often, let, pronunciation, also, forming, breath, glide, close, symbols, written, way, point, ing, two, without, manner, symbol, difference, french, sometimes, pronouncing, tip, lip, touch, nasal, long, rule, rounding, rounded, founding, cut, stops, ezh, many, fault, mentioned, word, much, following, positions, short, like, order, hard, well, rimes, pronounced, syllabic, first, though, breathed, make, see, diphthong, full, elision, dental, always, heard, made, ireland, etc, soft, feat, liquids, part, towards, letter',\n",
              " 37: 'sound, sounds, voice, vowels, mouth, consonants, tongue, vocal, vowel, letters, organs, breath, lips, speech, tone, formed, elements, called, teeth, articulation, position, letter, two, consonant, elementary, pitch, palate, soft, first, simple, heard, produced, form, thus, without, element, different, throat, hard, nasal, air, note, quality, formation, upper, represented, open, pure, like, following, force, long, table, represent, english, combinations, made, utterance, three, short, used, distinct, part, alphabet, aspirate, ear, vibration, nose, also, sharp, lip, flat, mutes, words, articulate, must, lower, pronounce, word, second, make, passage, found, tones, diphthongs, uttered, stress, little, opening, close, give, compound, character, upon, middle, vibrations, aperture, practice, aspirates, larynx',\n",
              " 38: 'government, war, political, law, policy, state, party, british, letter, civil, rights, foreign, peace, states, power, france, general, towards, country, england, politics, nations, united, would, right, great, liberty, military, germany, russia, see, property, american, trade, russian, parties, nation, laws, affairs, church, europe, system, constitution, catholic, could, free, causes, character, upon, german, people, french, union, revolution, justice, administration, land, public, national, america, republic, present, religion, religious, private, league, first, population, treaty, economic, labour, service, effect, two, protestant, authority, time, international, history, defence, world, countries, provinces, democracy, governments, powers, interests, part, controversy, hungary, control, european, poland, irish, rule, social, democratic, situation, duty, also',\n",
              " 39: \"greek, roman, latin, history, ancient, hebrew, greeks, eloquence, french, romans, greece, modern, poetry, philosophy, rome, old, origin, christian, testament, english, scripture, writers, first, bible, among, grecian, italian, church, page, nations, writing, cicero, writings, books, part, progress, arabic, religion, sacred, jews, scriptures, jewish, derived, book, empire, hebrews, tragedy, treatise, natural, antiquities, german, chronology, europe, language, hist, parts, geography, law, holy, historical, comedy, egypt, critical, ancients, spanish, persian, italy, syriac, word, great, cicero's, mythology, century, popular, aristotle, prophets, astronomy, logic, character, solomon, tenets, gives, chapter, περί, theology, ecclesiastical, egyptian, christ, demosthenes, antiquity, says, public, others, languages, lipsius, upon, gothic, persians, many, pronunciation\",\n",
              " 40: \"upon, great, yet, much, fame, 'tis, poet, never, fee, like, tho', wit, thus, without, many, well, made, let, would, find, fay, every, think, ever, make, nature, time, makes, true, world, kind, men, self, give, poets, though, thought, noble, honour, little, author, virgil, others, nothing, could, gives, must, whose, whole, indeed, far, first, shall, vain, place, sublime, man, art, name, poem, mind, homer, genius, cannot, design, either, therefore, reason, good, judgment, love, perhaps, loft, th', might, equal, part, age, appear, care, take, since, often, happy, light, found, please, even, foul, force, cause, praise, seems, numbers, rife, thro', know, leave, mean, ill\",\n",
              " 41: 'rhythm, two, time, first, number, second, foot, would, three, long, syllables, series, case, feet, also, cases, short, type, verse, line, form, syllable, fact, different, group, length, point, equal, times, rhythmical, intervals, certain, given, thus, however, less, made, four, groups, even, difference, following, results, must, used, thesis, third, found, rhythmic, per, therefore, interval, upon, possible, another, word, effect, forms, pause, seems, accent, total, according, either, regular, pitch, order, duration, trochaic, marked, half, show, sense, table, types, metrical, arsis, greater, five, longer, movement, occur, average, proportion, normal, measure, position, etc, unit, place, iambic, might, differences, see, principle, passage, quite, use, course, could',\n",
              " 42: 'time, years, great, country, upon, people, men, king, father, son, made, old, would, two, man, church, house, death, among, many, could, life, family, long, day, place, war, city, battle, came, first, said, took, name, days, year, left, died, never, army, whose, land, might, became, still, every, three, soon, hundred, young, went, town, well, sent, rome, found, gave, england, last, even, without, though, brought, prince, like, lived, home, whole, called, also, born, little, good, friends, arms, passed, must, part, world, poor, age, till, yet, roman, story, much, brother, power, set, soldiers, side, taken, enemy, emperor, another, lord, away, back, twenty, received',\n",
              " 43: \"songs, song, minstrels, sung, minstrel, among, old, king, time, ancient, year, written, great, first, composed, bards, see, also, harp, sing, says, men, singing, reign, probably, ballads, sang, music, seem, would, verses, name, long, many, poetry, wrote, made, could, afterwards, said, seems, however, poem, like, times, well, ballad, given, preserved, life, came, profession, vol, though, rude, whose, death, called, took, tells, much, poet, popular, part, people, chivalry, kind, king's, story, might, appears, two, compositions, near, upon, till, cædmon, received, yet, another, last, writer, died, order, verse, still, years, told, appear, went, pieces, seen, even, mentioned, author, vid, without, never, passage, dance\",\n",
              " 44: \"god, 12mo, london, grammar, english, 1st, 18mo, boston, 8vo, lord, new, way, christ, york, man, cents, men, jesus, edition, philadelphia, ill, son, first, gram, life, cloth, mass, paul, gospel, john, law, 16mo, lesson, bad, anon, works, true, well, practical, truth, religion, cts, price, 4to, work, must, good, mind, love, mysticism, help, edinburgh, 10th, lessons, elements, christian, providence, composition, writings, joseph, great, whose, jean, name, sin, naturalist, let, like, book, many, would, treatise, jonathan, luther, history, mana, day, little, 4th, murray's, advocate, hierarchy, caleb, without, american, made, pages, god's, joy, david, peter, friedrich, spiritual, 5th, system, hartford, manual, boswell's, write, church\",\n",
              " 45: 'read, net, note, van, veritas, dele, bib, scientia, artes, bottom, pop, trap, phonograph, crown, rod, page, gift, colt, jest, table, sewn, bob, ban, see, bind, kick, add, library, egg, bang, jerk, gag, lin, bond, bran, help, band, gig, fan, crank, crab, flash, nun, lack, lash, dram, mum, drum, worm, fang, gall, bag, rondeau, lak, errata, bud, belt, pack, smoked, ill, jar, hop, jump, brim, comb, lull, letters, sand, rob, fork, hag, sack, dash, catch, sham, pap, gear, jet, michigan, hot, gang, recorder, rap, inn, yard, bunch, axle, gull, lick, puff, lock, kill, lag, blot, bank, wand, jam, plot, attis, buzz',\n",
              " 46: 'der, die, und, von, den, des, das, mit, dem, ist, sich, auf, ein, aus, für, nach, eine, aber, auch, zur, wie, durch, nicht, einer, ich, sie, als, vor, sind, wird, bei, über, man, noch, zum, ihm, oder, mir, nur, skt, einem, einen, ihr, vom, werden, war, ihn, unter, doch, seine, dass, hier, zeit, geist, selbst, sprache, sehr, also, mich, wohl, sein, ags, schon, diesen, zwischen, helmholtz, deutschen, können, hatte, dann, diese, zwei, alles, ballade, welche, mein, ihrer, beiden, weil, recht, leben, gott, alle, ihnen, etc, lässt, wort, immer, stelle, erste, regel, worden, ersten, rune, deren, denen, englischen, seit, zwar, diss',\n",
              " 47: 'edition, text, work, volume, first, many, notes, published, book, made, present, given, original, two, found, also, volumes, printed, editions, much, editor, time, works, years, second, mss, part, several, preface, great, manuscript, books, reader, would, however, author, pages, appeared, added, whole, passages, former, large, though, note, since, publication, copy, account, known, well, written, translation, date, especially, contains, additions, information, mentioned, perhaps, old, number, might, far, last, library, kind, three, history, collection, press, valuable, seems, various, modern, little, general, editors, latter, could, version, texts, considerable, folio, references, later, read, reference, form, english, copies, title, following, without, contained, must, complete, passage, taken, late',\n",
              " 48: 'plural, singular, nouns, ending, form, number, gen, adding, plurals, genitive, words, formed, dat, like, acc, change, sing, numbers, plur, following, men, nom, end, declension, two, man, rule, dative, abl, ends, used, termination, except, add, voc, names, also, preceded, word, many, stave, changing, calf, terminations, general, oxen, sometimes, staves, penny, church, thus, proper, consonant, tooth, generally, sound, deer, foot, vowel, others, foreign, mice, die, makes, sheep, regular, brethren, geese, changed, pence, added, make, mouse, dice, loaf, knife, goose, hard, children, wife, nominative, brother, teeth, feet, fox, ies, leaf, box, foxes, child, compounds, silent, cafes, apostrophe, women, soft, ves, staff, greek, lice',\n",
              " 49: 'obs, term, much, among, even, many, says, still, error, german, word, germans, taste, first, different, certain, perhaps, must, two, often, though, also, yet, writers, men, literature, works, truth, would, rather, like, another, shall, general, things, style, subject, thought, part, every, sometimes, cannot, great, present, degree, objection, enough, found, think, far, given, writer, find, others, nay, character, english, seems, anapæst, capitalized, something, fact, reference, reckoned, taken, man, therefore, sort, view, bad, indeed, rhetoric, could, assertion, definitions, ever, alone, little, instance, thing, art, without, wider, scarcely, seem, understood, people, ignorance, sense, various, less, always, sorts, used, known, nation, see, condition, respect, half',\n",
              " 50: \"reverend, church, hymn, hymns, two, pub, papal, brit, signor, though, iii, montagu, ibid, soph, good, pope, gach, gregory, rome, use, present, vol, pfal, title, cent, see, also, works, orig, vespasian, would, fleury, day, sai, lambert, johnson's, mss, percent, lord, rose, hys, following, paul, part, gallican, dude, lise, tarn, authority, never, father, letters, three, suppl, peter, world, philosopher, cong, amelia, remarked, lyra, thah, coll, collections, yet, whil, written, found, feast, prickly, limited, balaam, bishops, bibl, tise, daniel, province, opon, eccl, given, choice, imperator, sanctorum, blois, bacon, cento, presse, temporal, richer, evesham, without, power, overheard, add, childe, conduct, sala, ancient, franc, sermon\",\n",
              " 51: \"sir, court, king, lord, john, earl, henry, richard, william, street, thomas, duke, edward, london, college, king's, queen, royal, year, right, house, bishop, master, oxford, robert, upon, prince, reign, england, iii, law, parliament, first, james, fours, lane, honourable, church, son, great, elizabeth, surrey, place, see, name, knight, near, made, letter, road, cambridge, afterwards, lords, hall, dedicated, old, francis, anne, norfolk, two, servant, lady, ascham, second, queen's, marriage, leaves, tutor, charles, died, howard, also, death, gentlemen, george, chapel, office, time, philip, mary, late, anno, family, much, park, commons, title, oxon, day, simon, many, honour, john's, council, leicester, trinity, sidney, new, men, chancellor\",\n",
              " 52: 'see, angular, place, post, knave, table, ayr, scurvy, wale, momentum, strait, vessel, also, bronchi, maze, rein, sce, chirp, desert, scurf, whirl, plain, plane, woof, sort, rain, scots, wrench, plait, str, age, boo, fair, grass, mail, bail, force, stake, pale, thwart, quibusdam, fane, vail, warp, tinge, crooked, pledge, abd, plate, love, note, whale, wheel, asp, gait, kind, τοις, sin, rajna, lair, large, glass, rule, wain, luve, reed, raze, grate, knell, seine, rasp, tool, lament, desire, lackey, klenze, horse, bridle, narrow, unmarried, gage, physicist, ipsius, adriana, walking, sale, fau, pain, range, amaze, angle, quake, sunbeams, fishermen, spr, sail, fain, slay, wait, stanch',\n",
              " 53: 'kal, composition, every, never, well, attention, great, age, kai, study, much, elegance, public, style, would, times, language, tov, subject, words, present, proper, writing, thought, taste, beauties, indeed, could, formulas, speak, yet, must, false, opinion, reason, become, science, thoughts, ornaments, ornament, genius, upon, many, even, true, always, kind, requisite, form, often, prevent, think, none, ever, nothing, part, order, manners, author, fails, studied, knowledge, power, engrossed, importance, certain, eloquence, view, polishing, regard, years, time, loose, kara, sire, first, come, rhetoric, either, away, sentiment, forth, grace, arts, manly, prevalent, danger, back, knows, call, wherein, frivolous, except, mahomet, conception, quint, genuine, prosecuted, still, expression',\n",
              " 54: 'century, literature, english, england, french, latin, period, time, first, france, learning, language, years, great, italy, europe, centuries, italian, became, age, modern, early, ages, literary, greek, germany, written, still, classical, new, two, sixteenth, middle, influence, called, wrote, much, made, even, tongue, began, writers, came, history, later, people, ancient, many, country, last, year, poetry, old, roman, norman, fifteenth, renaissance, learned, fourteenth, schools, seventeenth, thirteenth, though, reign, native, school, known, beginning, chaucer, languages, twelfth, long, men, spain, less, revival, scholars, german, books, church, almost, national, rome, conquest, since, end, works, earlier, countries, hundred, reformation, among, took, foreign, letters, progress, classics, form, date, found',\n",
              " 55: 'sentence, sentences, words, chapter, rule, parts, subject, iii, exercises, part, use, section, page, word, examples, used, exercise, compound, speech, adverbs, simple, clauses, rules, phrases, punctuation, syntax, clause, analysis, adjective, adjectives, predicate, conjunctions, construction, verbs, phrase, prepositions, elements, verb, comma, forms, form, adverb, vii, two, nouns, pronouns, parsing, grammatical, interrogative, complex, viii, period, classes, meaning, conjunction, etc, principal, general, definition, classification, noun, composition, questions, order, marks, figures, relation, review, lesson, object, subordinate, preposition, adverbial, example, point, inflection, exclamation, relative, kinds, element, interjections, independent, continued, observations, comparison, different, complete, interrogation, colon, letters, following, pronoun, quotation, first, uses, structure, writing, note, letter, direct',\n",
              " 56: 'work, author, would, subject, many, upon, every, others, without, grammar, public, much, might, however, book, reader, present, opinion, well, far, following, could, must, works, new, though, principles, great, little, yet, therefore, part, found, perhaps, taken, time, even, think, shall, whose, general, hope, made, knowledge, writers, readers, cannot, treatise, first, since, either, language, edition, writer, plan, english, several, system, attention, authors, best, better, necessary, thought, view, useful, generally, authority, give, published, preface, design, merit, given, render, reason, whole, opinions, science, books, lectures, long, common, already, written, success, course, errors, pages, publication, matter, murray, attempt, whatever, examples, adopted, ever, apology, important, arrangement',\n",
              " 57: \"french, des, der, von, english, paris, german, see, notes, und, vol, berlin, leipzig, prof, london, modern, zur, note, die, paul, iii, halle, geschichte, sprache, englische, old, hugo, les, grammatik, meyer, edited, albert, etc, studien, france, goethe, englischen, skeat, goethe's, deutschen, max, oxford, otto, dictionary, karl, française, philologie, hermann, vols, language, sur, grammar, jean, works, van, soc, translation, engl, langue, grundriss, studies, deutsche, middle, wilhelm, early, texts, text, sources, study, ellis, idyllium, friedrich, cook, histoire, victor, dira, roman, bright, par, anglia, vii, metrik, françois, literatur, strassburg, new, society, ludwig, schiller, wright, lang, viii, sievers, 8vo, dieu, für, germany, ihre, richard, history\",\n",
              " 58: 'eyes, motion, anaphora, body, upon, eye, hand, head, countenance, draws, mouth, whole, tone, hands, person, drawn, face, right, fear, voice, velocity, gravity, look, manner, sometimes, time, eyebrows, close, joy, fixed, fork, open, object, aspect, thus, toward, looks, air, forehead, loose, state, without, motions, particle, pendulum, see, opens, tears, grief, mind, anger, back, distance, breast, arms, aversion, another, would, casts, said, sudden, every, modesty, appearance, features, statue, incomparable, lips, like, contrary, forces, little, thing, frame, guernsey, strings, perfect, concealed, cast, violent, much, isaac, soul, half, compact, power, sly, expressed, solitude, present, dici, though, agitated, particles, pain, tuning, use, expresses, general, anxiety',\n",
              " 59: 'language, grammar, letters, words, english, latin, parts, part, languages, speech, tongue, use, rules, letter, syntax, writing, orthography, etymology, called, greek, art, word, sounds, alphabet, treats, different, divided, two, proper, general, principles, many, syllables, lan, four, true, according, made, method, first, spelling, learned, written, particular, sentences, common, speaking, several, properly, express, guage, write, spoken, order, knowledge, make, prosody, terms, every, teaches, names, learn, viz, used, simple, together, much, grammarians, another, teach, right, manner, taught, yet, without, plain, twenty, gram, things, grammatical, therefore, cannot, nature, science, others, form, speak, signs, articulate, well, fame, either, thoughts, various, thing, number, certain, great, way, kinds',\n",
              " 60: 'would, man, little, much, could, well, know, like, good, said, time, never, life, great, must, might, think, say, even, nothing, way, men, old, ever, thing, yet, things, many, without, made, make, upon, better, always, every, day, enough, though, young, long, thought, something, take, perhaps, poor, first, often, rather, see, find, world, quite, let, cannot, two, mind, read, indeed, come, another, still, years, last, says, best, whole, poet, knew, true, work, friends, done, get, people, almost, kind, less, look, nature, friend, shall, matter, told, full, seen, found, put, really, sure, give, far, known, whether, truth, went, came, speak, among, got, mean',\n",
              " 61: 'plato, venice, cicero, rome, tacitus, plutarch, herodotus, like, livy, dionysius, thucydides, demosthenes, lucretius, catullus, terence, pliny, aristotle, xenophon, homer, isocrates, euripides, ovid, seneca, tibullus, hoy, statius, hesiod, florence, sallust, longinus, grandeur, socrates, valerius, sophocles, plautus, aristophanes, cadmus, quintilian, locke, theory, lucan, lucian, boundless, juvenal, fibres, virgil, suetonius, among, greek, phaedrus, character, propertius, passage, spengel, extended, flaccus, coin, milan, curtius, impression, orators, vicomte, paris, solon, use, maximus, though, francais, flourished, weight, pines, sparta, kind, little, lysias, strabo, roman, basle, acanthus, diodorus, brandon, ancient, polybius, horace, two, also, ocean, malebranche, gellius, short, law, anne, day, name, sentimentalism, look, height, life, hatchet, pindar',\n",
              " 62: 'english, study, language, use, work, writing, must, composition, student, good, book, thought, subject, write, first, written, make, expression, literature, much, class, upon, best, way, grammar, course, words, form, time, style, every, well, speech, many, correct, made, given, give, practice, teacher, literary, pupil, matter, forms, writer, possible, say, students, general, oral, would, two, attention, pupils, facts, knowledge, important, part, writers, even, read, books, learn, without, used, present, clear, need, little, order, purpose, usage, value, means, however, find, special, principles, necessary, material, know, method, also, often, school, better, subjects, great, fact, rules, aim, interest, cannot, far, certain, reading, help, rather, done, see',\n",
              " 63: 'first, impression, second, represents, upon, eye, made, also, word, elegy, mind, seeing, hearing, image, every, place, organ, pleasant, external, touching, third, sound, object, great, less, latter, sensible, feeling, pleasure, respect, much, tasting, smelling, ear, pleasures, makes, sense, following, would, gives, senses, organic, nature, painful, hand, feelings, however, dignity, former, see, till, rose, led, difference, two, touch, way, mono, kind, whether, example, perhaps, retina, rhetoric, author, nod, thus, orator, remarkably, caused, without, perceived, must, therefore, clipping, public, yet, power, sublime, given, find, perseus, many, exist, conceive, intellectual, gave, corporeal, knowledge, stone, persuade, appearance, melancholy, ocus, nothing, rule, conceived, degree, objects, persuasion',\n",
              " 64: 'sound, short, long, sounds, heard, broad, like, letter, french, open, pronunciation, letters, represented, vowel, note, italian, met, marked, father, thus, english, unaccented, see, words, nearly, pin, fate, give, obscure, followed, slender, full, fat, close, etc, pull, name, flat, far, error, following, consonant, thin, simple, mark, fall, machine, first, ale, oil, pine, old, move, notation, pronounced, substitute, also, middle, two, find, represents, diphthongal, ooze, call, element, seems, shortened, exercise, england, arm, second, mute, diphthong, slightly, zone, son, called, tin, sounded, intermediate, generally, sin, key, eel, word, true, represent, exactly, bar, sharp, though, ice, style, language, dot, table, vowels, italics, shorter, mate',\n",
              " 65: 'syllable, words, vowel, word, accent, vowels, two, syllables, long, consonant, found, pronounced, like, sound, short, consonants, final, rule, end, first, letter, always, sounds, thus, accented, founded, diphthong, ending, double, except, last, proper, three, single, beginning, sometimes, following, letters, generally, silent, mute, diphthongs, followed, also, pronunciation, sounded, preceded, called, written, many, though, quantity, compound, must, second, ends, exceptions, either, unaccented, preceding, english, pronounce, never, ing, general, rules, latin, without, monosyllables, different, hard, simple, middle, soft, divided, make, used, another, latter, begin, together, begins, third, improper, french, doubled, derived, primitive, former, note, placed, derivative, part, joined, added, marked, every, give, termination, often',\n",
              " 66: 'wil, hav, dat, dhi, wel, speling, hiz, bin, coll, haz, dis, wid, chr, wun, dan, wud, fer, sum, mad, wer, veri, mor, dar, eni, woz, men, ther, sed, man, oxon, red, giv, til, gud, must, sam, hwen, file, god, kan, reform, upon, fire, mai, hwig, devil, bai, hur, shal, alfabet, dere, hire, fonetic, sal, meni, pol, put, ilk, let, dem, leterz, med, stil, hate, sae, gon, onli, hom, thru, iii, nou, cud, thar, lif, pare, end, old, tel, fare, fir, bot, shud, hert, best, mile, tim, leter, rice, rov, aur, bad, wurdz, dun, dine, tui, mater, fere, joe, pore, rope',\n",
              " 67: \"printed, london, edition, vol, book, sir, first, poems, 8vo, new, edited, works, published, author, english, life, vols, chaucer, books, john, second, ballad, notes, volume, thomas, title, old, tale, henry, page, tales, two, copy, price, year, written, series, scottish, 4to, king, folio, cloth, story, part, leaves, poem, translated, three, death, ballads, volumes, lancelot, illustrated, history, library, original, work, george, collection, british, portrait, late, sold, arthur, society, company, oxford, containing, also, william, james, illustrations, knight, stories, third, prologue, ancient, cambridge, date, last, love, four, introduction, letter, reprinted, memoir, etc, song, lord, press, chaucer's, col, songs, crown, complete, lydgate, messrs, museum, early, list\",\n",
              " 68: \"thou, thy, love, thee, shall, let, lord, loved, yet, doth, hath, thine, might, hast, art, heart, come, like, could, time, mine, unto, would, sing, man, though, god, still, good, shalt, live, well, made, give, life, must, make, day, night, take, wilt, name, humour, men, word, tell, much, say, never, fair, fear, last, mind, know, thus, dost, none, sweet, soul, true, die, see, call, light, ever, said, loves, every, things, didst, lie, thyself, i'll, 'tis, way, nay, great, hall, away, many, spirit, truth, first, done, little, hear, far, best, gentle, nature, dare, eyes, find, father, alone, poor, death, keep, long, friend\",\n",
              " 69: 'ter, per, ing, con, ble, man, ful, ment, der, com, ber, let, ver, ate, ness, pre, ant, dis, par, fin, tor, ger, gen, bit, bet, bid, fil, mer, ous, ner, bar, set, pen, mar, ten, cor, bed, ran, cat, pan, gar, fore, car, plu, cal, kin, sin, tal, fit, sel, mon, ler, ham, rid, bat, tur, tin, fer, tan, put, cut, tri, lesson, fat, gal, ling, met, rel, ent, den, ton, har, tre, led, pet, ish, dom, tra, mil, fed, tar, run, pro, lar, cap, two, bal, est, lit, son, red, pur, eat, ser, mat, come, get, hor, sing, tic',\n",
              " 70: 'book, school, work, schools, teacher, grammar, teachers, exercises, reading, teaching, study, english, pupils, education, pupil, lessons, instruction, use, practical, principles, knowledge, method, system, language, course, new, first, well, books, part, children, author, young, elementary, many, public, time, subject, students, taught, training, practice, given, made, composition, class, every, plan, important, student, text, elocution, analysis, designed, years, adapted, also, classes, high, pages, useful, attention, science, college, rules, best, great, exercise, much, manual, found, read, series, present, methods, art, examples, teach, common, upon, valuable, selections, experience, prepared, volume, arithmetic, make, general, learning, colleges, suggestions, learner, used, presented, advanced, rhetoric, aid, lectures, good, thorough',\n",
              " 71: \"sublime, sublimity, great, grandeur, awful, darkness, nature, ideas, mind, grand, objects, yet, much, also, best, though, striking, whose, highest, terrible, every, keller, glory, class, power, epicurus, imagination, manner, men, nay, controversy, together, time, thus, sciences, silence, poet, critical, courthope, obscurity, new, kames, amid, found, germans, idea, far, world, moral, musick, thick, beautiful, view, certain, publica, strikes, instances, superior, authour, mimus, supreme, hieroglyphic, name, aristotelians, oratorio, doubly, infinity, barrington, conception, like, shou'd, unknown, presented, eloquently, rome, shakspeare, novum, deity, might, evident, arises, disorder, good, folgenden, mathematicians, derived, firmament, schiller, fellow, maketh, applied, sentimental, descriptions, trinity, source, ashby, science, exertion, galen, produce\",\n",
              " 72: 'dis, act, signifies, together, prefix, root, con, pro, make, beyond, prefixes, put, take, away, denotes, another, place, press, sub, pre, side, mis, form, upon, com, draw, fore, part, give, back, anti, words, turn, round, opposite, sometimes, thus, meaning, two, signification, apart, means, join, wrong, trans, implies, per, super, ante, way, forms, inter, near, sense, cor, state, suffixes, contra, ness, drive, come, abs, duce, lead, letter, asunder, derived, word, going, denote, ant, forth, origin, like, derivatives, latin, circum, becomes, equal, forward, force, note, deprive, rank, double, within, roots, able, changed, fix, tend, pose, around, suffix, extra, legal, col, without, par, bring',\n",
              " 73: 'part, art, abbot, good, studies, arts, criticism, two, abbey, knowledge, sense, introd, abate, use, man, society, kind, useful, first, discourse, taste, every, authors, rank, philosophy, others, abbots, abb, works, terms, abatement, word, abandon, certain, must, without, genius, name, merit, hundred, nature, solid, german, true, employ, applied, rhetoric, manner, beauties, somewhat, among, well, also, abated, faults, increase, many, sometimes, much, frequently, persons, former, move, lower, understanding, disquisitions, liberal, humane, refined, place, confound, polite, though, public, proper, aims, abandoned, little, propagation, abbreviate, valuable, age, bearing, convent, cavil, exercise, teaches, importance, would, last, independent, real, could, natural, either, always, composition, wealth, shadow, means',\n",
              " 74: 'words, pronunciation, word, spelling, language, sounds, dictionary, would, letters, written, english, orthography, use, found, meaning, thus, many, pronounce, different, every, pronounced, sound, used, first, pronouncing, must, often, given, letter, spell, without, alphabet, syllables, though, correct, like, usage, according, number, dictionaries, find, rule, spelled, simple, common, two, well, difficulty, also, give, writing, made, marks, method, best, much, general, new, generally, sometimes, great, accent, others, reading, vowels, therefore, proper, place, let, rules, way, make, upon, less, always, read, whole, shall, latin, time, french, custom, order, even, adopted, similar, analogy, little, following, ought, form, seems, means, derived, marked, true, ear, frequently, almost, speakers',\n",
              " 75: 'words, word, sentence, sentences, subject, tell, write, following, lesson, predicate, exercise, two, called, see, say, something, boy, make, said, used, children, dog, boys, little, first, teacher, use, birds, john, verb, good, give, made, man, day, many, three, tree, horse, think, και, picture, kind, thought, must, house, object, name, show, story, tells, sun, thing, home, child, read, like, let, complete, saw, come, pupils, trees, question, written, went, bird, would, point, things, express, thus, ask, put, questions, action, water, way, every, horses, time, part, find, flowers, another, school, form, place, came, meaning, know, statement, play, describe, parts, well, done, book, old, verbs',\n",
              " 76: \"vol, review, magazine, turn, turned, foreign, fraser's, edinburgh, german, life, westminster, without, quarterly, contrary, carrier, state, novalis, eichhorn, bevis, writers, travel, preceded, ing, prefixed, see, additional, first, absolute, times, absent, iii, caution, following, kål, spy, abridge, happiest, character, history, ness, abstract, letter, worship, doubled, absurd, yat, abridgment, bias, guy, abortive, abuse, upon, cancel, deny, heeren, memoirs, nos, little, words, carry, many, could, chivalry, act, another, lexicon, improperly, word, quality, mirabilis, acquit, abs, epitome, absolution, less, pay, goethe's, bembo, happy, abrupt, scribble, charge, literature, full, annus, london, goethe, happier, thus, friedrich, procul, spies, art, medicine, ascham, thynne, general, scholemaster, complaynt, abstracted\",\n",
              " 77: 'verse, english, accent, stress, quantity, greek, chap, metre, syllables, rhythm, accents, latin, prosody, poetry, music, metrical, modern, classical, metres, would, language, must, time, measures, long, accentual, versification, prose, even, musical, say, theory, rhythms, hexameter, upon, first, syllable, system, ancient, though, much, measure, two, short, however, ear, quantities, part, hexameters, greeks, scansion, without, quantitative, far, poets, could, lines, harmony, rhythmical, seems, yet, also, great, still, syllabic, natural, certain, line, nature, saintsbury, speech, principle, well, made, whether, fact, different, rule, melody, might, common, old, feet, iii, cannot, find, question, many, accented, perhaps, like, laws, said, almost, variety, every, length, word, effect, think',\n",
              " 78: 'ant, edinburgh, belles, college, lettres, printed, kyng, london, society, curious, rhetoric, fellow, many, blair, row, edward, vol, author, royal, oure, edition, ancient, wes, ful, professor, university, hath, lucius, esq, hugh, wende, strand, iii, dcc, marcus, cambridge, church, high, rev, member, herte, volumes, paternoster, also, late, learned, academy, lond, trinity, col, lectures, mon, god, cadell, aurelius, thou, indebted, oxford, holy, abraham, sone, english, name, londe, editor, alexander, two, syllabus, inscriptions, wolde, honour, lowe, thomas, simpkin, ministers, ded, chaucer, see, longe, near, underestimated, remarks, wys, sold, particularly, engelond, pater, new, strong, wille, three, magdalen, prys, wel, bibl, alle, great, life, ingenious, richardson',\n",
              " 79: 'chapter, style, composition, description, paragraph, general, subject, theme, unity, writing, exposition, narration, narrative, kinds, story, thought, paragraphs, part, iii, definition, argument, writer, material, page, whole, topic, order, view, subjects, details, nature, discourse, outline, point, clearness, introduction, plan, must, reader, parts, themes, analysis, method, criticism, division, arrangement, first, different, topics, qualities, purpose, short, object, statement, principles, sentences, conclusion, two, essay, selection, character, arguments, proposition, form, structure, discussion, coherence, examples, concise, events, brief, clear, strength, argumentation, forms, interest, descriptions, divisions, figures, ideas, historical, descriptive, idea, relation, effect, simple, diffuse, make, specific, amplification, example, written, upon, compositions, explanation, main, summary, beginning, contrast, methods',\n",
              " 80: 'ideas, mind, language, idea, thought, man, things, objects, words, nature, object, express, thoughts, human, without, sense, upon, thing, another, thus, natural, word, power, expression, emotions, every, means, different, meaning, certain, cannot, relation, must, senses, first, signs, form, qualities, imagination, perception, order, use, actions, action, figure, expressed, men, relations, motion, body, called, feelings, make, passion, act, external, whole, also, two, common, others, existence, time, emotion, would, particular, life, passions, terms, eye, speech, represent, sounds, soul, feeling, cause, hence, world, connection, image, desire, part, mental, nothing, used, animals, truth, made, see, images, color, visible, connected, call, manner, reason, think, memory, beings, place',\n",
              " 81: 'long, short, like, sounds, broad, alphabetical, sound, heard, words, pronounced, written, sheridan, bury, people, found, cough, busy, four, laugh, boat, feud, beau, door, foe, sew, though, ought, buy, eau, guard, middle, toe, also, two, often, see, three, boy, rough, guide, prove, blood, shoe, coat, letters, leopard, heart, friend, soup, substitutes, sometimes, viz, aunt, tough, flood, wolf, marked, sieve, following, guilt, key, founded, diphthong, english, chaise, die, doe, move, view, first, names, dough, food, women, buchanan, fool, would, foot, build, bough, deceit, beauty, full, tomb, could, fruit, mete, eye, juice, love, proper, gauge, duple, aught, bawl, quay, heir, oil, hue, vow',\n",
              " 82: 'als, nicht, dass, hat, ist, und, der, sie, die, wie, ich, ein, wir, sein, eine, bei, bis, auch, wenn, haben, man, nur, mit, uns, den, dieser, seiner, diese, oder, felix, eines, sich, dem, nun, sei, war, wurde, noch, tag, seine, hier, das, kann, obit, faust, ganz, habe, gorgias, werden, denn, aus, mehr, aber, muss, erst, mann, des, gewesen, goethe, von, seines, dies, german, kein, doch, nach, gegen, zeit, dichter, oft, bouhours, lassen, tod, dichtung, ohne, etwas, seinen, fiir, flügel, vielleicht, drei, könnte, weit, dazu, protagoras, auf, namen, pere, vgl, sondern, werk, bald, art, jedoch, bat, dieses, einem, diogenes, ihren, durch',\n",
              " 83: \"play, plays, shakespeare, drama, stage, tragedy, dramatic, scene, comedy, first, shakespeare's, two, hamlet, characters, act, theatre, plot, character, shakspere, english, part, story, well, scenes, king, henry, elizabethan, time, actors, much, action, macbeth, later, also, iii, old, history, tragedies, shakspere's, evidence, would, early, new, second, upon, great, place, like, audience, written, however, players, see, hero, three, lear, date, seems, made, tragic, fact, dramas, probably, acted, known, richard, period, parts, dialogue, even, comedies, passages, othello, must, french, titus, whole, source, earlier, says, certain, show, found, dramatists, actor, original, life, far, another, juliet, comic, merchant, end, case, england, work, given, shows, romeo, theatres\",\n",
              " 84: 'english, language, saxon, anglo, history, origin, literature, words, saxons, england, latin, tongue, languages, norman, britain, celtic, british, great, dictionary, first, part, new, modern, old, early, philology, lecture, names, present, conquest, country, tribes, people, french, historical, many, ancient, original, parts, angles, vocabulary, native, teutonic, name, study, danish, alfred, period, introduction, changes, spoken, time, called, ireland, europe, chronicle, northern, also, work, north, philological, gothic, irish, race, elements, known, scotland, danes, chapter, use, upon, among, derived, society, iii, foreign, grammar, mother, speech, knowledge, anglosaxon, middle, terms, works, dialect, various, century, welsh, bede, made, years, german, classical, common, nations, island, general, stock, american, invaders',\n",
              " 85: \"falling, rising, inflection, slide, slides, thus, come, word, time, read, public, found, first, inflections, como, high, circumflex, great, form, frank, heard, part, good, hand, must, following, initiation, proceed, well, often, answer, man, examples, came, even, originally, years, surreptitious, sense, would, saw, arcite, exposed, might, tricks, rather, kind, pope, alas, frequently, conquering, affected, men, palamon, author, merely, handed, taken, quoted, currently, level, question, shall, four, esta, monotone, sale, view, best, presuppose, quien, choice, threatened, naevius, lectures, made, altogether, thought, much, composition, hear, give, music, fame, name, alexander, powerfully, study, greatest, twos, twenty, circulate, light, article, less, out', youth, waken, cannot, addison\",\n",
              " 86: 'non, est, cum, qui, quod, aut, per, sed, quam, sunt, nec, vel, lib, hoc, quid, esse, pro, enim, atque, etiam, dhe, quo, quae, hic, qua, cicero, mihi, says, inter, ego, nunc, neque, tibi, nam, sic, iii, autem, ante, modo, ita, tamen, ille, quia, tam, apud, res, quidem, quis, omnia, sit, nihil, cap, jam, rerum, ubi, quæ, nos, cui, fit, omnes, latin, quibus, fed, quintilian, quem, que, tum, nisi, quoque, ejus, sibi, dum, habet, sine, erat, haec, vero, omnibus, rebus, ipse, magis, deus, quasi, vis, semper, tantum, suit, verba, orat, passage, post, quos, illud, cic, natura, nobis, nomen, igitur, rem, genus',\n",
              " 87: 'new, year, american, states, committee, two, america, president, united, association, business, report, public, city, letter, day, state, members, war, meeting, bill, speech, house, country, years, number, first, national, school, money, board, last, five, three, made, week, time, address, great, would, south, per, england, annual, secretary, hundred, four, office, twenty, british, months, january, york, paper, general, trade, university, government, six, pay, june, price, shall, council, march, west, sent, work, july, mexico, december, capital, month, north, parliament, post, high, society, letters, days, college, october, home, member, upon, congress, washington, union, cents, journal, reform, half, take, class, held, people, question, street, local, paid',\n",
              " 88: 'thing, degree, good, positive, quality, comparative, superlative, signifies, comparison, degrees, word, bad, another, make, put, man, two, night, joined, used, god, like, adjectives, adding, place, est, added, compared, less, words, part, three, manner, better, rowed, ill, ing, high, men, help, little, great, thus, wise, tree, state, without, bie, kind, law, also, made, adje, call, wiser, cafe, near, qualities, way, makes, black, fay, viz, adjective, fore, lord, formed, beautiful, number, wife, take, time, things, four, give, many, adverb, together, tive, highest, name, comp, upon, compare, year, row, head, person, something, tives, com, day, hard, fair, belonging, old, writh, white, either, rule',\n",
              " 89: 'voice, speaking, reading, elocution, emphasis, speaker, expression, speech, must, force, tone, gesture, delivery, speak, words, manner, tones, good, public, action, utterance, audience, attention, articulation, practice, natural, eloquence, pauses, pause, every, speakers, read, upon, sense, proper, without, exercises, well, much, conversation, power, orator, oratory, reader, vocal, body, inflection, modulation, often, pitch, style, emphatic, use, variety, effect, always, importance, thought, energy, hearers, gestures, emotion, requires, inflections, make, express, necessary, feeling, give, pronunciation, faults, general, time, command, discourse, full, language, part, cadence, correct, sentence, habit, passion, meaning, pulpit, orators, passions, great, different, movement, sentiment, first, forcible, graceful, mind, exercise, nature, right, would, expressive',\n",
              " 90: 'art, rhetoric, science, principles, must, knowledge, nature, subject, system, speech, mind, language, rules, means, laws, every, thought, logic, form, arts, elements, necessary, theory, true, first, general, would, method, however, study, discourse, use, expression, truth, upon, well, thus, practice, principle, mental, sense, human, object, practical, application, view, without, power, various, others, also, powers, different, present, natural, speaking, therefore, action, end, essential, certain, either, even, term, fact, matter, effect, oratory, work, process, made, much, skill, aristotle, definition, shall, way, whether, part, attention, case, experience, forms, considered, whole, proper, far, hence, analysis, two, man, yet, result, great, cannot, law, words, order, scientific, good',\n",
              " 91: 'line, lines, rhyme, two, stanza, verse, four, first, rhymes, three, rime, poem, stanzas, alliteration, second, five, read, sonnet, metre, page, couplet, end, form, last, verses, eight, six, poems, following, seven, rimes, examples, half, see, also, rhyming, note, syllables, iii, ten, foot, sonnets, used, beginning, third, number, fourth, measure, couplets, example, syllable, alliterative, regular, twenty, occurs, thus, nine, forms, metrical, use, etc, sometimes, found, occur, iambic, find, english, passage, trochaic, alexandrine, double, repetition, common, psalm, blank, twelve, type, final, even, pentameter, another, rhymed, hundred, middle, written, refrain, later, quatrain, instances, old, heroic, ending, part, pause, measures, though, like, full, col, chaucer',\n",
              " 92: 'verb, verbs, person, pronouns, noun, case, pronoun, nouns, nominative, number, used, participle, plural, singular, third, gender, personal, infinitive, relative, adjective, second, called, form, active, rule, persons, mood, present, first, passive, neuter, word, action, thou, objective, object, tense, two, adjectives, participles, tenses, possessive, cafe, thing, transitive, substantive, love, time, denotes, nom, preposition, three, mode, conjugation, must, either, auxiliary, irregular, cases, man, thus, expresses, sing, obj, indicative, loved, subjunctive, sometimes, formed, always, expressed, past, also, express, forms, intransitive, adverb, different, words, adverbs, declension, poss, use, numbers, genitive, antecedent, moods, subject, regular, spoken, imperative, manner, agree, accusative, prepositions, many, relation, things, agent, sense',\n",
              " 93: 'also, haue, iii, tho, men, good, god, see, man, hath, pat, hym, well, many, doth, make, hys, made, thus, right, king, hem, alle, full, sir, unto, like, come, eights, doe, faire, ben, take, day, hir, set, neuer, booke, vnto, tyme, wel, yet, grace, tale, ther, bee, first, said, knight, owne, old, two, none, sonne, hit, hart, selfe, hee, wordes, word, euer, lord, whiche, forth, great, name, onely, best, whan, thinke, vpon, soule, noble, eke, downe, shall, long, following, hie, thow, loue, better, prol, ende, wise, part, maner, mee, wolde, euery, oure, gan, gode, vse, ouer, place, gold, matter, rose, hand',\n",
              " 94: 'part, sea, king, water, island, spring, coast, called, fide, river, land, north, name, britain, south, asia, place, east, son, town, west, kings, inhabitants, lower, islands, britons, africa, egypt, goths, wales, people, city, gaul, ancient, tribe, another, near, country, western, away, also, bard, old, spain, isle, miles, act, surrounding, upon, low, scot, satura, parts, inhabited, eastern, italy, mountains, roman, great, regions, state, lands, first, chief, known, confines, province, border, places, whence, persia, southern, three, cos, tribes, ocean, time, empire, high, peopled, rivers, adultery, race, rhine, way, according, power, tenn, ground, hence, came, along, jutland, inland, advise, year, adventure, mose, albion, territory',\n",
              " 95: 'taste, virtue, mind, nature, good, human, man, men, passions, life, sense, every, mankind, moral, admiration, beauty, delicacy, power, love, heart, state, great, eloquence, among, pleasure, reason, always, must, genius, sentiments, youth, beauties, poetry, high, degree, character, virtuous, less, spirit, virtues, relish, minds, without, sensibility, pleasures, kind, thing, early, public, strong, yet, natural, vice, affections, conduct, true, manners, common, though, delicate, objects, merit, many, noble, age, contempt, bad, truth, powerful, pleasing, nothing, exercise, necessary, beautiful, arts, faculty, elegant, whose, fine, proper, shall, appear, time, whatever, tender, improvement, religion, principles, influence, correctness, glory, others, esteem, general, pursuits, feeling, wisdom, frequent, external, correct',\n",
              " 96: \"speech, brutus, first, oration, time, part, cicero, senate, cæsar, upon, place, orator, scipio, ancient, army, eloquence, public, power, act, body, another, life, athenians, country, manner, cato, private, assembly, men, romans, hannibal, demosthenes, war, great, relating, speeches, character, little, pieces, rich, year, thought, good, times, following, make, among, soliloquy, ornament, importance, person, order, without, well, hopes, would, cassius, people, call, general, athens, battle, state, learning, death, author, account, collier's, second, command, use, art, concerning, chosen, catiline, choice, edinburgh, council, rank, bar, address, decree, affairs, soldiers, roman, amusement, every, field, real, marriage, world, attention, divorce, orations, rome, merit, occasion, temptation, care, reply\",\n",
              " 97: 'shall, tense, past, present, would, future, time, perfect, learn, might, learned, imperfect, mood, first, could, written, write, writing, used, sing, place, wrote, indicative, come, must, action, yet, mean, word, pers, express, thus, say, also, second, pluperfect, finished, seen, use, expresses, tenses, potential, done, form, see, means, called, many, take, call, thou, taken, thing, gone, different, represents, event, well, three, letters, act, though, every, like, read, let, mode, letter, pres, saw, singular, words, perf, often, completed, manner, imp, either, made, system, aorist, ought, begin, mentioned, men, english, rather, phrases, without, matter, two, learne, hereafter, question, james, wish, took, great, verb, case',\n",
              " 98: 'deriv, finger, harmonic, instruments, major, disc, moses, record, libre, lap, key, drum, tapping, gramophone, minor, pipes, keys, beat, made, first, harp, string, two, note, flap, left, clap, lab, papyrus, tap, amplitude, later, instrument, parchment, could, holes, thus, upon, among, vers, hand, whistle, music, take, rip, tuning, records, resonators, much, long, scale, cochlea, writing, enoch, next, ost, name, doubtless, rod, rather, horizontally, times, would, ribbon, adam, great, phiz, tomorrow, dua, also, taking, cain, said, known, longitudinal, least, admitted, vibrating, black, aga, vertically, porphyry, though, athelstan, order, signals, camp, stone, saet, others, tracing, miss, see, weighted, iii, repeated, notes, groove, paper, invented',\n",
              " 99: 'would, life, even, great, must, men, much, upon, man, time, literature, many, world, new, history, far, fact, yet, could, less, literary, indeed, still, work, true, might, people, power, mind, truth, say, knowledge, human, present, without, spirit, well, made, interest, though, character, like, question, however, whole, thought, every, age, perhaps, years, way, rather, almost, certain, little, philosophy, never, first, cannot, point, modern, criticism, things, social, day, among, ever, case, influence, whether, intellectual, least, also, general, seems, said, times, society, find, individual, make, national, always, facts, view, religious, whose, course, nothing, become, self, reason, moral, public, nature, mere, matter, doubt, religion, merely'}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_topic_terms(lda_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = lda_model.get_document_topics(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating over jsonl: 100%|██████████| 2097745/2097745 [03:12<00:00, 10887.70it/s]\n"
          ]
        }
      ],
      "source": [
        "# done_ids = {page_id for docid,page_id,toks in iter_corpus()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# undone_ids = all_page_ids - done_ids\n",
        "# # undone_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating over jsonl:   0%|          | 28/2097745 [00:00<11:07, 3144.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# doctopics=[]\n",
        "# for docid,pageid,toks in iter_corpus(None,None):\n",
        "#     if pageid in undone_ids:\n",
        "#         bow = dictionary.doc2bow(toks)\n",
        "#         docs = lda_model.get_document_topics(bow)\n",
        "#         index,vals = zip(*docs)\n",
        "#         sdocs = pd.Series(vals, index=index).sort_values(ascending=False)\n",
        "#         docs2 = lda_model.inference([bow])\n",
        "#         sdocs2 = pd.Series(docs2[0][0]).sort_values(ascending=False)\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.96467066"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sdocs.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26.886023"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sdocs2.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15    0.535725\n",
              "16    0.137418\n",
              "68    0.125203\n",
              "40    0.085632\n",
              "95    0.080688\n",
              "        ...   \n",
              "32    0.000372\n",
              "31    0.000372\n",
              "30    0.000372\n",
              "29    0.000372\n",
              "99    0.000372\n",
              "Length: 100, dtype: float32"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sdocs2/sdocs2.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m1</th>\n",
              "      <th>m2</th>\n",
              "      <th>diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.542084</td>\n",
              "      <td>0.535725</td>\n",
              "      <td>0.006359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.138032</td>\n",
              "      <td>0.137418</td>\n",
              "      <td>0.000614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.115294</td>\n",
              "      <td>0.125203</td>\n",
              "      <td>-0.009909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.088471</td>\n",
              "      <td>0.085632</td>\n",
              "      <td>0.002840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.080790</td>\n",
              "      <td>0.080688</td>\n",
              "      <td>0.000101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          m1        m2      diff\n",
              "15  0.542084  0.535725  0.006359\n",
              "16  0.138032  0.137418  0.000614\n",
              "68  0.115294  0.125203 -0.009909\n",
              "40  0.088471  0.085632  0.002840\n",
              "95  0.080790  0.080688  0.000101\n",
              "0        NaN  0.000372       NaN\n",
              "1        NaN  0.000372       NaN\n",
              "2        NaN  0.000372       NaN\n",
              "3        NaN  0.000372       NaN\n",
              "4        NaN  0.000372       NaN\n",
              "5        NaN  0.000372       NaN\n",
              "6        NaN  0.000372       NaN\n",
              "7        NaN  0.000372       NaN\n",
              "8        NaN  0.000372       NaN\n",
              "9        NaN  0.000372       NaN\n",
              "10       NaN  0.000372       NaN\n",
              "11       NaN  0.000372       NaN\n",
              "12       NaN  0.000372       NaN\n",
              "13       NaN  0.000372       NaN\n",
              "14       NaN  0.000372       NaN\n",
              "17       NaN  0.000372       NaN\n",
              "18       NaN  0.000372       NaN\n",
              "19       NaN  0.000372       NaN\n",
              "20       NaN  0.000372       NaN\n",
              "21       NaN  0.000372       NaN"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sdocdf=pd.DataFrame({'m1':sdocs, 'm2':sdocs2/sdocs2.sum()}).sort_values('m1',ascending=False)\n",
        "sdocdf['diff'] = sdocdf['m1'] - sdocdf['m2']\n",
        "sdocdf.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "s=pd.Series(docs2[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15    14.719552\n",
              "16     3.723455\n",
              "68     2.924183\n",
              "40     2.397353\n",
              "95     2.176149\n",
              "        ...    \n",
              "32     0.010000\n",
              "31     0.010000\n",
              "30     0.010000\n",
              "29     0.010000\n",
              "99     0.010000\n",
              "Length: 100, dtype: float32"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

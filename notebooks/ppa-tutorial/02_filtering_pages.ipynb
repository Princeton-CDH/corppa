{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 – Filtering pages with `corppa`\n",
    "\n",
    "The `corppa.utils.filter` helpers let you stream-filter the 1.8 GB PPA page\n",
    "file without ever loading it fully into RAM.  In this notebook we show how to\n",
    "\n",
    "* keep only **certain works** (by `work_id` or via a metadata query),\n",
    "* keep **specific page ranges** within those works,\n",
    "* include or exclude pages by **tags / labels**, and\n",
    "* **write** the result to a new compressed JSONL file.\n",
    "\n",
    "Everything below executes in well under a minute on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib, gzip, json, itertools, pandas as pd\n",
    "from corppa.utils.filter import filter_pages, save_filtered_corpus\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- data locations ---\n",
    "DATA_DIR = pathlib.Path('..') / 'shared_data' / 'ppa_corpus_2025-02-03_1308'\n",
    "PAGES_FILE = DATA_DIR / 'ppa_pages.jsonl.gz'\n",
    "META_CSV   = DATA_DIR / 'ppa_metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Filter entire works by a metadata query\n",
    "\n",
    "Suppose we want *all* pages from works printed **in 1890**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works published in 1890: 48\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(META_CSV, dtype=str)\n",
    "work_ids_1890 = metadata_df.loc[metadata_df['pub_year'] == '1890', 'work_id'].tolist()\n",
    "print(f\"Works published in 1890: {len(work_ids_1890)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo set → ['chi.12153205-p526', 'coo1.ark:/13960/t2m623920', 'hvd.ah2545']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering: checked 1,982,024 pages, selected 447 | elapsed: 00:09\n",
      "streaming: 447it [00:09, 47.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total pages kept: 447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect first 3 matches just for demo\n",
    "sample_work_ids = work_ids_1890[:3]\n",
    "print('Demo set →', sample_work_ids)\n",
    "\n",
    "pages_iter = filter_pages(PAGES_FILE, work_ids=sample_work_ids, disable_progress=False)\n",
    "page_count = 0\n",
    "for page in tqdm(pages_iter, desc='streaming'):\n",
    "    page_count += 1\n",
    "print(f\"\\nTotal pages kept: {page_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Filter by **page ranges** inside a work\n",
    "\n",
    "Here we take a single work and keep only pages 1–5 & 20–25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 0 pages for chi.12153205-p526. Labels → []\n"
     ]
    }
   ],
   "source": [
    "one_work = sample_work_ids[0]\n",
    "# dict format expected by filter_pages → {work_id: [list-of-orders]}\n",
    "work_pages = {one_work: list(range(1, 6)) + list(range(20, 26))}\n",
    "\n",
    "subset = list(filter_pages(PAGES_FILE, work_pages=work_pages, disable_progress=True))\n",
    "print(f\"Kept {len(subset)} pages for {one_work}. Labels → {[p['label'] for p in subset[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Include / exclude by tag or label\n",
    "\n",
    "Let’s keep pages tagged *`poem`* but **exclude** any whose label equals `[advertisement]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = filter_pages(\n",
    "    PAGES_FILE,\n",
    "    include_any_tags=['poem'],\n",
    "    exclude_labels=['[advertisement]'],\n",
    "    disable_progress=True,\n",
    ")\n",
    "first_five = list(itertools.islice(kept, 5))\n",
    "print([{'id': p['id'], 'label': p['label'], 'tags': p['tags']} for p in first_five])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Save a filtered corpus to disk\n",
    "\n",
    "The helper writes a **new gzipped JSONL** file that you can chain into later\n",
    "workflows or share with collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILE = DATA_DIR / 'subset_pages.jsonl.gz'\n",
    "save_filtered_corpus(\n",
    "    PAGES_FILE,\n",
    "    OUT_FILE,\n",
    "    work_ids=sample_work_ids,\n",
    "    include_any_tags=['poem'],\n",
    "    disable_progress=False,\n",
    ")\n",
    "print('Written →', OUT_FILE.relative_to(DATA_DIR.parent.parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Doing the same from the command line\n",
    "\n",
    "```bash\n",
    "# keep only poems from works printed in 1890\n",
    "corppa-filter \\\n",
    "  --pages ../shared_data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz \\\n",
    "  --output subset_1890_poems.jsonl.gz \\\n",
    "  --work-ids $(python -c \"import pandas as pd,sys; df=pd.read_csv('META.csv'); print(' '.join(df.query('pub_year==\\'1890\\'')['work_id']))\") \\\n",
    "  --include-any-tags poem\n",
    "```\n",
    "\n",
    "*(Wrap the `$(...)` bit or pre-compute a text file with work IDs.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "➡️ Proceed to `03_images_and_paths.ipynb` to learn how to resolve a page to its high-resolution scan and build image paths!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corppa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
